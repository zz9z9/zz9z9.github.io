---
title: Cubridì—ì„œ MySQLë¡œì˜ ì—¬ì •(ë²ˆì™¸) - CDC í™œìš© ìƒê°í•´ë³´ê¸°
date: 2025-11-03 22:00:00 +0900
categories: [ê²½í—˜í•˜ê¸°, ì‘ì—… ë…¸íŠ¸]
tags: [MySQL]
---

## ê¸°ì¡´ ë°©ì‹ì˜ í•œê³„ì 
---

- ì‹¤ì œ ì‘ì—…ì€ ì„œë¹„ìŠ¤ë¥¼ Read Only DBì— ë¶™ì—¬ì„œ Cubrid(Source DB)ì—ì„œì˜ ë³€ê²½ì„ ë§‰ê³  ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ì„ ì§„í–‰í–ˆë‹¤.
- ë”°ë¼ì„œ, ë§ˆì´ê·¸ë ˆì´ì…˜ ë™ì•ˆ ì¡°íšŒë¥¼ ì œì™¸í•œ ìš”ì²­ì€ ì‹¤íŒ¨ì²˜ë¦¬ëë‹¤.
- ë§ˆì´ê·¸ë ˆì´ì…˜ì€ ì•½ 20ë¶„ ì •ë„ ì†Œìš”ëë‹¤.

> í•˜ì§€ë§Œ, ì„œë¹„ìŠ¤ ë‹¤ìš´íƒ€ì„ì´ ì´ ì •ë„ê¹Œì§€ í—ˆìš©ë˜ì§€ ì•ŠëŠ” ì„œë¹„ìŠ¤ë“¤ì˜ ê²½ìš° ì–´ë–¤ ì „ëµì„ ì‚¬ìš©í•˜ë©´ ì¢‹ì„ê¹Œ ?

## CDC (Change Data Capture) ì ìš©í•´ë³´ê¸°
---
> CubridëŠ” ì§€ì›ë˜ëŠ” Source Connectorê°€ ì—†ì–´ì„œ CDCë¥¼ ì ìš©í•˜ê¸°ê°€ ì–´ë µê¸° ë•Œë¬¸ì—, Source DBê°€ MySQLì´ë¼ê³  ê°€ì •í–ˆì„ë•Œ

- íë¦„ ìƒìƒí•´ë³´ê¸°

![img.png](db-mig-cdc-simulation2.png)

```
1. ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜
(ë§ˆì´ê·¸ë ˆì´ì…˜ ë˜ëŠ” ë™ì•ˆ ë°œìƒí•œ DML ì´ë²¤íŠ¸ê°€ Kafka í† í”½ì— ìŒ“ì„)
2. ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ
3. ìŒ“ì¸ DML ì´ë²¤íŠ¸ ì²˜ë¦¬
4. ì´ë²¤íŠ¸ ì²˜ë¦¬ ê±°ì˜ë‹¤ ëì„ë•Œ ë”ì´ìƒ DML ì´ë²¤íŠ¸ ë°œìƒí•˜ì§€ ì•Šë„ë¡ ì„œë¹„ìŠ¤ ì ê²€
(ì„œë¹„ìŠ¤ ì¤‘ë‹¨)
5. ëª¨ë“  ì´ë²¤íŠ¸ ì²˜ë¦¬ ì™„ë£Œëìœ¼ë©´ Target DBë¥¼ ì„œë¹„ìŠ¤ DBë¡œ ë³€ê²½
(ì„œë¹„ìŠ¤ ì¬ê°œ)
```

## ê³ ë¯¼í•´ë³¼ ë¶€ë¶„ë“¤
---

### ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ë°©ì‹
> debezium connector initial snapshot vs ETL ë„êµ¬(PDI, DataX, etc)ê¸°ë°˜ ë¡œë”©

**debezium connector initial snapshot**
- ì¥ì 
  - í˜„ì¬ ìƒíƒœì˜ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ + ë§ˆì´ê·¸ë ˆì´ì…˜ë˜ëŠ” ë™ì•ˆ ë°œìƒí•œ ë³€ê²½ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì¤Œ (ìŠ¤ëƒ…ìƒ· ì™„ë£Œ ì´í›„ ë³€ê²½ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë°)

- ë‹¨ì 
  - ê¸€ë¡œë²Œ ì½ê¸° ì ê¸ˆì‹œ í…Œì´ë¸” ë½ì´ ë°œìƒ
  - í…Œì´ë¸” í¬ê¸°ê°€ í´ìˆ˜ë¡ ì˜¤ë˜ ê±¸ë¦°ë‹¤
  - `SELECT * FROM <table>`
    - ì¸ë±ìŠ¤ë¥¼ ì“°ì§€ ì•Šìœ¼ë¯€ë¡œ í…Œì´ë¸” ì „ì²´ë¥¼ ì½ìŒ â†’ ëŒ€ëŸ‰ì˜ sequential read.
    - InnoDB buffer poolì´ ë°€ë ¤ì„œ ì‹¤ì œ ì„œë¹„ìŠ¤ ì¿¼ë¦¬ ìºì‹œê°€ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŒ.
  -  snapshotì´ ëë‚  ë•Œê¹Œì§€:
    - long-running transactionì´ ìœ ì§€ë˜ê³ ,
    - MVCC snapshotì„ ìœ ì§€í•˜ê¸° ìœ„í•œ undo logë¥¼ InnoDBê°€ ë³´ê´€í•©ë‹ˆë‹¤.

| í•­ëª©              | ì˜í–¥                                    |
| --------------- | ------------------------------------- |
| undo log ë³´ê´€     | ìŠ¤ëƒ…ìƒ· ì‹œì  ì´í›„ ë³€ê²½ëœ rowì˜ undo version ê³„ì† ìœ ì§€ |
| buffer pool I/O | ëŒ€ëŸ‰ SELECTë¡œ ì¸í•œ ë””ìŠ¤í¬ read í­ì¦             |
| redo log ì˜í–¥     | ê±°ì˜ ì—†ìŒ (read-onlyì´ë¯€ë¡œ)                  |
| row lock        | ì—†ìŒ                                    |
| binlog          | snapshot ì´í›„ DMLì€ binlogë¡œ ë³„ë„ ì²˜ë¦¬        |

```
t0:   FLUSH TABLES WITH READ LOCK
t0.1: SHOW MASTER STATUS â†’ binlog pos = 12345
t0.2: UNLOCK TABLES
t0.3: START TRANSACTION WITH CONSISTENT SNAPSHOT
t0.4: SELECT * FROM users; â†’ emit user row events (op=r)
t1.0: SELECT * FROM orders; â†’ emit order row events (op=r)
t2.0: COMMIT
t2.1: start binlog streaming from pos=12345
t2.2: new inserts/updates/deletes â†’ emit op=c/u/d
```

- íŠ¹ì • ì¡°ê±´ í•„í„°ë§ ë¶ˆê°€(ì˜ˆ: ì´ë ¥ í…Œì´ë¸”ì˜ ê²½ìš° ì „ë‚  ìì •ê¹Œì§€ ìŒ“ì¸ê±´ ì´ë¯¸ ì™„ë£Œë˜ì–´ìˆê¸° ë•Œë¬¸ì— ìì • ~ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘í•˜ëŠ” ìƒˆë²½ì‹œê°„ëŒ€ê¹Œì§€ ìŒ“ì¸ê²ƒë§Œ ì˜®ê²¨ì£¼ë©´ë˜ëŠ”ë°)




**ETL ë„êµ¬**
> PDI (Pentaho Data Integration) / ETL ê¸°ë°˜ ë¡œë”©

- ì¥ì 
  - ì„±ëŠ¥ ë° ìœ ì—°ì„±
    - ë³‘ë ¬ë¡œ í…Œì´ë¸” ë‹¨ìœ„/íŒŒí‹°ì…˜ ë‹¨ìœ„ ë¡œë”© ê°€ëŠ¥.
    - DB â†’ File â†’ DB ë“± ë‹¤ì–‘í•œ ì¤‘ê°„ë‹¨ê³„(batch ê¸°ë°˜) ì²˜ë¦¬ ê°€ëŠ¥.
  - DB ë¶€ë‹´ ì¡°ì ˆ ê°€ëŠ¥
    - ëŒ€ìš©ëŸ‰ í…Œì´ë¸”ì„ ì‹œê°„ëŒ€ë³„/ì¡°ê±´ë³„ë¡œ ìª¼ê°œì„œ ë¡œë”© ê°€ëŠ¥.
  - ë°ì´í„° ì „ì²˜ë¦¬/ê²€ì¦ ìš©ì´
    - íƒ€ì… ë³€í™˜, Null ë³´ì •, ì»¬ëŸ¼ ë§¤í•‘ ë“± snapshot ì „ì— í•œ ë²ˆì— ì²˜ë¦¬ ê°€ëŠ¥.

- ë‹¨ì 
  - ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì  ì´í›„ì˜ ì´ë²¤íŠ¸ê°€ ì •í™•íˆ ì–´ë””ì„œë¶€í„°ì¸ì§€ íŒŒì•…ì´ ì–´ë µë‹¤.
    - ì¦‰, Debeziumì„ ì–¸ì œë¶€í„° binlogë¥¼ tailing ì‹œí‚¬ì§€(ì¦‰, ì‹œì‘ offsetì„ ì–´ë””ë¡œ ì¡ì„ì§€) ëª…í™•íˆ í•´ì•¼ í•¨.

**ë‚´ ìƒê°**
> ETL ë„êµ¬ + CDC ë°©ì‹ ì‚¬ìš©ì´ ë” ì¢‹ì€ ê²ƒ ê°™ë‹¤.<br>
> initial snapshotì˜ ë‹¨ì ì´ ë°ì´í„°ê°€ ì ì§€ ì•Šì€ ìš°ë¦¬ì˜ ìš´ì˜ í™˜ê²½ì—ëŠ” ê½¤ ì¹˜ëª…ì ìœ¼ë¡œ ì‘ìš©í•  ê²ƒ ê°™ë‹¤. (íŠ¹íˆ DBì—)


**í•´ê²°í•´ì•¼í•  ê³¼ì œ**
> ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ ì´í›„ ë§ˆì´ê·¸ë ˆì´ì…˜ ë™ì•ˆ ë°œìƒí•œ ë³€ê²½ ì´ë²¤íŠ¸ë¥¼ ì‹±í¬í• ë•Œ ë°ì´í„° ì •í•©ì„± ê¹¨ì§€ì§€ ì•Šê²Œ í•˜ë ¤ë©´ ?

- ETL ë„êµ¬ë¥¼ ì´ìš©í•´ì„œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘í•˜ëŠ” ì‹œì ì˜ binlog ìœ„ì¹˜ë¥¼ ì •í™•íˆ ì•„ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥í•˜ë‹¤ê³  ìƒê° (ì´ˆê¸° ìŠ¤ëƒ…ìƒ·ì²˜ëŸ¼ í…Œì´ë¸” ì ê¸ˆí•˜ê³  binlog ìœ„ì¹˜ë¥¼ ì–»ì–´ì˜¤ëŠ”ê²Œ ì•„ë‹ˆê¸° ë•Œë¬¸ì—)
- ë”°ë¼ì„œ, ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘ë³´ë‹¤ ì¡°ê¸ˆ ì•ì„  ì‹œì ì— CDCë¡œ ë³€ê²½ ì´ë²¤íŠ¸ ìˆ˜ì§‘
- ë§ˆì´ê·¸ë ˆì´ì…˜ ì´í›„ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë° í•  ë•Œ, **ìŠ¤ëƒ…ìƒ· ë°ì´í„°ì— ì´ë¯¸ ë°˜ì˜ëœ ë³€ê²½ ì´ë²¤íŠ¸ëŠ” ì²˜ë¦¬ë˜ì§€ ì•Šë„ë¡ ë˜ëŠ” ì •í•©ì„±ì— ì˜í–¥ ì—†ë„ë¡ í•˜ë ¤ë©´ ??**

- ë‚´ ìƒê°
  - create : í•´ë‹¹ pkë¡œ selectí•´ì„œ ìˆìœ¼ë©´ skip
  - update : í•´ë‹¹ pk í–‰ì´ ì—†ê±°ë‚˜, ë³€ê²½ì¼ì‹œê°€ ë³€ê²½ ì´ë²¤íŠ¸ì˜ afterì— ìˆëŠ” ë³€ê²½ì¼ì‹œë³´ë‹¤ ì´í›„ì´ë©´ skip
     - ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ update ì²˜ë¦¬ì‹œ ë¬´ì¡°ê±´ ë³€ê²½ì¼ì‹œë„ ì—…ë°ì´íŠ¸ ë˜ë„ë¡ í•´ë†“ëŠ”ê²Œ ë³´ì¥ë˜ì–´ì•¼í•œë‹¤ ?
     - ë³€ê²½ ì´ë²¤íŠ¸ beforeì˜ updated_atê³¼ afterì˜ updated_atì´ ê°™ìœ¼ë©´ ?? (ì¦‰, ì—…ë°ì´íŠ¸ëŠ” í•˜ì§€ë§Œ ë³€ê²½ì¼ì‹œëŠ” ì—…ë°ì´íŠ¸ ì²˜ë¦¬ë¥¼ ì œëŒ€ë¡œ ì•ˆí•´ë†“ì€ ê²½ìš°)
     - msê¹Œì§€ ê´€ë¦¬í•˜ì§€ ì•ŠëŠ” ì´ìƒ, 04:00:01.100ì— ì—…ë°ì´íŠ¸ 04:00:01.111ì— ETL ì‹œì‘í•œ ê²½ìš°, ë˜ëŠ” ê·¸ ë°˜ëŒ€ì¸ ê²½ìš°, ì—…ë°ì´íŠ¸ë¥¼ í•´ì•¼í• ì§€ ë§ì•„ì•¼í• ì§€ íŒë‹¨ì´ ì–´ë µë‹¤
     - `updated_at`ì´ ê°™ì€ ê²½ìš° ? => ë³€ê²½ ì»¬ëŸ¼ë“¤ ë‹¤ ë¹„êµí•´ì„œ ë™ì¼í•˜ì§€ ì•Šìœ¼ë©´ update
  - delete : í•´ë‹¹ pk ì¡°íšŒí–ˆì„ ë•Œ ì—†ìœ¼ë©´ skip
  - 4ì‹œì— ETL ì‹œì‘í–ˆë‹¤ê³  í•˜ë©´, 4ì‹œ 1ë¶„ì •ë„ ê¹Œì§€ì˜ ë³€ê²½ ì´ë²¤íŠ¸ì˜ í…Œì´ë¸”,pk í™•ë³´í•´ì„œ ì¶”í›„ì— ë°°ì¹˜ë¡œ ì œëŒ€ë¡œ ì •í•©ì„± ë§ëŠ”ì§€ ë¹„êµ ??

=> https://techblog.lycorp.co.jp/ko/migrating-large-data-with-kafka-and-etl => cdc ì‚¬ìš©í•´ì„œ DB ë§ˆì´ê·¸ë ˆì´ì…˜
ì—¬ê¸´ í…Œì´ë¸” í•˜ë‚˜ê¸´í•¨
```
ETL ì´í›„ë¶€í„° í˜„ì¬ê¹Œì§€ì˜ ë³€ê²½ ì‚¬í•­ì„ MongoDBì— ì ìš©í•˜ë ¤ë©´ CDC í† í”½ì— ë“¤ì–´ì˜¨ MySQL CDC ë©”ì‹œì§€ë¥¼ MongoDBì— ë°˜ì˜í•˜ë©´ ë˜ëŠ”ë° ì´ë•Œ ì–¸ì œë¶€í„°ì˜ ë©”ì‹œì§€ë¥¼ ì†Œë¹„í•  ê²ƒì¸ì§€ë¥¼ ì •í•´ì•¼ í•©ë‹ˆë‹¤. ì‚¬ì‹¤ ETL ë°ì´í„°ê°€ ì •í™•íˆ ì–´ëŠ ì‹œì ì˜ ë°ì´í„°ì¸ì§€ëŠ” ì•Œ ìˆ˜ ì—†ìœ¼ë©°, ì‹œì ì„ ì •í–ˆì„ ë•Œ ì˜¤ì°¨ê°€ ë°œìƒí•  í™•ë¥ ë„ ë†’ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ìš°ì„  ëˆ„ë½ëœ ë©”ì‹œì§€ë¥¼ ì—†ì• ê¸° ìœ„í•´ ETLì„ ë‚´ë¦¬ëŠ” ì‹œì  ì§ì „ìœ¼ë¡œ MySQL CDC ì˜¤í”„ì…‹ì„ ì¡°ì ˆí–ˆìŠµë‹ˆë‹¤.

ì´ë ‡ê²Œ ì¡°ì¹˜í•˜ë©´ ëˆ„ë½ëœ ë©”ì‹œì§€ëŠ” ì—†ì„ í…Œì§€ë§Œ ì´ë¯¸ ETL í…Œì´ë¸”ì— ë°˜ì˜ëœ ë©”ì‹œì§€ê°€ ì¡´ì¬í•  ìˆ˜ ìˆëŠ”ë°ìš”. ê° CDC ë©”ì‹œì§€ëŠ” ë©±ë“±ì„±ì´ ë³´ì¥ëœ ë°ì´í„°ê°€ ì•„ë‹ˆê¸°ì— ì¤‘ë³µ ì²˜ë¦¬ë¥¼ í•˜ë©´ ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤. ì¦‰, ì¤‘ë³µëœ ë©”ì‹œì§€ëŠ” ë‹¤ì‹œ ì²˜ë¦¬ë˜ì§€ ì•Šê²Œ ë¡œì§ì„ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.
=> ì´ë¯¸ì§€ í…Œì´ë¸”ì— updatedDateë¼ëŠ” í•„ë“œê°€ ì¡´ì¬í–ˆê¸°ì— í•´ë‹¹ í•„ë“œë¥¼ ì‚¬ìš©í•´ì„œ ìµœì‹  ë©”ì‹œì§€ë§Œ ì ìš©í•˜ë„ë¡ ë©”ì‹œì§€ë¥¼ íŒë³„í•˜ëŠ” í•¨ìˆ˜ë¥¼ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤. ì´ë¡œì¨ ì¤‘ë³µ ë©”ì‹œì§€ê°€ ë“¤ì–´ì™€ë„ ìµœì‹  ë³€ê²½ ì‚¬í•­ë§Œ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
=> fun isNewerMessage(message: Value): Boolean {
        val existingImage = imageRepository.findById(message.id)
        return existingImage.isEmpty || mapper.convertValue(message.updateDate.get(), OffsetDateTime::class.java).isAfter(existingImage.get().updatedDate)
```

ê·¼ë° ì¤‘ë³µì²˜ë¦¬ ë˜ë”ë¼ë„ ê²°êµ­ ìŒ“ì¸ê±° ë‹¤ ì†Œë¹„í•˜ë©´ ìµœì¢… ìƒíƒœëŠ” ê°™ì€ê±° ì•„ë‹Œê°€ ?

> "ìµœì¢… ìƒíƒœëŠ” ê°™ì€ê±° ì•„ë‹Œê°€?"

**â†’ ë„¤, ë§ìŠµë‹ˆë‹¤!**

í•˜ì§€ë§Œ:
- **ì„±ëŠ¥**: ì¤‘ë³µ ì œê±°ê°€ í›¨ì”¬ ë¹ ë¦„
- **ë¹„ìš©**: ë¶ˆí•„ìš”í•œ Write ê°ì†Œ
- **ì•ˆì •ì„±**: Downstream ì˜í–¥ ìµœì†Œí™”

ì§€ë§Œ í˜„ì‹¤ì ìœ¼ë¡œ ì´ëŸ° ë¬¸ì œê°€ ìƒê¹€
ğŸ”¹ â‘  offset ì¡°ì • ì‹œ â€˜1ï¸âƒ£ë§Œ ì¼ë¶€ ë‹¤ì‹œ ì½íˆëŠ”â€™ ê²½ìš°

Kafka ì˜¤í”„ì…‹ì„ â€œETL ì¢…ë£Œ ì§ì „â€ìœ¼ë¡œ ë˜ëŒë¦°ë‹¤ê³  í–ˆì£ ?
ê·¸ëŸ°ë° ê·¸ ì˜¤í”„ì…‹ì´ 1ï¸âƒ£ ì§í›„, 2ï¸âƒ£ ì§ì „ì— ì €ì¥ëœ ìƒíƒœë¼ë©´?

â€¦offset=12345(1ï¸âƒ£ ë), offset=12346(2ï¸âƒ£ ì‹œì‘)


CDC connectorë¥¼ ì¬ì‹œì‘í•˜ë©´

offset=12345ë¶€í„° ë‹¤ì‹œ ì½ìŒ â†’ 1ï¸âƒ£ ì¬ì²˜ë¦¬

í•˜ì§€ë§Œ 2ï¸âƒ£ ì´ë²¤íŠ¸ëŠ” Kafka log retention ì •ì±… ë•Œë¬¸ì— ì´ë¯¸ ì‚¬ë¼ì§ or skipë¨

ê²°ê³¼:

ì´ë²¤íŠ¸	MongoDB ìµœì¢… ìƒíƒœ
1ï¸âƒ£ (replay only)	âŒ rollback to 200

ì¦‰, 2ï¸âƒ£ì€ ë‹¤ì‹œ ì ìš©ë˜ì§€ ëª»í•´ ê²°ê³¼ê°€ ê³¼ê±° ìƒíƒœë¡œ ëŒì•„ê°‘ë‹ˆë‹¤.

ğŸ”¹ â‘¡ connectorê°€ í•œ ë²ˆì— ì—¬ëŸ¬ íŒŒí‹°ì…˜ì„ ë³‘ë ¬ consumeí•  ë•Œ

Debezium â†’ Kafka â†’ Sink connector ì²´ì¸ì—ì„œ
MySQL row â†’ Kafka record â†’ MongoDB write
ì´ê²Œ ëª¨ë‘ exactly-onceê°€ ì•„ë‹™ë‹ˆë‹¤.

ì˜ˆ:
1ï¸âƒ£, 2ï¸âƒ£ ì´ë²¤íŠ¸ê°€ ì„œë¡œ ë‹¤ë¥¸ Kafka ë°°ì¹˜ì— ë‚˜ë‰˜ì–´ ì „ì†¡
â†’ Mongo sinkì—ì„œ 2ï¸âƒ£ ì»¤ë°‹ ì„±ê³µ
â†’ connector crash â†’ offset ì»¤ë°‹ ì‹¤íŒ¨
â†’ ì¬ì‹œì‘ ì‹œ offset=1ï¸âƒ£ ë¶€í„° ë‹¤ì‹œ ì½ìŒ
â†’ 1ï¸âƒ£ë§Œ ì¬ì ìš©ë¨
â†’ 2ï¸âƒ£ì€ Kafka ì¸¡ì—ì„œ skipë¨ (ì´ë¯¸ ack ì²˜ë¦¬ëœ batch)

ê²°ê³¼: rollback ğŸ’¥

ğŸ”¹ â‘¢ MongoDB sink connector ìì²´ì˜ ì¬ì‹œì‘ íƒ€ì´ë°

MongoDB sinkëŠ” â€œKafka offset commitâ€ê³¼ â€œDB write commitâ€ì´ ì›ìì ì´ì§€ ì•ŠìŠµë‹ˆë‹¤.
ì¦‰, ë‹¤ìŒ ìˆœì„œë¡œ ì¼ì–´ë‚  ìˆ˜ ìˆì–´ìš” ğŸ‘‡

MongoDBì— 2ï¸âƒ£ (200â†’300) ì ìš©

connector crash

offset commit ëˆ„ë½

ì¬ì‹œì‘ â†’ 1ï¸âƒ£ë¶€í„° ë‹¤ì‹œ ì½ìŒ

1ï¸âƒ£ (100â†’200) ì¬ì ìš©ë¨ â†’ rollback ë°œìƒ

ğŸ’¥ ìµœì¢…ì ìœ¼ë¡œ MongoDBëŠ” 200,
Kafka offsetì€ 1ï¸âƒ£ ì¬ì²˜ë¦¬ ì¤‘ â†’ ë°ì´í„° ë¶ˆì¼ì¹˜ ë°œìƒ.

ğŸ§  3ï¸âƒ£ í•µì‹¬ ì›ì¸ ìš”ì•½
ì›ì¸	ì„¤ëª…
offset rewind ì‹œì  ë¶€ì •í™•	ì¼ë¶€ ì´ë²¤íŠ¸ë§Œ ì¬ì²˜ë¦¬ë¨
Kafka log retention	ì¬ì²˜ë¦¬ êµ¬ê°„ ì¼ë¶€ ì†ì‹¤ ê°€ëŠ¥
connector crash íƒ€ì´ë°	Mongo writeëŠ” ëëŠ”ë° offset ì»¤ë°‹ ì•ˆ ë¨
sink idempotency ì—†ìŒ	ì¬ì ìš© ì‹œ ê³¼ê±° ìƒíƒœë¡œ overwrite
ì´ë²¤íŠ¸ ìˆœì„œ ê¹¨ì§	ë™ì¼ PKë¼ë„ ìˆœì„œ ì—­ì „ ê°€ëŠ¥ (ë‹¤ì¤‘ topic ë³‘í•© ë“±)
âœ… 4ï¸âƒ£ ì•ˆì „í•˜ê²Œ í•˜ë ¤ë©´
ì „ëµ	ì„¤ëª…
idempotent write	MongoDBì— ì ìš©í•  ë•Œ ts_msë‚˜ transaction_id ê¸°ì¤€ìœ¼ë¡œ ìµœì‹  ì´ë²¤íŠ¸ë§Œ ë°˜ì˜
CDC ì´ë²¤íŠ¸ deduplication	MongoDB ë¬¸ì„œì— â€œlast_cdc_posâ€ ì €ì¥ â†’ pos ì‘ìœ¼ë©´ skip
exactly-once connector ì„¤ì •	Kafka Connect transactional producer/sink í™œì„±í™”
offset rewind í­ ìµœì†Œí™”	ETL ì¢…ë£Œ ì§ì „ì´ ì•„ë‹ˆë¼ snapshot anchor ì§ì „ìœ¼ë¡œ ì¡°ì •
replay ê²€ì¦ ë¡œì§ ì¶”ê°€	replay í›„ row count, checksum ë¹„êµë¡œ ì´ìƒ íƒì§€
âœ… 5ï¸âƒ£ ê²°ë¡  ìš”ì•½
í•­ëª©	ì„¤ëª…
â€œ2ë²ˆì§¸ ì´ë²¤íŠ¸ë„ ë‹¤ì‹œ ì²˜ë¦¬ë˜ë©´ ê²°êµ­ 3 ë˜ì§€ ì•Šë‚˜?â€	ì´ë¡ ì ìœ¼ë¡œëŠ” ë§ì§€ë§Œ,
ì‹¤ì œ í™˜ê²½ì—ì„œëŠ”	offset ì¡°ì •, crash, ìˆœì„œ ë¶ˆì¼ì¹˜ë¡œ ì¸í•´ ì¼ë¶€ ì´ë²¤íŠ¸ë§Œ ì¬ì²˜ë¦¬ë  ìˆ˜ ìˆìŒ
ê²°ê³¼	ìµœì‹  ìƒíƒœ(3)ê°€ ê³¼ê±° ìƒíƒœ(2)ë¡œ rollbackë  ìˆ˜ ìˆìŒ
ì˜ˆë°©	idempotent ì²˜ë¦¬ or latest-event-only ì ìš© í•„ìš”

### Consumer ì„ íƒ
>  Sink Connector vs Spring boot application

- create - í•´ë‹¹ pkë¡œ selectí•´ì„œ ìˆìœ¼ë©´ skip
- update - í•´ë‹¹ pk í–‰ì´ ì—†ê±°ë‚˜, ë³€ê²½ì¼ì‹œê°€ ë³€ê²½ ì´ë²¤íŠ¸ì˜ afterì— ìˆëŠ” ë³€ê²½ì¼ì‹œë³´ë‹¤ ì´í›„ì´ë©´ skip
- delete - í•´ë‹¹ pk ì¡°íšŒí–ˆì„ ë•Œ ì—†ìœ¼ë©´ skip

**ì§ì ‘ consumerë¥¼ êµ¬í˜„í•˜ëŠ” ê²½ìš°**
- Kafka consumerë¥¼ ì§ì ‘ ë§Œë“¤ì–´ (ì˜ˆ: Spring Kafka, Java Kafka Streams ë“±), CDC ì´ë²¤íŠ¸(JSON)ë¥¼ ì½ì–´ì„œ, ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ì½”ë“œë¡œ ì œì–´í•©ë‹ˆë‹¤.
- ì˜ˆì‹œ (Spring Kafka ê¸°ë°˜)

```java
@KafkaListener(topics = "dbserver1.mydb.user")
public void consume(String message) {
    DebeziumEvent event = parse(message);
    User user = event.getAfter();

    if (event.isCreate()) {
        if (exists(user.getId())) return; // skip
        insert(user);
    } else if (event.isUpdate()) {
        User existing = findById(user.getId());
        if (existing == null) return;
        if (existing.getUpdatedAt().isAfter(user.getUpdatedAt())) return; // skip
        update(user);
    } else if (event.isDelete()) {
        if (!exists(user.getId())) return;
        delete(user.getId());
    }
}
```

- ì¥ì 
  - ë¹„ì¦ˆë‹ˆìŠ¤ ë£° ì œì–´ ììœ ë„ 100%
    - ì˜ˆ: ë³€ê²½ì¼ì‹œ, ìƒíƒœ í”Œë˜ê·¸, íŠ¹ìˆ˜ í…Œì´ë¸” ì œì™¸ ë“± ì™„ì „ ì»¤ìŠ¤í…€ ê°€ëŠ¥
  - ì •í•©ì„± ê²€ì¦ ë° ì¬ì²˜ë¦¬ ë¡œì§ ì§ì ‘ ì‚½ì… ê°€ëŠ¥
    - ì˜ˆ: fallback, alert, dead-letter ë“±
  - í…ŒìŠ¤íŠ¸/ì‹œë®¬ë ˆì´ì…˜ ìš©ì´

- ë‹¨ì 
  - êµ¬í˜„ ë° ìœ ì§€ë³´ìˆ˜ ë³µì¡
    - offset ê´€ë¦¬ / ì¥ì•  ë³µêµ¬ / ì¬ì‹œì‘ ì‹œ replay ì²˜ë¦¬ ë“±ë„ ì§ì ‘ ê³ ë ¤í•´ì•¼ í•¨
  - ëŒ€ëŸ‰ ì´ë²¤íŠ¸ ì²˜ë¦¬ ì„±ëŠ¥(ë³‘ë ¬ì„±, batch write) íŠœë‹ í•„ìš”
  - ì¦‰, ì™„ì „í•œ â€œì œì–´ê¶Œâ€ì„ ê°–ì§€ë§Œ, ê·¸ë§Œí¼ â€œìš´ì˜ ì±…ì„â€ë„ ì „ë¶€ ë³¸ì¸ì´ ì§Šì–´ì§€ê²Œ ë©ë‹ˆë‹¤.

**Kafka Connect JDBC Sink Connector ì‚¬ìš©í•˜ëŠ” ê²½ìš°**

- Debezium Source Connector â†’ Kafka Topic â†’ JDBC Sink Connector
- ì´ Sinkê°€ Target DBì— **ìë™ìœ¼ë¡œ upsert/merge ìˆ˜í–‰**í•©ë‹ˆë‹¤.
- Debezium (source) â†’ Kafka (topic) â†’ JDBC Sink â†’ MySQL (target)

- ê¸°ë³¸ ë™ì‘ ë°©ì‹
  - INSERT, UPDATE, DELETE ì´ë²¤íŠ¸ë¥¼ ëª¨ë‘ ì¸ì‹
  - `insert.mode` ì„¤ì •ì— ë”°ë¼ ë‹¤ë¥¸ ë™ì‘ ìˆ˜í–‰:
    - insert : ë‹¨ìˆœ insert
    - upsert : PK ì¤‘ë³µ ì‹œ update
    - replace : ì „ì²´ row ë®ì–´ì“°ê¸°

- ì˜ˆì‹œ config (sink.properties)

```
name=mysql-sink
connector.class=io.confluent.connect.jdbc.JdbcSinkConnector
topics=dbserver1.mydb.user
connection.url=jdbc:mysql://target-db:3306/mydb
connection.user=root
connection.password=****
auto.create=true
auto.evolve=true
insert.mode=upsert
pk.mode=record_key
pk.fields=id
fields.whitelist=id,name,age,updated_at
delete.enabled=true
```

- ì´ ì„¤ì •ì´ë©´:
  - CREATE â†’ INSERT ... ON DUPLICATE KEY UPDATE
  - UPDATE â†’ ë™ì¼ PK update
  - DELETE â†’ DELETE FROM ... WHERE id=?
  - í˜•íƒœë¡œ ìë™ ì²˜ë¦¬ë©ë‹ˆë‹¤.

- Sink Connector vs Custom Consumer ë¹„êµ

| í•­ëª©                    | Sink Connector           | ì§ì ‘ Consumer |
| --------------------- | ------------------------ | ----------- |
| **ê¸°ë³¸ ë™ì‘**             | ìë™ upsert/delete         | ì§ì ‘ êµ¬í˜„       |
| **ë³€ê²½ì¼ì‹œ ë¹„êµ**           | âŒ ê¸°ë³¸ ë¯¸ì§€ì› (DBì—ì„œ ìë™ ì—…ë°ì´íŠ¸ë¨) | âœ… ê°€ëŠ¥        |
| **PK ì¡´ì¬ ì—¬ë¶€ íŒë‹¨**       | ìë™ ì²˜ë¦¬                    | ì§ì ‘ ì²˜ë¦¬       |
| **ì •í•©ì„± ì»¤ìŠ¤í…€ ë¡œì§**        | âŒ ì œí•œì                     | âœ… ììœ         |
| **ìš´ì˜ ë‚œì´ë„**            | ë‚®ìŒ                       | ë†’ìŒ          |
| **ì„±ëŠ¥ (ëŒ€ëŸ‰ ì²˜ë¦¬)**        | âœ… ë³‘ë ¬, batch ì²˜ë¦¬           | âš™ï¸ ì¡°ì • í•„ìš”    |
| **Offset ê´€ë¦¬ / ì¥ì•  ë³µêµ¬** | ìë™ (Connect í”„ë ˆì„ì›Œí¬)       | ì§ì ‘ êµ¬í˜„ í•„ìš”    |

- Sink Connectorì—ì„œë„ "ë³€ê²½ì¼ì‹œ ë¹„êµ"ë¥¼ êµ¬í˜„í•  ìˆ˜ëŠ” ìˆë‹¤
  - Kafka Connect JDBC SinkëŠ” `insert.mode=upsert` ì™¸ì—ë„
  - ì»¤ìŠ¤í…€ upsert SQL í…œí”Œë¦¿ì„ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```
insert.mode=upsert
pk.mode=record_key
pk.fields=id
delete.enabled=true
upsert.sql=MERGE INTO user AS t
USING (SELECT ? AS id, ? AS name, ? AS updated_at) AS s
ON (t.id = s.id)
WHEN MATCHED AND t.updated_at < s.updated_at THEN
UPDATE SET name=s.name, updated_at=s.updated_at
WHEN NOT MATCHED THEN
INSERT (id, name, updated_at) VALUES (s.id, s.name, s.updated_at);
```

- ì¦‰, Sink Connectorë¥¼ ì¨ë„, DB ë ˆë²¨ì—ì„œ â€œupdated_at ë¹„êµâ€ë¥¼ SQLì— ë‚´ì¥ì‹œì¼œ idempotent merge ë¡œì§ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ê·¸ëŸ¬ë©´ ë³„ë„ì˜ consumer ì—†ì´, Debezium + Sink Connectorë¡œ ì™„ì „ ìë™í™” ê°€ëŠ¥

### ë³€ê²½ ì´ë²¤íŠ¸ ë°œí–‰ ìˆœì„œê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‚˜ ??
> ë™ì¼í•œ PKì— ëŒ€í•œ DML 1,2,3 ì¸ë° ë™ì¼í•œ íŒŒí‹°ì…˜ì— ì´ë²¤íŠ¸ê°€ 2,1,3 ì´ëŸ°ì‹ìœ¼ë¡œ ë„ì°©í•  ìˆ˜ ìˆë‚˜ ?

- Debezium + Kafka êµ¬ì¡°ì—ì„œëŠ” ë™ì¼í•œ PK(Row)ì— ëŒ€í•œ ì»¤ë°‹ ìˆœì„œê°€ Kafka íŒŒí‹°ì…˜ì—ì„œ ë’¤ë°”ë€ŒëŠ” ì¼ì€ ì—†ìŠµë‹ˆë‹¤.
- ì¦‰, ê°™ì€ rowì— ëŒ€í•œ ë³€ê²½ ì´ë²¤íŠ¸ëŠ” commit ìˆœì„œëŒ€ë¡œ (1 â†’ 2 â†’ 3), Kafka íŒŒí‹°ì…˜ì— ìˆœì„œëŒ€ë¡œ publishë©ë‹ˆë‹¤.
- Debeziumì€ binlog ì´ë²¤íŠ¸ë¥¼ ì½ì„ ë•Œ, â€œíŠ¸ëœì­ì…˜ ë‹¨ìœ„(transactional order)â€ë¥¼ ìœ ì§€í•´ì„œ ì»¤ë°‹ ìˆœì„œëŒ€ë¡œ publish í•©ë‹ˆë‹¤.

- Debeziumì€ ê¸°ë³¸ì ìœ¼ë¡œ Kafkaì— recordë¥¼ ë³´ë‚¼ ë•Œ partition keyë¥¼ ì•„ë˜ì²˜ëŸ¼ ê³„ì‚°í•©ë‹ˆë‹¤:
`partition_key = hash(primary_key_values)`

- ì¦‰, ê°™ì€ PK ê°’ì€ í•­ìƒ ê°™ì€ partitionìœ¼ë¡œ ê°‘ë‹ˆë‹¤.
- KafkaëŠ” ê°™ì€ partition ë‚´ì—ì„œëŠ” record orderë¥¼ ì ˆëŒ€ ë°”ê¾¸ì§€ ì•ŠìŠµë‹ˆë‹¤.
- ê·¸ë¦¬ê³  Debeziumì˜ producerëŠ” binlog ì´ë²¤íŠ¸ ìˆœì„œëŒ€ë¡œ Kafkaì— publishí•˜ë©°, Kafka clientëŠ” ë‚´ë¶€ì ìœ¼ë¡œ 1 partitionë‹¹ ë‹¨ì¼ append queueë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.

**=> producerëŠ” ìˆœì„œëŒ€ë¡œ publish (ì˜ˆ : 1->2->3) í–ˆëŠ”ë°, TCP ë„¤íŠ¸ì›Œí¬ ë ˆë²¨ì—ì„œ 2,1,3 ì´ëŸ°ì‹ìœ¼ë¡œ kafkaì— ìˆœì„œê°€ ë°”ê»´ì„œ ë„ì°©í•  ìˆ˜ë„ ìˆëŠ”ê±° ì•„ë‹ˆì•¼ ?**

- TCP ë ˆë²¨ì—ì„œë¼ë„ ìˆœì„œê°€ ë’¤ë°”ë€Œì–´ Kafka brokerì— 2,1,3 ìˆœì„œë¡œ ë“¤ì–´ê°€ëŠ” ì¼ì€ ì—†ìŠµë‹ˆë‹¤.
- TCPëŠ” ë°”ì´íŠ¸ ìŠ¤íŠ¸ë¦¼ ë ˆë²¨ì—ì„œì˜ ì „ì†¡ ìˆœì„œë¥¼ ë³´ì¥í•˜ê³ , Kafka Producer í´ë¼ì´ì–¸íŠ¸ëŠ” ë‹¨ì¼ íŒŒí‹°ì…˜ì— ëŒ€í•´ ìˆœì°¨ì  ì „ì†¡(ì‹±ê¸€ ì¸í”Œë¼ì´íŠ¸ ê·œì¹™)ì„ ì ìš©í•˜ê¸° ë•Œë¬¸ì— ê°™ì€ partition ë‚´ recordì˜ ìˆœì„œëŠ” ì ˆëŒ€ ê¹¨ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.

- TCPëŠ” ë‹¤ìŒ ë‘ ê°€ì§€ë¥¼ í•­ìƒ ë³´ì¥í•©ë‹ˆë‹¤:
  - ì‹ ë¢°ì„± (íŒ¨í‚· ì†ì‹¤ ì‹œ ì¬ì „ì†¡)
  - ìˆœì„œ ë³´ì¥ (ë°”ì´íŠ¸ ìŠ¤íŠ¸ë¦¼ ìˆœì„œ ê·¸ëŒ€ë¡œ ë„ì°©)

- ì¦‰, Debezium Producer â†’ Kafka Broker ê°„ì˜ í†µì‹ ì€ TCP ìœ„ì—ì„œ ì´ë¤„ì§€ë¯€ë¡œ, 1â†’2â†’3 ìˆœì„œë¡œ ë³´ë‚¸ ë°ì´í„°ê°€ ë„¤íŠ¸ì›Œí¬ì—ì„œ 2â†’1â†’3 ìˆœì„œë¡œ ë„ì°©í•˜ëŠ” ì¼ì€ ì—†ìŠµë‹ˆë‹¤.
- TCPì˜ sequence numberì™€ ì¬ì „ì†¡ ë©”ì»¤ë‹ˆì¦˜ì´ ìˆœì„œë¥¼ ë³´ì¡´í•´ì¤ë‹ˆë‹¤.
- TCPëŠ” â€œin-order, reliable byte streamâ€ì„ ë³´ì¥í•˜ë¯€ë¡œ, íŒ¨í‚· ìˆœì„œê°€ ì–´ê¸‹ë‚˜ëŠ” ì¼ì€ ì»¤ë„ ë ˆë²¨ì—ì„œ ì´ë¯¸ ì •ë ¬ë©ë‹ˆë‹¤.

- Kafkaì˜ ProducerëŠ” ë‚´ë¶€ì ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê±°ì¹©ë‹ˆë‹¤:

```
RecordAccumulator â†’ (ë°°ì¹˜ ì „ì†¡) â†’ Kafka Broker Partition Leader
```

- ê°™ì€ Partitionì— ì†í•œ recordëŠ” í•˜ë‚˜ì˜ FIFO í(batch) ë¡œ ëˆ„ì ë©ë‹ˆë‹¤.
- `max.in.flight.requests.per.connection=1` ì´ë©´,
  - í•œ ë²ˆì— í•˜ë‚˜ì˜ ë°°ì¹˜(batch)ë§Œ ë¸Œë¡œì»¤ë¡œ ë³´ëƒ…ë‹ˆë‹¤.
  - ì¦‰, 1ë²ˆ ë°°ì¹˜ê°€ ACK ì˜¤ê¸° ì „ê¹Œì§€ 2ë²ˆ ë°°ì¹˜ëŠ” ì ˆëŒ€ ì „ì†¡ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

- ë”°ë¼ì„œ ë¸Œë¡œì»¤ ì¸¡ì— ë„ì°©í•˜ëŠ” ìˆœì„œë„ í•­ìƒ Producer ì „ì†¡ ìˆœì„œì™€ ë™ì¼í•©ë‹ˆë‹¤.

```
Producer API (send í˜¸ì¶œ)
        â†“
RecordAccumulator (ë²„í¼ì— ìŒ“ìŒ)
        â†“
Sender Thread (ë°°ì¹˜ ë‹¨ìœ„ë¡œ flush)
        â†“
TCP ì „ì†¡ â†’ Broker
```

```
Producer: [1, 2, 3, 4, 5] â†’ í•œ ë°°ì¹˜ë¡œ ì „ì†¡
Broker:   append(1, 2, 3, 4, 5)
```

- TCPëŠ” ì´ ë°”ì´íŠ¸ ìŠ¤íŠ¸ë¦¼ ìˆœì„œë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë¯€ë¡œ ê²°êµ­ Kafka logì—ë„ ê°™ì€ ìˆœì„œë¡œ appendë©ë‹ˆë‹¤.


### ì„œë¹„ìŠ¤ í•œë²ˆë„ ì•ˆë©ˆì¶”ê³  í•  ìˆ˜ ìˆëŠ” ë°©ë²• ??



### ì´ë²¤íŠ¸ ìœ ì‹¤ë ë•Œ ??

### í† í”½ / íŒŒí‹°ì…˜ êµ¬ì„± ..
- ê¸°ë³¸ì ìœ¼ë¡œ debezium source connectorì—ì„œ í† í”½ì€ topic.prefixì™€ í…Œì´ë¸”ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ ìë™ìƒì„±í•¨
- Kafkaì—ì„œ ìë™ìƒì„± í—ˆìš©í•´ì¤˜ì•¼ë¨
- ê·¼ë° ë³´í†µ ìš´ì˜ì—ì„œ ìë™ìƒì„± ì•ˆí•˜ê¸´ í•˜ëŠ” ê²ƒ ê°™ë˜ë°

### ê° ì»´í¬ë„ŒíŠ¸ ì¥ì•  ìƒí™©
- connector, kafka, consumer, target db, binlog ì§€ì›Œì§€ë©´ ? ë“±

### PDIë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜ ë˜ëŠ”ë™ì•ˆ ë³€ê²½ ì´ë²¤íŠ¸ë¥¼ ë¶™ì¡ê³ (?)ìˆëŠ”ê±´ ì–´ë–»ê²Œ í•´ì•¼ë¼ ?
> ì¦‰, CDCëŠ” ê³„ì† ì½ë˜, Targetì—ëŠ” ë°˜ì˜í•˜ì§€ ì•Šê³  ëŒ€ê¸°ì‹œì¼œì•¼ í•œë‹¤

**Sink Connector ì‚¬ìš©í•˜ëŠ” ê²½ìš°**
- Sink Connector ë©ˆì¶°ë†“ê¸°

```
curl -X PUT localhost:8083/connectors/mysql-sink/pause
# ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ í›„
curl -X PUT localhost:8083/connectors/mysql-sink/resume
```

**ì§ì ‘ êµ¬í˜„í•œ ê²½ìš°**

Spring Kafkaì˜ `KafkaMessageListenerContainer` ë˜ëŠ” `ConcurrentMessageListenerContainer` ê°ì²´ì—ëŠ”
Kafkaì˜ native pause/resume ê¸°ëŠ¥ì´ ê·¸ëŒ€ë¡œ ë…¸ì¶œë˜ì–´ ìˆìŠµë‹ˆë‹¤.

```java
@Autowired
private KafkaListenerEndpointRegistry registry;

public void pauseSink() {
    registry.getListenerContainer("my-sink-listener").pause();
}

public void resumeSink() {
    registry.getListenerContainer("my-sink-listener").resume();
}
```

- `@KafkaListener(id = "my-sink-listener", topics = "dbserver1.mydb.user")`
- ì´ë ‡ê²Œ idë¥¼ ì§€ì •í•´ë‘ë©´, ë‚˜ì¤‘ì— registryë¡œ ì ‘ê·¼ ê°€ëŠ¥í•©ë‹ˆë‹¤.
- ì´ ë°©ë²•ì€ Kafka Consumer Group ë ˆë²¨ì—ì„œ ì‹¤ì œë¡œ poll()ì„ ë©ˆì¶”ëŠ” ë°©ì‹ì´ë¼,
- Broker ì…ì¥ì—ì„œëŠ” â€œì´ Consumerê°€ ë” ì´ìƒ ë°ì´í„°ë¥¼ ì½ì§€ ì•ŠìŒâ€ ìƒíƒœê°€ ë©ë‹ˆë‹¤.
â†’ ë©”ì‹œì§€ëŠ” Kafka topicì— ê³„ì† ìŒ“ì…ë‹ˆë‹¤ (offsetë„ ì¦ê°€í•˜ì§€ ì•ŠìŒ).

| í•­ëª©                        | ì„¤ëª…                                                     |
| ------------------------- | ------------------------------------------------------ |
| **Kafka Topic Retention** | pauseëœ ë™ì•ˆì—ë„ Kafkaì˜ `retention.ms` ì„¤ì •ë³´ë‹¤ ì˜¤ë˜ ìŒ“ì´ë©´ ì‚­ì œë¨ (ì£¼ì˜) |
| **Consumer Group Lag**    | pause ë™ì•ˆ lagê°€ í­ì¦í•  ìˆ˜ ìˆìŒ (ì •ìƒ í˜„ìƒ)                         |
| **ìˆœì„œ ë³´ì¥**                 | partitionë³„ ìˆœì„œ ìœ ì§€ë¨ (pause/resumeë¡œëŠ” ê¹¨ì§€ì§€ ì•ŠìŒ)              |
| **Rebalance ì£¼ì˜**          | stop()/start()ë³´ë‹¨ pause()/resume() ì¶”ì²œ                   |


### ë§ˆì´ê·¸ë ˆì´ì…˜ ë™ì•ˆ ìŒ“ì¸ DML ì´ë²¤íŠ¸ê°€ ë§ì€ ê²½ìš° ì´ìŠˆ ?
> Kafka ì´ìŠˆ ? target mysql ì´ìŠˆ ? ëŒ€ì—­í­ ?

snapshotì´ ì™„ë£Œë  ë•Œê¹Œì§€ binlogë¥¼ ì½ê¸´ í•˜ì§€ë§Œ
ì•„ì§ Kafkaë¡œ â€œì ìš©(commit)â€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

ì¦‰, binlog ì´ë²¤íŠ¸ëŠ” ì»¤ë„¥í„° ë‚´ë¶€ì˜ í(ë©”ëª¨ë¦¬ ë²„í¼)ì— ì ì‹œ ìŒ“ì´ê±°ë‚˜,
Kafka ì „ì†¡ì´ snapshot ì´í›„ì— ëª°ë ¤ ë°œìƒí•©ë‹ˆë‹¤.

ë”°ë¼ì„œ snapshotì´ ìˆ˜ì‹œê°„ ê±¸ë¦°ë‹¤ë©´,
â†’ ê·¸ë™ì•ˆ ë°œìƒí•œ DML ì´ë²¤íŠ¸ê°€ ìˆ˜ë°±ë§Œ ê±´ ì´ìƒ backlogë¡œ ìŒ“ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ§¨ 2ï¸âƒ£ ì£¼ìš” ë¬¸ì œì 
(1) Kafka ë¸Œë¡œì»¤ ë””ìŠ¤í¬ ì••ë°•

snapshot ì™„ë£Œ í›„ í•œêº¼ë²ˆì— DML ì´ë²¤íŠ¸ê°€ flushë˜ë©´
íŠ¹ì • partitionì— ë‹¨ê¸°ê°„ massive writeê°€ ë°œìƒí•©ë‹ˆë‹¤.

ë§Œì•½ topic retentionì´ ì§§ê±°ë‚˜ segment size ì„¤ì •ì´ ì‘ìœ¼ë©´,
disk IO burst + log segment churnì´ ì¼ì–´ë‚©ë‹ˆë‹¤.

(2) Consumer Lag í­ì¦

Downstream (ì˜ˆ: SinkConnector, Stream Processor)ì´
snapshot ì¢…ë£Œ í›„ ê°‘ìê¸° ëŒ€ëŸ‰ ì´ë²¤íŠ¸ë¥¼ ì²˜ë¦¬í•´ì•¼ í•˜ë¯€ë¡œ
consumer lagì´ ìˆ˜ì‹­~ìˆ˜ë°±ë§Œ ê±´ìœ¼ë¡œ íŠ€ì–´ì˜¤ë¦„.

ì´ ì‹œì ì— TPSê°€ í‰ì†Œë³´ë‹¤ 10ë°° ì´ìƒìœ¼ë¡œ ì¦ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

(3) ë©”ëª¨ë¦¬/GC ë¶€í•˜ (Connector ë‚´ë¶€)

snapshot ë™ì•ˆ bufferëœ ì´ë²¤íŠ¸ê°€ ë§ìœ¼ë©´
ì»¤ë„¥í„° JVM heapì´ ê¸‰ê²©íˆ ì¦ê°€, GC pause ìœ ë°œ ê°€ëŠ¥.

íŠ¹íˆ snapshot.fetch.size ë˜ëŠ” max.batch.size ê¸°ë³¸ê°’ì´ í´ ë•Œ ì‹¬ê°í•´ì§.

(4) ë°ì´í„° ìˆœì„œ ì—­ì „ ê°€ëŠ¥ì„±

snapshot ì™„ë£Œ ì§ì „ê¹Œì§€ DMLì´ ì§€ì†ì ìœ¼ë¡œ ë°œìƒí•˜ë©´,
snapshotì˜ íŠ¹ì • rowë³´ë‹¤ ë” â€œì˜¤ë˜ëœâ€ ë³€ê²½ ì´ë²¤íŠ¸ê°€ ë‚˜ì¤‘ì— ë“¤ì–´ì˜¬ ìˆ˜ ìˆìŒ.
â†’ Downstreamì—ì„œ merge logicì´ í•„ìš”í•  ìˆ˜ë„ ìˆìŒ.

### ë§ˆì´ê·¸ë ˆì´ì…˜ ëë‚œ í…Œì´ë¸”ì€ ê³„ì† ì´ë²¤íŠ¸ ìŒ“ì§€ì•Šê³ , íì—ì„œ ì†Œì§„ë˜ê²Œ í•  ìˆ˜ ìˆì„ê¹Œ ?? (kafka ë¦¬ì†ŒìŠ¤ ê³„ì† ì¡ì•„ë¨¹ê²Œë˜ë‹ˆê¹Œ)
> PDIì—ì„œ ë§ˆì´ê·¸ë ˆì´ì…˜ ëë‚˜ê¸°ê¹Œì§€ ì´ë²¤íŠ¸ê°€ ê³„ì† ìŒ“ì´ëŠ”ê²ƒ ë³´ë‹¤ëŠ”, ì™„ë£Œëœ í…Œì´ë¸”ì€ ë¨¼ì € CDC ì´ë²¤íŠ¸ ë°˜ì˜í•´ë„ ë  ê²ƒ ê°™ì€ë°, pause, resumeì„ í† í”½ë³„ë¡œ ë‹¤ë¥´ê²Œ í•  ìˆ˜ë„ ìˆë‚˜ ?

- Debeziumì˜ ê¸°ë³¸ ë™ì‘ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

| í…Œì´ë¸”     | Kafka Topic              |
| ------- | ------------------------ |
| user    | `dbserver1.mydb.user`    |
| order   | `dbserver1.mydb.order`   |
| product | `dbserver1.mydb.product` |

- ì¦‰, í…Œì´ë¸” ë‹¨ìœ„ë¡œ Topicì´ ë¶„ë¦¬ë˜ì–´ ìˆê¸° ë•Œë¬¸ì—
  - í† í”½ ë‹¨ìœ„ë¡œ consumerë¥¼ ë¶„ë¦¬í•˜ê±°ë‚˜,
  - ë™ì¼ ì»¨ìŠˆë¨¸ ê·¸ë£¹ì—ì„œ íŠ¹ì • í† í”½ë§Œ pause í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```java
@Autowired
private KafkaListenerEndpointRegistry registry;

public void pauseTopic(String listenerId, String topicName) {
    var container = registry.getListenerContainer(listenerId);
    container.getAssignedPartitions().stream()
        .filter(tp -> tp.topic().equals(topicName))
        .forEach(tp -> container.pause(Collections.singleton(tp)));
}

public void resumeTopic(String listenerId, String topicName) {
    var container = registry.getListenerContainer(listenerId);
    container.getAssignedPartitions().stream()
        .filter(tp -> tp.topic().equals(topicName))
        .forEach(tp -> container.resume(Collections.singleton(tp)));
}
```

- ë˜ëŠ” í† í”½ë³„ë¡œ ë¦¬ìŠ¤ë„ˆ ë¶„ë¦¬ë„ ê°€ëŠ¥

```java
@KafkaListener(id = "user-sink", topics = "dbserver1.mydb.user")
public void consumeUser(...) { ... }

@KafkaListener(id = "order-sink", topics = "dbserver1.mydb.order")
public void consumeOrder(...) { ... }
```


## ê³µë¶€
---

### `SELECT * FROM <table>` => ìš´ì˜ DBì— ë¶€í•˜
- Repeatable Read ì˜¤ë˜ ì¡ê³ ìˆëŠ”ë‹¤ ? => I/O ë¶€ë‹´ í¼ ??
ì–´ë–¤ ë¶€í•˜ , ë¶€ë‹´ ?

### PDI ?
PDI (Pentaho Data Integration)ëŠ” ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ì´ë‚˜ ETL(Extract, Transform, Load) ì‘ì—…ì„ ìë™í™”í•˜ê¸° ìœ„í•œ ëŒ€í‘œì ì¸ ì˜¤í”ˆì†ŒìŠ¤ ë„êµ¬ì…ë‹ˆë‹¤.
ë³´í†µ â€œPentahoâ€ë¼ê³  í•˜ë©´ BI(ë¹„ì¦ˆë‹ˆìŠ¤ ì¸í…”ë¦¬ì „ìŠ¤) ì „ì²´ í”Œë«í¼ì„ ë§í•˜ê³ ,
ê·¸ ì¤‘ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ë¶€ë¶„ë§Œ ë”°ë¡œ ë–¼ì–´ë‚¸ ê²Œ PDIì…ë‹ˆë‹¤.

ê°„ë‹¨íˆ ë§í•˜ë©´ ğŸ‘‡

â€œSQLì´ë‚˜ ìŠ¤í¬ë¦½íŠ¸ë¡œ ë°ì´í„° ì˜®ê¸°ë˜ ê±¸ ì‹œê°ì ìœ¼ë¡œ ì„¤ê³„í•´ì„œ, ë³‘ë ¬ë¡œ ë¹ ë¥´ê³  ì•ˆì •ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ê²Œ í•´ì£¼ëŠ” íˆ´â€

| êµ¬ì„± ìš”ì†Œ              | ì—­í•                                                            | ë¹„ìœ              |
| ------------------ | ------------------------------------------------------------ | -------------- |
| **Transformation** | ë°ì´í„°ë¥¼ ì¶”ì¶œ(Extract), ë³€í™˜(Transform), ì ì¬(Load)í•˜ëŠ” **ë‹¨ì¼ ë°ì´í„° íë¦„ ë‹¨ìœ„** | â€œSQL ì¿¼ë¦¬ í•œ ë©ì–´ë¦¬â€ |
| **Job**            | ì—¬ëŸ¬ Transformationì„ ìˆœì°¨/ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ëŠ” **ì›Œí¬í”Œë¡œìš° ë‹¨ìœ„**                  | â€œë°°ì¹˜ ìŠ¤í¬ë¦½íŠ¸â€      |

Transformation: MySQL â†’ CSVë¡œ export, ë°ì´í„° ì •ì œ

Job: â€œ1. export â†’ 2. íŒŒì¼ ì „ì†¡ â†’ 3. ì™„ë£Œ ë¡œê·¸ ê¸°ë¡â€

| ê¸°ëŠ¥                             | ì„¤ëª…                                                                              |
| ------------------------------ | ------------------------------------------------------------------------------- |
| **Extract (ë°ì´í„° ì¶”ì¶œ)**           | MySQL, Oracle, PostgreSQL, Cubrid, CSV, Excel, JSON, REST API ë“± ê±°ì˜ ëª¨ë“  ë°ì´í„° ì†ŒìŠ¤ ì§€ì› |
| **Transform (ë³€í™˜)**             | í•„ë“œ ë§¤í•‘, í˜•ë³€í™˜, Null ì²˜ë¦¬, ì¡°ê±´ ë¶„ê¸°, ì¡°ì¸, ì§‘ê³„, í•„í„°ë§ ë“± GUI ê¸°ë°˜ ë°ì´í„° ë³€í™˜                         |
| **Load (ì ì¬)**                  | RDBMS, íŒŒì¼, Kafka, MongoDB, Elastic ë“± ë‹¤ì–‘í•œ íƒ€ê²Ÿìœ¼ë¡œ ë¡œë”© ê°€ëŠ¥                             |
| **ë³‘ë ¬ ì²˜ë¦¬ (Parallel Execution)** | í…Œì´ë¸”/íŒŒí‹°ì…˜ ë‹¨ìœ„ë¡œ ë³‘ë ¬ Extract/Load ê°€ëŠ¥                                                  |
| **ì—ëŸ¬ ì²˜ë¦¬ / ë¡œê¹…**                 | stepë³„ ì‹¤íŒ¨ ì²˜ë¦¬, ì¬ì‹œë„, ì˜¤ë¥˜ ë¡œê·¸ íŒŒì¼ ê¸°ë¡ ê¸°ëŠ¥ ë‚´ì¥                                             |
| **ìŠ¤ì¼€ì¤„ë§ / ìë™í™”**                 | cron, Carte server, Kitchen(PDI CLI) ë“±ìœ¼ë¡œ ìë™ ì‹¤í–‰ ê°€ëŠ¥                               |


- ëŒ€ì²´ ë„êµ¬ ë¹„êµ

| ë„êµ¬                      | íŠ¹ì§•                    | ë¹„ê³               |
| ----------------------- | --------------------- | --------------- |
| **PDI (Pentaho)**       | ì•ˆì •ì , GUI ê¸°ë°˜, ë²”ìš©       | ì˜¤í”ˆì†ŒìŠ¤, ìƒìš© ì§€ì›ë„ ìˆìŒ |
| **Apache Nifi**         | ìŠ¤íŠ¸ë¦¬ë° ETL, ì‹¤ì‹œê°„ íŒŒì´í”„ë¼ì¸   | CDCì— ë” ê°€ê¹Œì›€      |
| **Airbyte / Singer.io** | ëª¨ë˜ ETL í”„ë ˆì„ì›Œí¬, YAML ê¸°ë°˜ | CDC/Batch ê²¸ìš©    |
| **AWS DMS**             | í´ë¼ìš°ë“œ ê¸°ë°˜ ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ë¬¸     | ìë™í™” ì‰¬ì›€, ìœ ë£Œ      |
| **DataX (Alibaba)**     | ì´ˆëŒ€ìš©ëŸ‰ ë°ì´í„° ë³‘ë ¬ ë³µì‚¬ì— íŠ¹í™”    | êµ­ë‚´ì—ì„œë„ ì¢…ì¢… ì‚¬ìš©ë¨    |


### TCPëŠ” â€œin-order, reliable byte streamâ€ì„ ë³´ì¥ ?
- TCPì˜ sequence numberì™€ ì¬ì „ì†¡ ë©”ì»¤ë‹ˆì¦˜ì´ ìˆœì„œë¥¼ ë³´ì¡´í•´ì¤ë‹ˆë‹¤.
- TCPëŠ” â€œin-order, reliable byte streamâ€ì„ ë³´ì¥í•˜ë¯€ë¡œ, íŒ¨í‚· ìˆœì„œê°€ ì–´ê¸‹ë‚˜ëŠ” ì¼ì€ ì»¤ë„ ë ˆë²¨ì—ì„œ ì´ë¯¸ ì •ë ¬ë©ë‹ˆë‹¤.

??

### ì‹±ê¸€ ì¸í”Œë¼ì´íŠ¸(Single In-Flight) ê·œì¹™
  - Kafka Producerê°€ í•˜ë‚˜ì˜ TCP ì—°ê²°(connection) ì—ì„œ í•œ ë²ˆì— ì˜¤ì§ í•˜ë‚˜ì˜ ìš”ì²­(batch) ë§Œ â€œë‚ ì•„ê°€ë„ë¡â€ ì œì–´í•˜ëŠ” ê·œì¹™ì´ì—ìš”.
  - ì´ê±´ â€œìˆœì„œ ë³´ì¥ì„ ìœ„í•´ ì„±ëŠ¥ì„ í¬ìƒí•˜ëŠ” ì„¤ì •â€

ë¨¼ì €, â€œin-flight requestâ€ê°€ ë­ëƒ?

Kafka ProducerëŠ” ë°°ì¹˜(batch)ë¥¼ ë§Œë“¤ì–´ Brokerë¡œ ì „ì†¡í•  ë•Œ,
ê·¸ ê²°ê³¼(ack)ë¥¼ ê¸°ë‹¤ë¦¬ê¸° ì „ì— ë‹¤ìŒ ë°°ì¹˜ë¥¼ ë³´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ë•Œ

ì´ë¯¸ ë³´ëƒˆì§€ë§Œ ì•„ì§ ì‘ë‹µ(ack)ì„ ë°›ì§€ ì•Šì€ ìš”ì²­ë“¤ì„ in-flight requests ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤.

ì¦‰, â€œë‚ ì•„ê°€ ìˆëŠ” ìš”ì²­ë“¤â€ì´ì—ìš”

âš™ï¸ 2ï¸âƒ£ max.in.flight.requests.per.connection

Kafka Producer ì„¤ì • ì¤‘ í•˜ë‚˜ê°€ ë°”ë¡œ ì´ê²ë‹ˆë‹¤

| ê°’      | ì˜ë¯¸                                   |
| ------ | ------------------------------------ |
| **1**  | í•œ ë²ˆì— í•˜ë‚˜ì˜ ìš”ì²­ë§Œ ë³´ëƒ„ (ì‹±ê¸€ ì¸í”Œë¼ì´íŠ¸ ê·œì¹™)        |
| **>1** | ì—¬ëŸ¬ ìš”ì²­ì„ ë™ì‹œì— ë³´ë‚¼ ìˆ˜ ìˆìŒ (ì„±ëŠ¥â†‘, ìˆœì„œ ê¼¬ì¼ ê°€ëŠ¥ì„±â†‘) |

- ì‹±ê¸€ ì¸í”Œë¼ì´íŠ¸ (=1) ì¸ ê²½ìš°

```
Producer: send(batch#1)
â†’ ê¸°ë‹¤ë¦¼ (ack)
â†’ ack ë°›ìŒ
â†’ send(batch#2)
â†’ ê¸°ë‹¤ë¦¼ (ack)
â†’ ack ë°›ìŒ
â†’ send(batch#3)
...
```

- í•­ìƒ ë³´ë‚´ê³  â†’ ì‘ë‹µë°›ê³  â†’ ë‹¤ìŒê±° ì „ì†¡
  - ë”°ë¼ì„œ ìˆœì„œê°€ ì ˆëŒ€ ê¼¬ì´ì§€ ì•ŠìŒ
  - í•˜ì§€ë§Œ ë³‘ë ¬ì„±ì´ ë‚®ìŒ

- ë©€í‹° ì¸í”Œë¼ì´íŠ¸ (=2) ì¸ ê²½ìš°

```
Producer: send(batch#1)
â†’ ì•„ì§ ack ì•ˆ ì™”ì§€ë§Œ send(batch#2)
â†’ ack#2 ë¨¼ì € ë„ì°©
â†’ ack#1 ë‚˜ì¤‘ ë„ì°©
```

- ì´ëŸ¬ë©´ Kafka Producer ë‚´ë¶€ì—ì„  â€œ2ë²ˆì´ ë¨¼ì € ì„±ê³µ, 1ë²ˆì´ ë‚˜ì¤‘ ì„±ê³µâ€ ì´ë€ ì‚¬ì‹¤ë§Œ ì•Œê²Œ ë©ë‹ˆë‹¤.
  â†’ ê·¸ëŸ°ë° ë§Œì•½ retries > 0 ì¸ ìƒíƒœì—ì„œ 1ë²ˆ ì „ì†¡ ì¤‘ ì˜¤ë¥˜ê°€ ë‚˜ë©´?

1ë²ˆ ë°°ì¹˜ë¥¼ ì¬ì „ì†¡í•˜ê²Œ ë˜ê³ ,
ê·¸ ì¬ì „ì†¡ëœ 1ë²ˆì´ ì´ë¯¸ ack ë°›ì€ 2ë²ˆë³´ë‹¤ ëŠ¦ê²Œ Brokerì— ê¸°ë¡ë  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

Producer ì „ì†¡ ìˆœì„œ: 1 â†’ 2
Broker ê¸°ë¡ ìˆœì„œ: 2 â†’ 1  âŒ (ìˆœì„œ ì—­ì „)
â¡ï¸ ì´ë ‡ê²Œ ë˜ë©´ ë™ì¼ partition ë‚´ì—ì„œ ìˆœì„œ ê¼¬ì„ ë°œìƒ ê°€ëŠ¥
â¡ï¸ ê·¸ê±¸ ë°©ì§€í•˜ë ¤ë©´ â€œì‹±ê¸€ ì¸í”Œë¼ì´íŠ¸ ê·œì¹™â€ì„ ì ìš©í•´ì•¼ í•©ë‹ˆë‹¤.

ì‹¤ì œ Kafkaì—ì„œëŠ” ì•„ë˜ ì„¸ ê°€ì§€ ì„¤ì •ì´ í•­ìƒ ì„¸íŠ¸ë¡œ ë™ì‘í•©ë‹ˆë‹¤:

```
max.in.flight.requests.per.connection=1
acks=all
enable.idempotence=true
```

| ì„¤ì •                                        | ì—­í•                                              |
| ----------------------------------------- | ---------------------------------------------- |
| `max.in.flight.requests.per.connection=1` | ì‹±ê¸€ ì¸í”Œë¼ì´íŠ¸ â€” ìˆœì„œ ë³´ì¥                               |
| `acks=all`                                | ë¸Œë¡œì»¤ ë³µì œë³¸ ëª¨ë‘ê°€ ê¸°ë¡ ì™„ë£Œí•´ì•¼ ack                        |
| `enable.idempotence=true`                 | ì¤‘ë³µ ì „ì†¡ì´ ì¼ì–´ë‚˜ë„ ì¤‘ë³µ ê¸°ë¡ ë°©ì§€ (PID, sequence number í™œìš©) |

