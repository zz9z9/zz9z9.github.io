
## 현재 방식 vs Docker/ECS 방식 비교
---

```
[로컬 개발]
      ↓
   GitHub
      ↓
[Jenkins - CI]
  - 코드 체크아웃
  - Maven/Gradle 빌드
  - 유닛 테스트
  - 정적 분석(Optional)
      ↓
[Jenkins - CD]
  - JAR 생성
  - 배포 서버로 파일 전송
  - SSH로 배포 스크립트 실행
      ↓
[운영 서버(Spring Boot / Tomcat)]
```

```
[로컬 개발]
      ↓
    GitHub
      ↓
[GitHub Actions - CI]
  - 코드 체크아웃
  - 테스트 실행
  - 빌드
  - Lint/정적 분석(Optional)
      ↓
[GitHub Actions - CD]
  - Docker 이미지 빌드
  - ECR에 Push
  - ECS 서비스 업데이트 트리거
      ↓
[ECS가 컨테이너 실행 / 교체]
      ↓
[운영 서버(컨테이너)]
```

| 항목               | 현재 방식 (Jenkins + JAR) | Docker/ECS 방식                 |
| ---------------- | --------------------- | ----------------------------- |
| **빌드 위치**        | Jenkins 서버            | GitHub Actions                |
| **빌드 결과물**       | JAR 파일                | Docker 이미지 (컨테이너)             |
| **배포 방식**        | 서버에 JAR 복사 + 스크립트로 실행 | ECS가 컨테이너 이미지 가져와 자동 배포       |
| **배포 대상**        | 물리/VM 서버              | ECS 컨테이너(Task)                |
| **버전 관리**        | JAR 파일 버전             | Docker 이미지 태그                 |
| **운영 서버 의존성**    | Java 버전/OS 설정 다 맞아야 함 | 모두 Docker 이미지에 포함되어서 의존성이 사라짐 |
| **확장성(스케일링)**    | 수동으로 서버 늘림            | ECS가 자동으로 컨테이너 늘림             |
| **무중단 배포**       | 직접 스크립트 짜야 함          | ECS Blue/Green 자동 지원          |
| **Rollback**     | 수동 rollback           | 이전 이미지 태그로 자동 rollback        |
| **CI/CD 구성 난이도** | Jenkins를 직접 운영해야 함    | GitHub Actions에서 YAML만 작성하면 됨 |

| 구분           | 기존 방식                 | Docker/ECS 방식                         |
| ------------ | --------------------- | ------------------------------------- |
| **CI 실행 위치** | Jenkins 서버            | GitHub Actions                        |
| **CI 내용**    | Maven/Gradle 빌드 + 테스트 | Docker build 포함한 전체 테스트               |
| **CD 실행 위치** | Jenkins → SSH 배포 스크립트 | GitHub Actions → ECR push → ECS 자동 배포 |
| **배포 대상**    | OS 서버 + JAR           | Docker 컨테이너                           |
| **롤백**       | 직접 수동                 | ECS가 기존 이미지로 자동 롤백                    |

### 주요 차이
- “JAR 배포”에서 → “Docker Image 배포”로 전환
- 직접 운영 서버 접속 → ❌
- 배포 스크립트 직접 실행 → ❌
- ECS가 자동으로 배포/롤백 → ⭕
- GitHub Actions가 CI/CD 서버 → ⭕
- 서버 의존성 문제 0 → ⭕
- 확장성 + 장애 복구 자동화 → ⭕


## 멀티모듈 vs 프로젝트 분리
---
- Docker + GitHub Actions + ECS 같은 클라우드·컨테이너 기반 환경에서는 멀티모듈 사용이 확실히 줄어든다.

### 왜 기존 환경에서는 멀티모듈을 많이 썼을까?
- 기존(Jenkins + JAR 직접 배포) 환경에서 멀티모듈을 많이 쓴 이유는:
  - 공통 모듈(core)을 여러 서비스(api/admin/batch)가 공유해야 했고
  - **하나의 프로젝트 안에 있는게 빌드/버전 관리/배포가 쉽기 때문**

- 즉, 아래와 같은 구조로 전체를 한 번에 빌드 → 각각 JAR 만들어 배포하는 패턴.

```
parent
├─ core
├─ api
├─ admin
├─ batch
```

### Docker 기반에서는 왜 멀티모듈을 덜 쓸까?
- Docker + ECS 같은 환경에서는 서비스가 각각 독립적인 컨테이너가 됨.
- 즉, “서비스 = 컨테이너 = 하나의 독립된 배포 단위”
- 예를 들어:
  - API → 하나의 컨테이너
  - ADMIN → 하나의 컨테이너
  - BATCH → 하나의 컨테이너
- 각각 별도로 빌드/배포/롤백된다.
- 이러면 멀티모듈 내부에서 한 번에 빌드할 필요가 줄어듦 ??
- 왜냐면 멀티모듈의 장점(한 번에 전체 빌드 & 패키징)이 컨테이너 환경에서는 오히려 단점이 되기 때문. ??

### Docker 환경에서 멀티모듈이 줄어드는 실질적인 이유
- 서비스가 각각 독립 빌드 → 멀티모듈이 발목 잡음
  - ECS는 “하나의 서비스 = 하나의 이미지” 개념
  - 멀티모듈이면 모든 모듈을 매번 함께 빌드해야 함
  - 이건 Docker 이미지 빌드 속도를 크게 느리게 함

- core 모듈 같은 공통 라이브러리는 Nexus/ECR에서 버전으로 관리하는게 일반적
  - 컨테이너 시대에는 공유 기능은 보통:
    - 독립 repo로 운영됨
    - Maven Central 또는 사내 Nexus/ECR로 배포됨
    - 서비스는 그걸 dependency로 받음
  - 즉 멀티모듈 유지할 이유가 많이 사라짐.

- 빌드 캐시 효율성이 떨어짐
  - Docker 빌드는 레이어 캐시가 핵심인데 멀티모듈이면:
    - 소스 전체가 변경되었는지 판단하기 어렵고
    - Docker 캐시가 계속 깨짐
  -  매우 비효율적

- 반면 서비스별 repo로 분리하면:
  - 변경된 서비스만 Docker build
  - Build 속도 빠름
  - ECS 배포도 더 가벼움

### 그렇다면 멀티모듈은 아예 안 쓰는 걸까?
> 여전히 사용되는 경우가 있음:

**1. 단일 프로젝트 모노리식 구조**
- 예: 하나의 API 서버 + 내부 기능이 여러 모듈로 나뉨

```
my-app
├─ core
├─ domain
├─ infrastructure
└─ api
```

- 최종적으로 JAR 1개 생성 + Docker 이미지 1개 만들면 OK
- 이런 구조는 “아키텍처를 정리하는 용도”로 멀티모듈 사용

**2. 같이 버전 관리하는 라이브러리 묶음**
- 예: 회사 내부 SDK, 인증 라이브러리 등
  - 이런 건 멀티모듈을 유지해도 됨
  - 출시는 Nexus나 internal registry로 진행

### 실제 AWS ECS 운영 환경에서는 어떤 방식이 표준일까?

- 이전 방식(모놀리식 멀티모듈)

```
parent
├─ core
├─ api
├─ admin
├─ batch
```

- 추천 방식(컨테이너 시대)

```
core-lib       (공통 라이브러리 repo)
api-service    (독립 repo → 독립 Docker image)
admin-service  (독립 repo → 독립 Docker image)
batch-service  (독립 repo → 독립 Docker image)
```

이게 배포/스케일링/롤백에서 훨씬 유리함.

## QQ
-  컨테이너 시대에는 공유 기능은 보통 독립 repo로 운영됨 ?? 같은 프로젝트 내에서 하는거보다 이점 ?
- **하나의 프로젝트 안에 있는게 빌드/버전 관리/배포가 쉽기 때문** ??

- 즉, 아래와 같은 구조로 전체를 한 번에 빌드 → 각각 JAR 만들어 배포하는 패턴.
=> 한 번에 빌드라는게 뭔 의미지 ?

  - API → 하나의 컨테이너
  - ADMIN → 하나의 컨테이너
  - BATCH → 하나의 컨테이너

- API → 하나의 컨테이너, ADMIN → 하나의 컨테이너
=> 각각 별도로 빌드/배포/롤백된다 ??
=> 멀티모듈인 경우에도 서비스별 jar로 되니까, 각각 되는거아닌가 ?? 다만, 버전은 같이 올라가긴 하겠지만


- 왜냐면 멀티모듈의 장점(한 번에 전체 빌드 & 패키징)이 컨테이너 환경에서는 오히려 단점이 되기 때문. ??

- Docker 빌드는 레이어 캐시가 핵심인데 멀티모듈이면:
  - 소스 전체가 변경되었는지 판단하기 어렵고
  - Docker 캐시가 계속 깨짐 ??
