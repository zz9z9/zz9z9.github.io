[ { "title": "Debezium MySQL Connector Snapshot 알아보기", "url": "/posts/debezium-mysql-connector-snapshot/", "categories": "지식 더하기, 이론", "tags": "Infra", "date": "2025-10-31 22:00:00 +0900", "snippet": "Snapshot 소스 DB의 현재 데이터와 스키마 구조를 캡처해, 변경 이벤트 스트리밍의 기준선을 만드는 초기 동기화 과정 Debezium MySQL 커넥터가 시작될 때, 데이터베이스의 일관된 초기 스냅샷(initial consistent snapshot)을 수행한다. 이 스냅샷은 커넥터가 현재 데이터베이스 상태의 기준선(baseline)을 설정...", "content": "Snapshot 소스 DB의 현재 데이터와 스키마 구조를 캡처해, 변경 이벤트 스트리밍의 기준선을 만드는 초기 동기화 과정 Debezium MySQL 커넥터가 시작될 때, 데이터베이스의 일관된 초기 스냅샷(initial consistent snapshot)을 수행한다. 이 스냅샷은 커넥터가 현재 데이터베이스 상태의 기준선(baseline)을 설정할 수 있도록 해준다. Debezium은 스냅샷을 실행할 때 여러 가지 모드를 사용할 수 있으며, 어떤 모드로 실행할지는 snapshot.mode 설정에 의해 결정된다. 커넥터는 스냅샷을 수행할 때 여러 단계의 작업을 수행하며, 이 단계들은 스냅샷 모드와 데이터베이스에서 적용 중인 테이블 잠금 정책에 따라 달라진다. 예를 들어, 글로벌 읽기 잠금(global read lock) 또는 테이블 단위 잠금(table-level locks)을 사용하는 초기 스냅샷을 수행할 때, 각기 다른 단계를 거친다. 커넥터가 시작될 때 ? 소스 DB와 연결(connection)을 맺고, Task를 실행하기 시작하는 시점 상황 설명 새 커넥터를 등록할 때 /connectors POST 호출 시, 커넥터 Task가 처음 시작됨 기존 커넥터를 재시작할 때 /connectors/{name}/restart 또는 Kafka Connect 자체 재시작 시 커넥트 클러스터 재기동 시 connect worker가 올라오면서 task가 다시 시작됨 커넥터 설정만 업데이트할 때 특정 config 변경 시 Task 재시작이 포함될 경우만 해당 snapshot.mode 커넥터가 시작될 때 스냅샷을 실행할지 여부와 실행 기준을 결정 (Default : initial)always 커넥터가 시작될 때마다 항상 스냅샷을 수행한다. 스냅샷에는 캡처 대상 테이블의 스키마와 데이터가 모두 포함된다.initial 커넥터가 이전에 오프셋을 기록한 적이 없거나, 이전 스냅샷이 완료되지 못한 상태로 중단되었을 때만 스냅샷을 수행한다. 스냅샷이 완료된 후에는 바이너리 로그(binlog)를 읽어 변경 이벤트 스트리밍을 시작한다.initial_only 이전에 오프셋이 없을 때만 스냅샷을 수행한다. 스냅샷이 끝난 후에는 커넥터가 종료되며, binlog를 통해 변경 이벤트 스트리밍을 하지 않는다.no_data 테이블 데이터는 캡처하지 않고, 스키마 정보만 캡처한다. 즉, 데이터의 일관된 스냅샷은 필요 없고, 최근의 스키마 변경 사항만 반영하고 싶을 때 사용하는 모드이다. 예를 들어 “데이터는 이미 target DB에 있고, 스키마만 맞춰야 하는 경우”에 유용 recovery 손상되거나 유실된 스키마 히스토리 토픽을 복원할 때 사용한다. 커넥터가 재시작되면, 소스 테이블의 구조를 읽어 스키마 히스토리 토픽을 다시 빌드한다. 또한, 예기치 않게 커진 스키마 히스토리 토픽을 주기적으로 정리(prune) 할 수도 있다. 단, 커넥터가 종료된 이후 DB에 새로운 스키마 변경이 적용된 경우, 이 모드를 사용해서는 안 됩니다. (스키마 불일치 위험)never 커넥터가 시작될 때 스냅샷을 수행하지 않고, 즉시 binlog 스트리밍을 시작한다. 즉, 기존 데이터는 무시하고 이후 변경만 추적한다. 이 옵션은 앞으로 no_data로 대체될 예정이다. (deprecated 예정)when_needed 커넥터가 시작된 후, 아래 조건 중 하나라도 충족되면 자동으로 스냅샷을 수행한다: Kafka 토픽에서 이전 오프셋 정보를 찾을 수 없는 경우 이전에 저장된 오프셋(binlog 위치 또는 GTID)이 서버에서 더 이상 유효하지 않은 경우 (예: binlog가 삭제됨, position이 유효하지 않음 등) 즉, 필요할 때만 스냅샷을 다시 수행하는 “자기 복구형 모드”이다.configuration_based 이 모드를 사용하면, snapshot.mode.configuration.based.* 로 시작하는 커넥터 설정 속성을 통해 스냅샷 동작을 세부적으로 직접 제어할 수 있다. 즉, 구성 파일 기반으로 세밀한 조건 지정이 가능하다.custom snapshot.mode.custom.name 속성에 정의된 사용자 구현체(io.debezium.spi.snapshot.Snapshotter 인터페이스 구현)를 이용해 스냅샷을 수행한다. 즉, 사용자가 직접 구현한 스냅샷 로직을 사용한다.글로벌 읽기 잠금(global read lock)을 사용하는 초기 스냅샷Step1 : 데이터베이스 연결Step2 : 캡처할 테이블 결정 기본적으로 커넥터는 시스템 테이블을 제외한 모든 테이블의 데이터를 캡처한다. 시스템 테이블 : MySQL 서버의 동작 및 설정, 성능 모니터링 등을 위한 내부용 테이블 예 : information_schema.TABLES, performance_schema.threads, mysql.user 등 스냅샷이 완료된 후, 커넥터는 지정된 테이블의 데이터 변경 사항을 스트리밍한다. 특정 테이블만 캡처하고 싶다면, table.include.list 또는 table.exclude.list 등의 설정을 사용한다.Step3 : 글로벌 읽기 잠금 획득 캡처할 테이블에 대해 글로벌 읽기 잠금을 획득하여 다른 데이터베이스 클라이언트의 쓰기 작업을 차단한다. 단, 스냅샷 자체는 다른 클라이언트가 DDL 문을 실행하는 것을 완전히 막지는 못한다. 이러한 DDL은 커넥터가 binlog 위치나 테이블 스키마를 읽는 과정에 영향을 줄 수 있다. 커넥터는 binlog 위치를 읽는 동안 잠금을 유지한다. Step4 : 트랜잭션 시작 Repeatable Read 격리 수준으로 트랜잭션을 시작하여, 이 트랜잭션 내의 모든 읽기 작업이 일관된 스냅샷 상태를 기준으로 수행되도록 한다. 만약 스냅샷 수행 시간이 너무 오래 걸린다면, 다른 격리 수준 설정을 사용하거나 초기 스냅샷을 건너뛰고 증분 스냅샷(incremental snapshot)을 수행하는 방법을 고려할 수 있다.Step5 : 현재의 binlog 위치를 읽음 이 위치는 이후 변경 데이터 스트리밍을 시작할 기준점이 된다.Step6 : 캡처 대상 테이블들의 스키마를 캡처 커넥터는 스키마 정보를 내부의 schema history 토픽에 저장하며, 이 안에는 필요한 모든 DROP, CREATE DDL 문도 포함된다. schema history는 변경 이벤트가 발생했을 때 적용 중인 구조에 대한 정보를 제공한다. 기본적으로 커넥터는 데이터 캡처 설정 여부와 상관없이 모든 테이블의 스키마를 캡처한다. 단, 캡처 대상으로 지정되지 않은 테이블의 경우, 데이터는 캡처하지 않고, 구조만 저장한다. Step7 : (3단계에서 획득한) 글로벌 읽기 잠금 해제 다른 데이터베이스 클라이언트들의 쓰기 작업이 가능해진다.Step8 : (5단계에서 읽은) binlog 위치를 기준으로, 지정된 테이블들을 스캔하기 시작 이 과정에서 커넥터는 다음 작업들을 수행한다. 스냅샷 시작 전에 생성된 테이블인지 확인 스냅샷이 시작된 후 새로 생성된 테이블이라면 해당 테이블은 건너뜀 다만, 스냅샷이 끝나고 커넥터가 스트리밍 모드로 전환된 후에는, 그 시점 이후 생성된 테이블에 대한 변경 이벤트를 생성 테이블의 각 행에 대해 read 이벤트를 생성 이 이벤트들은 모두 동일한 binlog 위치를 포함 각 read 이벤트를 해당 테이블에 대응하는 Kafka 토픽으로 전송 필요하다면, 사용된 데이터 테이블 잠금을 해제 즉, 초기 스냅샷은 캡처 대상 테이블의 각 행의 현재 상태를 저장한다. 이렇게 생성된 기준 상태(baseline)를 바탕으로, 커넥터는 이후 발생하는 모든 변경 사항을 순차적으로 캡처한다.Step9 : 트랜잭션 커밋Step10 : 스냅샷이 성공적으로 완료되었음을 커넥터의 오프셋(offset)에 기록 이 오프셋 정보는 이후 재시작 시 커넥터가 어디서부터 데이터를 읽어야 하는지를 결정하는 기준이된다.정리(예시 상황) 시간 액션 설명 10:00 글로벌 읽기 잠금 획득 및 트랜잭션 시작 consistent snapshot 생성 10:00 binlog pos = 20000 기록 baseline 위치 저장 10:01 잠금 해제 다른 클라이언트들이 다시 쓰기 가능 10:02 DML 발생 (UPDATE, INSERT 등) binlog에 pos 20001 이후로 기록됨 10:03 ~ 10:05 Debezium이 스냅샷 데이터 SELECT 여전히 10:00 시점 스냅샷 뷰를 읽음(Repeatable Read 격리 수준의 일관된 읽기) 10:05 이후 Debezium이 binlog 20000부터 스트리밍 시작 스냅샷 중 발생한 변경부터 순서대로 반영 테이블 단위 잠금(table-level lock)을 사용하는 초기 스냅샷 일부 데이터베이스 환경에서는 글로벌 읽기 잠금을 허용하지 않는 경우가 있다. 이때 Debezium MySQL 커넥터는 글로벌 락을 사용할 수 없음을 감지하면, 테이블 단위 잠금을 사용하여 스냅샷을 수행한다. 이 방식을 사용하려면, Debezium 커넥터가 MySQL에 접속할 때 사용하는 데이터베이스 계정에 LOCK TABLES 권한이 반드시 있어야 한다. 글로벌 읽기 잠금때와 동일한 단계는 (위와 동일)로 표시Step1 : 데이터베이스 연결 (위와 동일)Step2 : 캡처할 테이블 결정 (위와 동일)Step3 : 테이블 단위 잠금 획득 캡처 대상이 되는 각 테이블에 대해 테이블 단위 잠금을 획득한다. 각 테이블 단위로 읽기 전용 잠금(READ LOCK)을 설정하여 해당 테이블에 쓰기 작업이 일시적으로 차단된다.Step4 : 트랜잭션 시작 (위와 동일)Step5 : 현재의 binlog 위치를 읽음 (위와 동일)Step6 : 캡처 대상 테이블들의 스키마를 캡처 (위와 동일)Step7 : 데이터 읽기 (Step 5에서 얻은) binlog 위치를 기준으로, 커넥터는 캡처 대상 테이블을 순서대로 스캔하며 데이터를 읽습니다. 이 과정에서 커넥터는 다음 작업을 수행합니다: 스냅샷이 시작되기 전에 생성된 테이블인지 확인 만약 스냅샷 도중에 새로 생성된 테이블이라면 이 스냅샷에서는 건너뛰고, 나중에 스트리밍 모드로 전환되면 그때부터 해당 테이블의 변경 이벤트를 캡처 각 테이블의 모든 행을 읽어 “read event”를 생성 이 이벤트에는 Step5에서 얻은 동일한 binlog 위치가 포함되어 있어, “스냅샷 전체가 같은 시점에서 찍힌 데이터임”을 보장한다. 이렇게 생성된 이벤트들을 Kafka 토픽으로 전송한다. 테이블 단위 잠금이 설정되어 있었다면, 테이블 데이터를 모두 읽은 후 잠금을 해제한다.Step8 : 트랜잭션 커밋Step9 : 잠금 해제Step10 : 스냅샷이 성공적으로 완료되었음을 커넥터의 오프셋(offset)에 기록 (위와 동일)정리 (예시 상황) 시각 이벤트 설명 10:00 Debezium 스냅샷 시작 binlog pos = 1000 기록 10:00 테이블 A 스캔 시작 (락 유지 중) 10:01 사용자 UPDATE A (불가 — 잠금 중) 막힘 10:01 사용자 UPDATE B binlog pos = 1001 10:02 테이블 A 스캔 완료 → UNLOCK TABLE A   10:02 테이블 B 스캔 시작 (락 유지 중) 10:03 테이블 B 스캔 완료   10:05 Debezium 스냅샷 완료   10:06 Debezium 스트리밍 모드 전환 (binlog pos=1000부터)   - 글로벌 읽기 잠금 : 잠금 해제 --&gt; 데이터 읽음- 테이블 단위 잠금 : (테이블별로) 데이터 읽음 --&gt; 잠금 해제참고 자료 https://debezium.io/documentation//reference/stable/connectors/mysql.html https://debezium.io/documentation/reference/stable/connectors/mysql.html#mysql-property-snapshot-mode" }, { "title": "Debezium 알아보기", "url": "/posts/debezium-intro/", "categories": "지식 더하기, 이론", "tags": "Infra", "date": "2025-10-29 22:00:00 +0900", "snippet": "Debezium Debezium은 CDC(Change Data Capture)를 위한 오픈 소스 분산 플랫폼이다. (공식 사이트) Kafka Connect의 Source Connector 플러그인 중 하나 (내가 지금까지 이해한 정도) Debezium은 Kafka Connect와 호환되는 소스 커넥터(Source Connector)들을 제공한다. ...", "content": "Debezium Debezium은 CDC(Change Data Capture)를 위한 오픈 소스 분산 플랫폼이다. (공식 사이트) Kafka Connect의 Source Connector 플러그인 중 하나 (내가 지금까지 이해한 정도) Debezium은 Kafka Connect와 호환되는 소스 커넥터(Source Connector)들을 제공한다. 각 커넥터는 특정 DBMS와 연동되며, DBMS에서 발생하는 데이터 변경 사항을 실시간으로 감지하여 그 기록을 Kafka 토픽으로 스트리밍한다. 그 후, 애플리케이션은 Kafka 토픽에 저장된 이벤트 레코드들을 읽어서 해당 변경 내역을 처리할 수 있습니다. Kafka를 활용함으로써, Debezium은 애플리케이션이 데이터베이스에서 발생한 변경 사항을 정확하고 완전하게 소비할 수 있도록 한다. 애플리케이션이 갑작스럽게 중단되거나 연결이 끊기더라도, 그 기간 동안 발생한 이벤트를 놓치지 않는다. 애플리케이션이 다시 시작되면, 중단되었던 지점(topic의 offset 위치)부터 이어서 이벤트를 읽기 시작힌다. 아키텍처 Debezium은 일반적으로 Kafka Connect를 통해 배포된다. Kafka Connect는 다음과 같은 작업을 구현하고 운영하기 위한 프레임워크이자 실행 환경이다: Debezium처럼 Kafka로 레코드를 전송하는 소스 커넥터(Source Connector) Kafka 토픽의 레코드를 다른 시스템으로 전달하는 싱크 커넥터(Sink Connector) (출처 : https://debezium.io/documentation//reference/stable/architecture.html)※ Debezium을 배포한다 ? 어떤 방식으로 CDC를 수행할지 정한 뒤, 그에 맞게 Debezium을 구성하고 실행 환경에 올리는 것을 의미한다. (참고) 예시: 배포 방식 실제로 하는 일 설명 Kafka Connect 기반 배포 Debezium 커넥터를 Kafka Connect 클러스터에 등록하고 실행 소스 커넥터로 DB의 변경사항을 읽어 Kafka에 넣음 Debezium Server 배포 Debezium Server 애플리케이션을 실행하고 환경 설정 지정(예: DB 연결 정보, 메시징 대상) Kafka 없이 Kinesis, Pub/Sub 등으로 CDC 스트리밍 Debezium Engine 내장형 배포 Java 애플리케이션에 Debezium 라이브러리를 포함시켜 코드 레벨에서 실행 앱 내부에서 CDC 수행 (Kafka Connect 불필요) # Kafka Connect 기반 Debezium 배포 예시docker run -it --rm \\ -e GROUP_ID=1 \\ -e CONFIG_STORAGE_TOPIC=my_connect_configs \\ -e OFFSET_STORAGE_TOPIC=my_connect_offsets \\ -p 8083:8083 \\ debezium/connect:latest# Debezium Server 실행 예시java -jar debezium-server.jar --config application.propertiesMySQL 커넥터 등록하기 /connector API를 호출해서 등록 (Kafka Connect REST API) (참고 : Debezium, Kafka Connect, MySQL 버전 호환성 : https://debezium.io/releases/) 등록된 커넥터는 MySQL 서버의 binlog를 감시하기 시작한다. binlog에는 데이터베이스에서 발생한 모든 트랜잭션 기록(개별 행의 변경이나 스키마의 변경 등)이 저장된다. 커넥터는 이러한 변경을 감지하여 change event(변경 이벤트)를 생성한다. 앞으로의 예시에서는, Kafka가 자동으로 토픽을 생성하도록 설정되어 있으며, 각 토픽은 복제본(replica)이 1개뿐인 단순한 구성을 사용한다고 가정한다. 운영 환경에서는 보통 두 가지 방법 중 하나를 사용한다: Kafka 도구(Kafka CLI 등)를 이용해 필요한 토픽을 수동으로 생성하고, 복제(replica) 개수 같은 세부 설정을 직접 지정 Kafka Connect 설정을 통해, 자동으로 생성되는 토픽의 설정을 커스터마이징 예시:$ curl -i -X POST -H \"Accept:application/json\" -H \"Content-Type:application/json\" localhost:8083/connectors -d'{ \"name\": \"inventory-connector\", \"config\": { \"connector.class\": \"io.debezium.connector.mysql.MySqlConnector\", \"tasks.max\": \"1\", \"database.hostname\": \"mysql\", \"database.port\": \"3306\", \"database.user\": \"debezium\", \"database.password\": \"dbz\", \"database.server.id\": \"184054\", \"topic.prefix\": \"dbserver1\", \"database.include.list\": \"inventory\", \"schema.history.internal.kafka.bootstrap.servers\": \"kafka:9092\", \"schema.history.internal.kafka.topic\": \"schema-changes.inventory\" }}'(Request Body에 담긴) 커넥터 관련 속성값들을 살펴보자.name 커넥터의 이름 커넥터 인스턴스를 구분하기 위한 고유한 이름을 지정config 커넥터가 어떻게 동작할지를 정의하는 세부 설정 항목들(참고 : MySQL connector configuration properties)tasks.max 커넥터가 생성할 수 있는 최대 태스크(task)의 개수 Kafka Connect 서비스는 커넥터를 통해 하나 이상의 태스크를 실행할 수 있으며, 실행 중인 태스크들을 Kafka Connect 클러스터 내의 여러 노드에 자동으로 분산 배치한다. 만약 어떤 서비스가 중지되거나 충돌하더라도, 해당 태스크는 자동으로 다른 실행 중인 서비스로 재할당된다. 다만, MySQL 커넥터는 항상 단일 태스크만 사용하기 때문에 이 기본값을 변경해도 아무런 효과가 없다.※ 단일 태스크 사용 이유 Debezium MySQL Connector는 MySQL에 replication 클라이언트(slave)로 연결해서 binlog dump 프로토콜(replication 프로토콜의 일부)을 통해 변경 이벤트를 받는다. 이 스트림은 하나의 연속된 이벤트 스트림이므로, 여러 태스크가 나눠서 병렬로 읽을 수 있는 구조가 아니다. 만약 여러 태스크가 binlog를 병렬로 읽는다면, 동일한 트랜잭션 이벤트가 서로 다른 태스크로 분산되어 순서가 깨질 수 있다.database.hostname 데이터베이스 호스트명 Docker를 사용하는 경우, MySQL 서버를 실행하는 Docker 컨테이너 이름이다. 만약 MySQL이 일반적인 네트워크 환경(즉, Docker가 아닌)에서 실행 중이라면, MySQL 서버의 IP 주소나 DNS로 해석 가능한 호스트 이름을 지정해야 한다.database.server.id Debezium 커넥터(MySQL 클라이언트)의 고유 숫자 ID 지정된 ID는 현재 MySQL 클러스터 내에서 실행 중인 모든 데이터베이스 프로세스와 중복되지 않아야한다. 앞서 살펴본바와 같이, Debezium MySQL 커넥터는 MySQL의 replication 기능을 이용해서 binlog를 읽는다. 즉, Debezium은 MySQL 입장에서 보면 하나의 복제 서버처럼 동작한다. MySQL 복제 환경에서는 각 서버(마스터, 슬레이브 등)가 서로를 구분하기 위해 server_id를 사용한다. 따라서 Debezium 커넥터도 database.server.id 값을 통해 자신을 MySQL 클러스터 내에서 고유하게 식별해야 한다. 이 값이 중복되면 MySQL이 “같은 ID의 클라이언트가 이미 연결되어 있다”며 복제를 거부한다.topic.prefix 데이터베이스 서버 또는 클러스터를 구분하기 위한 네임스페이스(namespace)를 지정하는 문자열 이 값은 Kafka 토픽 이름의 접두사로 사용되며, 커넥터가 생성하는 모든 이벤트 토픽 이름이 이 값을 기반으로 만들어진다. 따라서 각 커넥터마다 이 값은 고유해야 한다. 영문자, 숫자, 하이픈(-), 점(.), 밑줄(_)만 사용할 수 있다. 예를 들어, topic.prefix가 dbserver1이라면 dbserver1.inventory.customers, dbserver1.inventory.orders과 같이 토픽이 생성된다. database.include.list 커넥터가 변경 사항을 캡처할 데이터베이스 이름을 지정 (필수 옵션은 아님) 쉼표(,)로 구분된 정규식 목록으로 작성한다. 커넥터는 이 리스트에 명시된 이름과 일치하는 데이터베이스의 변경만 캡처하며, 여기에 포함되지 않은 데이터베이스의 변경 사항은 캡처하지 않는다. 기본적으로(즉, 이 값을 지정하지 않은 경우), 커넥터는 모든 데이터베이스의 변경 사항을 캡처한다.schema.history.internal.kafka.bootstrap.servers 커넥터가 Kafka 클러스터에 초기 연결을 맺을 때 사용할 호스트/포트 쌍 목록 이 연결은 두 가지 목적으로 사용된다: 커넥터가 이전에 Kafka에 저장한 데이터베이스 스키마 이력을 가져올 때 소스 데이터베이스에서 읽어들인 DDL문을 Kafka에 기록할 때 따라서 이 설정은 데이터 변경 이벤트용 Kafka 연결과는 별개로, 스키마 이력 관리용 Kafka 연결을 정의하는 것이다.schema.history.internal.kafka.topic 커넥터가 데이터베이스 스키마 변경 이력을 저장하는 Kafka 토픽의 전체 이름을 지정 Debezium은 단순히 데이터의 변경만 캡처하지 않고, 테이블 구조도 함께 추적한다. 예를 들어, ALTER TABLE customers ADD COLUMN phone VARCHAR(20); 이때 Debezium은 단순히 customers 테이블의 데이터만 바뀌었다고 알리는 게 아니라, “이 시점에 테이블 구조가 이렇게 바뀌었음” 이라는 스키마 변경 이력을 별도의 토픽에 기록해 둔다. 이렇게 해야 Debezium이 재시작되거나 장애 복구 후에도 정확한 스키마 정보로 binlog를 다시 해석할 수 있습니다. 역할 설명 DDL 기록 테이블 생성, 삭제, 컬럼 추가/변경 등의 DDL 문을 저장 스키마 복원 커넥터가 재시작되면, 이 토픽에서 과거 스키마를 불러와 재구성 이벤트 해석 기준 binlog의 각 이벤트를 당시의 테이블 구조에 맞게 해석하기 위함 내부 관리용 이 토픽은 보통 내부 관리용이므로 외부 애플리케이션이 구독하지 않음 Debezium은 내부적으로 이런 흐름을 가진다:MySQL binlog↓Debezium Connector├─ DDL 감지 (스키마 변경)├─ DML 감지 (데이터 변경)↓Kafka├─ schema-changes.inventory ← 스키마 이력 저장└─ dbserver1.inventory.customers ← 데이터 변경 이벤트 저장변경 이벤트 살펴보기 dbserver1.inventory.customers 토픽 가정 각 이벤트는 두 개의 JSON 데이터로 구성된다: Key : 해당 행의 기본 키 정보 Value : 실제 데이터 변경 내용 JSON 데이터는 두 부분으로 구성된다: schema : payload의 구조를 설명하는 Kafka Connect 스키마 정의 payload : 실제 데이터 값 Create Event Key{ \"schema\": { \"type\": \"struct\", \"fields\": [ { \"type\": \"int32\", \"optional\": false, \"field\": \"id\" } ], \"optional\": false, \"name\": \"dbserver1.inventory.customers.Key\" }, \"payload\": { \"id\": 1004 }} Value{ \"schema\": { \"type\": \"struct\", \"fields\": [ { \"type\": \"struct\", \"fields\": [ { \"type\": \"int32\", \"optional\": false, \"field\": \"id\" }, { \"type\": \"string\", \"optional\": false, \"field\": \"first_name\" }, { \"type\": \"string\", \"optional\": false, \"field\": \"last_name\" }, { \"type\": \"string\", \"optional\": false, \"field\": \"email\" } ], \"optional\": true, \"name\": \"dbserver1.inventory.customers.Value\", \"field\": \"before\" }, { \"type\": \"struct\", \"fields\": [ { \"type\": \"int32\", \"optional\": false, \"field\": \"id\" }, { \"type\": \"string\", \"optional\": false, \"field\": \"first_name\" }, { \"type\": \"string\", \"optional\": false, \"field\": \"last_name\" }, { \"type\": \"string\", \"optional\": false, \"field\": \"email\" } ], \"optional\": true, \"name\": \"dbserver1.inventory.customers.Value\", \"field\": \"after\" }, { \"type\": \"struct\", \"fields\": [ { \"type\": \"string\", \"optional\": true, \"field\": \"version\" }, { \"type\": \"string\", \"optional\": false, \"field\": \"name\" }, { \"type\": \"int64\", \"optional\": false, \"field\": \"server_id\" }, { \"type\": \"int64\", \"optional\": false, \"field\": \"ts_sec\" }, { \"type\": \"string\", \"optional\": true, \"field\": \"gtid\" }, { \"type\": \"string\", \"optional\": false, \"field\": \"file\" }, { \"type\": \"int64\", \"optional\": false, \"field\": \"pos\" }, { \"type\": \"int32\", \"optional\": false, \"field\": \"row\" }, { \"type\": \"boolean\", \"optional\": true, \"field\": \"snapshot\" }, { \"type\": \"int64\", \"optional\": true, \"field\": \"thread\" }, { \"type\": \"string\", \"optional\": true, \"field\": \"db\" }, { \"type\": \"string\", \"optional\": true, \"field\": \"table\" } ], \"optional\": false, \"name\": \"io.debezium.connector.mysql.Source\", \"field\": \"source\" }, { \"type\": \"string\", \"optional\": false, \"field\": \"op\" }, { \"type\": \"int64\", \"optional\": true, \"field\": \"ts_ms\" }, { \"type\": \"int64\", \"optional\": true, \"field\": \"ts_us\" }, { \"type\": \"int64\", \"optional\": true, \"field\": \"ts_ns\" } ], \"optional\": false, \"name\": \"dbserver1.inventory.customers.Envelope\", \"version\": 1 }, \"payload\": { \"before\": null, \"after\": { \"id\": 1004, \"first_name\": \"Anne\", \"last_name\": \"Kretchmar\", \"email\": \"annek@noanswer.org\" }, \"source\": { \"version\": \"3.3.1.Final\", \"name\": \"dbserver1\", \"server_id\": 0, \"ts_sec\": 0, \"gtid\": null, \"file\": \"mysql-bin.000003\", \"pos\": 154, \"row\": 0, \"snapshot\": true, \"thread\": null, \"db\": \"inventory\", \"table\": \"customers\" }, \"op\": \"r\", \"ts_ms\": 1486500577691, \"ts_us\": 1486500577691547, \"ts_ns\": 1486500577691547930 }} 이 스키마는 dbserver1.inventory.customers.Envelope (version 1)라는 이름의 Kafka Connect 스키마를 포함하며, 다섯 가지 필드를 가진다: op : 이벤트가 어떤 작업(operation) 인지를 나타내는 문자열 값 (필수 필드) c - create (INSERT) u - update d - delete r - read (스냅샷 시 기존 데이터 읽을 때) before : 선택적(optional) 필드이며, 존재할 경우, 이벤트 발생 이전의 행(row) 상태를 포함 after : 선택적 필드이며, 존재할 경우, 이벤트 발생 이후의 행(row) 상태를 포함 source : 필수 필드이며, 이벤트의 출처(source) 메타데이터를 담고 있는 구조체 MySQL 커넥터의 경우, source 안에는 다음 정보들이 포함된다: 커넥터 이름 (dbserver1 등) 이벤트가 기록된 binlog 파일 이름 해당 이벤트가 binlog 내에서 나타난 위치(position) (한 binlog 이벤트에 여러 행이 있는 경우) 행의 인덱스 영향을 받은 데이터베이스 이름 및 테이블 이름 변경을 수행한 MySQL 스레드 ID 이 이벤트가 스냅샷 중 발생한 것인지 여부 (가능한 경우) MySQL 서버 ID 이벤트가 기록된 타임스탬프 (초 단위) ts_ms : 선택적 필드이며, 존재할 경우 이 값은 커넥터가 이벤트를 처리한 시각(밀리초 단위)을 포함. 이 시간은 Kafka Connect 작업이 실행 중인 JVM의 시스템 시계 기준 ※ Avro 고려하기 Avro는 데이터를 컴퓨터가 이해하고 저장/전송하기 쉬운 이진 형식으로 변환하는 ‘데이터 직렬화 시스템’이다. JSON과 달리 스키마 진화(Schema Evolution)를 지원하여 데이터 구조 변경을 유연하게 처리할 수 있어 빅데이터 시스템에서 효율적으로 사용된다. Kafka Connect의 JSON 변환기는 모든 메시지마다 key와 value의 스키마 정의를 함께 포함하기 때문에, 이벤트 메시지가 매우 장황해진다. 이런 문제를 해결하기 위해 Apache Avro 포맷을 사용할 수도 있다. 즉, Kafka Connect 스키마를 Avro 스키마로 변환하고, 이 스키마는 별도의 Schema Registry 서비스에 저장되며, 실제 메시지에는 스키마 전체가 아니라 고유한 스키마 ID 와 Avro 인코딩된 바이너리 데이터만 포함된다. 이 방식 덕분에, 네트워크로 전송되거나 Kafka에 저장되는 메시지 크기가 JSON 포맷에 비해 훨씬 작아진다. 또한 Avro Converter는 스키마 진화(schema evolution) 기능을 지원하므로, Schema Registry에 각 스키마 버전의 변경 이력을 관리할 수도 있다.Update Event UPDATE customers SET first_name=’Anne Marie’ WHERE id=1004; Key : 전과 동일 Value{ \"schema\": {...}, \"payload\": { \"before\": { \"id\": 1004, \"first_name\": \"Anne\", \"last_name\": \"Kretchmar\", \"email\": \"annek@noanswer.org\" }, \"after\": { \"id\": 1004, \"first_name\": \"Anne Marie\", \"last_name\": \"Kretchmar\", \"email\": \"annek@noanswer.org\" }, \"source\": { \"name\": \"3.3.1.Final\", \"name\": \"dbserver1\", \"server_id\": 223344, \"ts_sec\": 1486501486, \"gtid\": null, \"file\": \"mysql-bin.000003\", \"pos\": 364, \"row\": 0, \"snapshot\": null, \"thread\": 3, \"db\": \"inventory\", \"table\": \"customers\" }, \"op\": \"u\", \"ts_ms\": 1486501486308, \"ts_us\": 1486501486308910, \"ts_ns\": 1486501486308910814 }}Primary Key Update 기본 키 필드가 변경되는 경우에는 일반적인 UPDATE 이벤트 대신, 커넥터가 다음과 같은 두 개의 이벤트를 생성한다: 기존 키에 대한 DELETE 이벤트 레코드 새(변경된) 키에 대한 CREATE 이벤트 레코드 두 이벤트는 일반적인 이벤트 구조와 내용을 가지지만, 기본 키 변경과 관련된 특수한 메시지 헤더가 추가로 포함된다. DELETE 이벤트 레코드에는 __debezium.newkey라는 메시지 헤더가 포함된다. 이 헤더의 값은 변경된 행의 새로운 primary key값 즉, \"이 행이 삭제되었지만, 사실은 새 키로 다시 생성될 것이다\"라는 의미를 담고 있다. CREATE 이벤트 레코드에는 __debezium.oldkey라는 메시지 헤더가 포함된다. 이 헤더의 값은 업데이트 이전의(예전) 기본 키 값 즉, \"이 새 행은 이전에 이 키를 가진 행으로부터 온 것이다\"라는 정보를 담고 있다. Delete Event DELETE FROM customers WHERE id=1004; 테이블에서 한 행(row)을 삭제하면, Debezium MySQL 커넥터는 두 개의 이벤트를 Kafka로 보낸다.첫번째 이벤트 Key 영역 : 전과 동일 Value 영역{ \"schema\": {...}, \"payload\": { \"before\": { \"id\": 1004, \"first_name\": \"Anne Marie\", \"last_name\": \"Kretchmar\", \"email\": \"annek@noanswer.org\" }, \"after\": null, \"source\": { \"name\": \"3.3.1.Final\", \"name\": \"dbserver1\", \"server_id\": 223344, \"ts_sec\": 1486501558, \"gtid\": null, \"file\": \"mysql-bin.000003\", \"pos\": 725, \"row\": 0, \"snapshot\": null, \"thread\": 3, \"db\": \"inventory\", \"table\": \"customers\" }, \"op\": \"d\", \"ts_ms\": 1486501558315, \"ts_us\": 1486501558315901, \"ts_ns\": 1486501558315901687 }}두번째 이벤트 tombstone event : key는 동일하지만, value가 완전히 null인 이벤트 Key : 전과 동일 Value{ \"schema\": null, \"payload\": null} Kafka는 log compaction이라는 기능을 가지고 있다. 이는 토픽 내 메시지를 key 기준으로 압축하는 기능이다. 각 key에 대해 가장 최신 메시지만 남기고, 이전 메시지들은 삭제할 수 있다. 삭제(delete)라는 개념을 표현하기 위해, Kafka는 key는 있고 value는 null인 tombstone event를 사용한다. Kafka는 이를 보고 “이 key에 대한 데이터는 삭제된 상태다”라고 인식하고, 해당 key의 이전 모든 메시지를 지운다. 즉시 삭제를 수행하지는 않고, Kafka의 log cleaner 스레드가 주기적으로 compaction을 수행할 때 처리된다. 즉, compaction이 실행되기 전까지는 tombstone과 이전 메시지 모두 로그에 남아 있을 수 있다. 참고 자료 https://debezium.io/documentation//reference/stable/architecture.html https://debezium.io/documentation//reference/stable/tutorial.html https://debezium.io/documentation//reference/stable/connectors/mysql.html" }, { "title": "Kafka Connect 알아보기", "url": "/posts/kafka-connect-intro/", "categories": "지식 더하기, 이론", "tags": "Infra", "date": "2025-10-27 22:50:00 +0900", "snippet": "Kafka Connect Apache Kafka와 다른 데이터 시스템 간에, 데이터를 확장 가능하고 안정적으로 스트리밍하기 위한 도구 대규모 데이터를 Kafka로 가져오거나 Kafka에서 내보내는 커넥터를 빠르고 간단하게 정의할 수 있게 해준다. 전체 데이터베이스를 가져오거나, 애플리케이션 서버 전반에서 수집한 메트릭(metrics)을 Kafka...", "content": "Kafka Connect Apache Kafka와 다른 데이터 시스템 간에, 데이터를 확장 가능하고 안정적으로 스트리밍하기 위한 도구 대규모 데이터를 Kafka로 가져오거나 Kafka에서 내보내는 커넥터를 빠르고 간단하게 정의할 수 있게 해준다. 전체 데이터베이스를 가져오거나, 애플리케이션 서버 전반에서 수집한 메트릭(metrics)을 Kafka 토픽으로 전송하여 낮은 지연(latency)으로 스트림 처리에 활용할 수 있다. 내보내기 작업(export job)을 통해 Kafka 토픽의 데이터를 보조 스토리지나 조회 시스템, 또는 배치(batch) 시스템으로 전달하여 오프라인 분석에 사용할 수도 있다.주요 특징1. Kafka 커넥터를 위한 공통 프레임워크 Kafka Connect는 외부 데이터 시스템과 Kafka 간의 통합을 표준화하여, 커넥터의 개발·배포·관리를 단순화한다.2. 분산(distributed) 모드와 단일(standalone) 모드 지원 대규모 중앙 관리형 서비스로 확장하거나, 개발·테스트·소규모 운영 환경에 맞게 축소하여 사용할 수 있다.3. REST 인터페이스 제공 간단한 REST API를 통해 Kafka Connect 클러스터에 커넥터를 등록하고 관리할 수 있다.4. 자동 오프셋(offset) 관리 커넥터가 최소한의 정보만 제공해도 Kafka Connect가 오프셋 커밋 과정을 자동으로 관리한다.5. 분산 및 확장 가능 구조 Kafka Connect는 Kafka의 그룹 관리 프로토콜(group management protocol)을 기반으로, 워커를 추가하기만 해도 클러스터 규모를 손쉽게 확장할 수 있다.핵심 개념1. Connectors Kafka Connect의 커넥터(connector)는 데이터가 어디에서 어디로 복사되어야 하는지를 정의Source Connector 외부 데이터 소스에서 전체 데이터베이스를 가져오고, 테이블의 변경 사항을 실시간으로 Kafka 토픽으로 스트리밍※ “전체 데이터베이스를 가져온다 (ingest entire database)” ? 단순히 DB 전체를 한 번에 복사한다는 뜻이 아니라, 초기 데이터 적재 단계에서의 동작을 의미 초기 적재 (Initial Snapshot / Full Load) CDC 파이프라인을 처음 시작할 때, 현재 데이터베이스에 들어있는 모든 테이블의 전체 데이터를 한 번 읽어서 Kafka로 보내는 단계 CDC는 원래 “변경 사항만 감지”하지만, 시스템이 처음 시작할 때는 “기존 데이터”가 Kafka에 없기 때문에 초기 스냅샷이 필요 예시: MySQL의 경우, Debezium이 처음 시작할 때 각 테이블을 SELECT * FROM table 식으로 읽음. 그 데이터를 Kafka 토픽에 넣어 “현재 상태”를 맞춘 뒤, 이후부터는 binlog 기반의 변경 이벤트만 처리. 따라서, 테이블 크기가 크면, 전체 스캔(Full Table Scan), 네트워크 전송 (DB → Kafka Connect), JSON 직렬화 → Kafka 발행까지 다 포함되어, DB I/O, 네트워크, Kafka 전송 지연이 한꺼번에 누적된다. Debezium 설정 snapshot.mode로 스냅샷 방식을 제어할 수 있다. 모드 설명 initial (기본값) 전체 데이터를 한 번 읽은 뒤 binlog로 전환 schema_only 스키마만 가져오고 실제 데이터는 binlog로만 반영 never 스냅샷 생략, 이미 target이 동기화되어 있는 경우 사용 initial_only binlog로 넘어가지 않고 스냅샷까지만 수행 Sink Connector Kafka 토픽에 저장된 데이터를 외부 시스템으로 내보내는 역할 커넥터가 구현하거나 사용하는 모든 클래스는 커넥터 플러그인(connector plugin) 안에 정의되어 있습니다. 즉, “커넥터 플러그인(plugin)”은 코드(클래스, 설정 등)가 들어 있는 구현 단위, “커넥터 인스턴스(instance)”는 그 플러그인을 실제로 실행 중인 작업 단위라고 할 수 있습니다. 둘 다 “커넥터(connector)”라고 부르기도 하지만, 맥락에 따라 의미가 구분됩니다. 예를 들어, “install a connector(커넥터를 설치한다)” → 플러그인(plugin)을 의미 “check the status of a connector(커넥터 상태를 확인한다)” → 인스턴스(instance)를 의미 Confluent는 사용자가 가능한 한 기존 커넥터를 활용할 것을 권장※ Confluent ? https://www.confluent.io/ Apache Kafka의 상용 배포판을 만든 회사이자, Kafka의 창시자인 Jay Kreps가 공동 창립한 회사 항목 설명 Kafka Connect Apache Kafka의 공식 컴포넌트 (데이터 통합 프레임워크) Confluent Kafka 및 Connect를 포함한 상용 배포/운영 플랫폼 Confluent Platform Kafka Connect + Schema Registry + ksqlDB + 관리도구 등 포함 Confluent Cloud 완전관리형 Kafka 서비스 (Kafka Connect 포함) 2. Tasks Task가 실제로 데이터를 복사하는 작업을 수행 각 커넥터 인스턴스(connector instance)는 여러 개의 태스크(task)를 조정 커넥터가 하나의 작업(job)을 여러 태스크로 나누어 병렬로 실행할 수 있게 함으로써, Kafka Connect는 병렬 처리(parallelism) 와 확장 가능한 데이터 복제(scalable data copying) 를 복잡한 설정 없이도 기본적으로 지원 각 태스크 자체는 내부적으로 상태(state)를 저장하지 않습니다. 대신 태스크의 상태 정보는 Kafka의 특별한 토픽에 저장됩니다: config.storage.topic status.storage.topic 이 상태들은 해당 커넥터 인스턴스가 관리합니다. 즉, 태스크는 무상태(stateless) 로 설계되어 있고, 필요할 때마다 Kafka 내부 토픽을 통해 상태를 복구하거나 관리할 수 있다. 태스크는 언제든지 시작(start), 중지(stop), 재시작(restart) 할 수 있습니다. 이런 구조 덕분에 Kafka Connect는 장애 복구(resilience) 와 확장성(scalability) 을 동시에 갖춘 안정적인 데이터 파이프라인을 제공(출처 : https://docs.confluent.io/platform/current/connect/index.html)Task Rebalancing Kafka Connect에서 커넥터가 클러스터에 처음 등록되면, 클러스터 내의 모든 워커(worker)들이 협력하여 모든 커넥터와 태스크를 재분배(rebalance) 합니다. 이 과정을 통해 각 워커는 대략 동일한 양의 작업을 담당하게 됩니다. 리밸런싱 절차는 다음과 같은 상황에서도 수행됩니다: 커넥터가 필요한 태스크 개수를 늘리거나 줄일 때 커넥터의 설정이 변경될 때 워커가 장애로 인해 중단되었을 때, 남아 있는 활성 워커(active workers)들로 태스크가 재분배됨 즉, 클러스터는 항상 부하를 균등하게 분산시키는 방향으로 자동 조정된다. 태스크가 개별적으로 실패했을 때(task failure)는 리밸런싱이 자동으로 트리거되지 않는다. 태스크 실패는 일반적인 운영 시나리오가 아닌 예외적인 상황으로 간주되기 때문입니다. 따라서, 실패한 태스크는 Kafka Connect 프레임워크가 자동으로 재시작하지 않습니다. 대신 REST API 를 통해 수동으로 재시작해야 합니다. (출처 : https://docs.confluent.io/platform/current/connect/index.html)3. Workers 커넥터(Connector) 와 태스크(Task) 는 논리적인 작업 단위이므로, 이들이 실제로 실행되기 위해서는 어떤 프로세스(process) 위에 스케줄링되어야 합니다. Kafka Connect에서는 이러한 프로세스를 워커(Worker) 라고 부릅니다.Standalone Workers (단일 모드 워커) Standalone 모드는 가장 단순한 실행 모드입니다.하나의 프로세스가 모든 커넥터와 태스크를 직접 실행합니다. 특징: 설정이 매우 간단 (단일 프로세스만 실행) 개발 초기 단계나 테스트, 또는 단일 호스트에서 로그를 수집하는 등의 간단한 상황에 적합 모든 실행이 하나의 프로세스에서 처리됨 제한점: 확장성(scalability) 이 제한됨 (프로세스 1개가 전부) 장애 복구(fault tolerance) 불가능 (프로세스가 죽으면 전체 중단) 외부 모니터링을 붙이지 않으면 자동 복구 기능 없음 Distributed Workers (분산 모드 워커) Kafka Connect의 확장성(scalability)과 자동 장애 복구(fault tolerance)를 제공하는 운영 환경용 모드 특징: 여러 워커 프로세스를 동일한 group.id 로 실행하면, 이들이 하나의 Connect 클러스터를 형성함. 워커들이 서로 협력하여 커넥터와 태스크를 자동으로 분산 배치. 워커를 추가하거나 중단하거나 장애가 발생하면, 남은 워커들이 이를 감지하고 태스크를 자동 재분배(rebalance) 함. 이 과정은 Kafka Consumer Group 리밸런싱과 유사한 방식으로 동작함. 예시: Worker A: group.id = connect-cluster-a Worker B: group.id = connect-cluster-a 두 워커는 자동으로 하나의 클러스터 connect-cluster-a 를 형성함. (출처 : https://docs.confluent.io/platform/current/connect/index.html)4. Converters Kafka Connect가 외부 시스템과 데이터를 주고받을 때, 데이터를 Kafka 내부 형식(바이트 배열)과 Connect의 내부 데이터 구조 사이에서 변환해주는 역할 즉, Task가 데이터를 처리할 때, 컨버터를 사용하여 바이트 형태의 데이터를 Kafka Connect 내부 데이터 형식으로 변환하거나, 반대로 내부 형식을 바이트로 직렬화 예시: 컨버터 설명 JsonConverter 데이터를 JSON 형태로 변환 StringConverter 단순 문자열로 변환 AvroConverter Avro 스키마를 기반으로 변환 (주로 Schema Registry와 함께 사용) ProtobufConverter Protobuf 포맷으로 직렬화/역직렬화 ByteArrayConverter 원시 바이트 그대로 처리 (가공 없이 전송할 때) 컨버터는 커넥터로부터 독립적으로 분리되어 설계되어 있다. 따라서, 커넥터는 자신이 다루는 데이터 소스나 타깃(DB, 파일 등)에 집중할 수 있고 데이터의 표현 방식(Avro, JSON, String 등)은 컨버터가 전담 (출처 : https://docs.confluent.io/platform/current/connect/index.html)5. Transforms 하나의 레코드를 입력받아 수정된 레코드를 출력하는 간단한 함수 레코드 ? 커넥터가 주고받는 한 건의 데이터 이벤트를 의미 Source Connector → Kafka 로 데이터를 보낼 때: SourceRecord Kafka → Sink Connector 로 데이터를 보낼 때: SinkRecord orders 테이블에서 한 행이 변경됐을때, Debezium 같은 Source Connector가 다음과 같은 SourceRecord 를 생성{ \"topic\": \"dbserver1.inventory.orders\", \"key\": { \"order_id\": 101 }, \"value\": { \"order_id\": 101, \"status\": \"SHIPPED\", \"amount\": 30000 }, \"timestamp\": 1735200000000, \"schema\": { ... }}Source Connector에서의 Transform 동작 Source Connector → Transform 1 → Transform 2 → … → Kafka Source Connector 가 새로운 소스 레코드를 생성하면, Kafka Connect는 그 레코드를 첫 번째 변환(Transform)으로 전달한다. 변환이 수행되어 수정된 새로운 레코드가 생성된다. 이 결과 레코드는 다음 변환으로 전달되고, 같은 방식으로 반복된다. 마지막 변환을 거친 최종 레코드는 바이너리 형태로 직렬화되어 Kafka에 저장된다.Sink Connector에서의 Transform 동작 Kafka → Transform 1 → Transform 2 → … → Sink Connector Kafka Connect는 Kafka 토픽으로부터 메시지를 읽어 바이너리 데이터를 Sink Record 로 변환한다. 변환이 설정되어 있으면, 그 레코드는 첫 번째 변환을 거친다. 수정된 레코드는 다음 변환으로 전달되어 또 한 번 갱신된다. 모든 변환을 거친 최종 Sink Record가 Sink Connector 로 전달되어 최종 처리된다.예시1 토픽 이름 변경 (RegexRouter 사용)\"transforms\": \"routeByStatus\",\"transforms.routeByStatus.type\": \"org.apache.kafka.connect.transforms.RegexRouter\",\"transforms.routeByStatus.regex\": \"orders\",\"transforms.routeByStatus.replacement\": \"orders.${order_status}\" RegexRouter 변환은 메시지가 들어가는 토픽 이름을 동적으로 변경한다. ${order_status} 같은 필드를 사용해 메시지 값(value)에 따라 라우팅이 가능 (다만 기본 SMT는 값 참조를 직접 지원하지 않으므로, 커스텀 Transform으로 구현하기도 한다.) 결과적으로, orders.NEW, orders.CANCELLED처럼 주문 상태에 따라 다른 토픽으로 전송됨예시2 특정 필드 제거\"transforms\": \"removePII\",\"transforms.removePII.type\": \"org.apache.kafka.connect.transforms.ReplaceField$Value\",\"transforms.removePII.blacklist\": \"ssn,email\" ssn, email 같은 개인정보 필드를 메시지에서 제거6. Dead Letter Queue Sink Connector에서 처리할 수 없는 잘못된 레코드를 별도 토픽으로 보내는 에러 처리 메커니즘 Dead Letter Queue(DLQ)는 Sink Connector에서만 사용되는 기능 즉, Sink Connector에서 형식 불일치나 변환 오류 등으로 특정 레코드를 처리할 수 없을 때, 실패한 레코드를 특별한 Kafka 토픽(Dead Letter Queue Topic)으로 따로 보관한다. 예시 : Kafka 메시지는 JSON 형식인데, Sink Connector는 Avro 형식을 기대하고 있을 때 메시지에 필수 필드가 누락되었을 때 스키마 호환성 문제로 Sink 시스템에 쓸 수 없을 때 이런 경우, Kafka Connect는 errors.tolerance 설정에 따라 대응한다. 설정값 의미 none (기본값) - 오류 발생 시 즉시 태스크 실패. 커넥터는 실패 상태(failed)가 되어 중단됨 - 운영자는 Worker 로그를 확인하고 원인을 수정한 뒤 커넥터를 재시작해야 함 all - 모든 오류나 잘못된 레코드를 무시하고 계속 처리. 다만 로그에는 기록되지 않음. - 따라서, 실패한 레코드 수를 확인하려면 내부 메트릭(metrics)이나 원본과 결과의 카운트를 비교해야 한다. errors.tolerance=all 로 설정된 경우, 추가 설정을 통해 실패한 레코드를 DLQ 토픽으로 자동 전송할 수 있다.errors.tolerance = allerrors.deadletterqueue.topic.name = &lt;dead-letter-topic-name&gt; 기본적으로 DLQ 토픽에는 레코드 데이터만 저장되며, “왜 실패했는지”에 대한 정보가 없다. 아래 옵션을 추가해서 에러 메타데이터를 함께 기록할 수 있다.errors.deadletterqueue.context.headers.enable = true 이 설정을 켜면 DLQ에 전송되는 레코드의 헤더에 에러 원인 정보가 추가된다. 헤더 키들은 _connect.errors. 로 시작하며, kafkacat 같은 도구로 이 헤더를 읽어보면 실패 이유를 직접 확인할 수 있다. 예:_connect.errors.exception.class=org.apache.kafka.connect.errors.DataException_connect.errors.exception.message=Invalid schema참고 자료 https://kafka.apache.org/documentation.html#connect https://docs.confluent.io/platform/current/connect/index.html" }, { "title": "CDC(Change Data Capture) 알아보기", "url": "/posts/cdc-intro/", "categories": "지식 더하기, 이론", "tags": "Infra", "date": "2025-10-26 00:50:00 +0900", "snippet": "CDC(Change Capture Data)란 ? 데이터베이스나 데이터 웨어하우스 같은 데이터 소스의 모든 변경 사항을 추적하여, 이러한 변경 내용을 대상 시스템에 반영할 수 있도록 하는 과정(출처 : https://www.confluent.io/learn/change-data-capture/#quick-intro-to-cdc)주요 이점1. 대용량 ...", "content": "CDC(Change Capture Data)란 ? 데이터베이스나 데이터 웨어하우스 같은 데이터 소스의 모든 변경 사항을 추적하여, 이러한 변경 내용을 대상 시스템에 반영할 수 있도록 하는 과정(출처 : https://www.confluent.io/learn/change-data-capture/#quick-intro-to-cdc)주요 이점1. 대용량 일괄 업데이트 제거 CDC는 변경된 데이터만을 증분 로드(incremental loading)하거나 실시간으로 스트리밍 방식으로 대상 저장소에 반영할 수 있다. 따라서 대규모 배치 업데이트가 필요하지 않다.2. 로그 기반 효율성(Log-Based Efficiency) 로그 기반 CDC는 트랜잭션 로그에서 직접 변경 사항을 캡처하기 때문에 원본 시스템에 미치는 영향을 최소화한다.3. 무중단 마이그레이션(Zero-Downtime Migrations) CDC가 제공하는 실시간 데이터 이동 기능은 무중단 데이터베이스 마이그레이션을 지원한다. 이를 통해 실시간 분석 및 리포팅에 필요한 최신 데이터를 이용할 수 있다.4. 시스템 간 동기화(Synchronization Across Systems) CDC는 여러 시스템 간의 데이터를 동기화 상태로 유지한다. 이는 데이터 속도가 빠르고 즉각적인 의사결정이 필요한 환경에서 특히 중요하다.5. 클라우드 및 스트림 처리 최적화(Optimized for Cloud and Stream Processing) CDC는 광역 네트워크(WAN)를 통해 데이터를 효율적으로 이동시킬 수 있어, 클라우드 환경 배포나 Apache Kafka와 같은 스트림 처리 솔루션과의 통합에 이상적이다.구현 방식 CDC는 일반적으로 두 가지 주요 방식, Push 또는 Pull 방식으로 구현한다.Push 소스 데이터베이스가 주요 역할을 담당한다. 즉, 데이터베이스 내에서 발생한 변경 사항을 직접 감지(capture)하고, 그 변경된 데이터를 대상 시스템으로 전송한다. 이 방식의 가장 큰 장점은 대상 시스템이 거의 실시간(near real-time)으로 최신 데이터를 받을 수 있다는 점이다. 데이터 변경이 발생하면 즉시 전송되므로, 지연이 최소화되고 데이터 일관성이 높게 유지된다. 하지만, 대상 시스템이 일시적으로 접속 불가하거나 오프라인 상태일 경우, 전송된 변경 데이터가 손실될 위험이 있다. 이러한 위험을 완화하기 위해, 일반적으로 소스와 대상 시스템 사이에 메시징 시스템을 둔다. Pull 소스 데이터베이스가 변경 내용을 직접 전송하는 대신, 각 테이블에 있는 특정 컬럼(예: last_updated나 modified_at) 등에 데이터 변경 로그를 기록한다. 그 후 대상 시스템이 주기적으로 소스 데이터베이스를 폴링(polling)하여 변경된 데이터를 감지하고 필요한 작업을 수행한다. 따라서 소스 시스템의 부하가 적고, 데이터베이스에 추가적인 부하 없이 간단하게 구현할 수 있다. 대상 시스템이 일시적으로 오프라인일 때 데이터 손실을 방지하기 위해 중간에 메시징 시스템을 두는 것이 일반적이다. 메시징 시스템은 변경 데이터를 임시 저장(버퍼링) 하여, 대상 시스템이 복구되면 안전하게 전달될 수 있도록 보장한다. 이 방식의 단점은 데이터 변경이 즉시 반영되지 않는다는 점이다. 대상 시스템이 일정 주기(예: 1분, 5분, 10분)에 한 번씩만 데이터를 가져오기 때문에, 그 사이에 발생한 변경 사항은 다음 폴링 시점까지 지연된다. 데이터 변경 감지 방식1. Timestamp-based 테이블에 LAST_MODIFIED, LAST_UPDATED 같은 타임스탬프 컬럼을 추가하여 각 레코드의 가장 최근 변경 시각을 기록하는 방식 대상 시스템은 이 컬럼을 조회해서 마지막 실행 이후 변경된 데이터를 가져온다. DELETE(삭제) 연산을 직접 감지할 수 없다. (즉, “소프트 삭제”만 가능 — 실제 삭제가 아닌 상태 플래그 업데이트 방식) 모든 행(row)을 스캔해야 하므로 소스 시스템 부하가 커질 수 있다. 어떤 데이터가 갱신되었는지를 확인하기 위해 전체 테이블을 조회해야 함. DB 스키마 변경이 필요하다. (LAST_UPDATED 컬럼 추가 등)2. Trigger-based 데이터베이스가 제공하는 트리거(trigger) 기능을 활용. 즉, INSERT, UPDATE, DELETE가 실행될 때마다 자동으로 실행되는 프로시저를 등록 각 트리거는 변경된 데이터를 별도의 테이블(“shadow table” 또는 “event table”)에 기록한다. 필요하다면 메시징 시스템에 이벤트를 전송할 수도 있다. 소스 DB 성능에 부정적인 영향을 미친다. (한 번의 데이터 변경이 여러 번의 쓰기 작업으로 이어짐 — 원본 테이블 + 이벤트 테이블) DB 스키마 변경이 필요합니다. (트리거 및 이벤트 테이블 추가) 트리거가 많아질수록 관리 복잡도 증가 (테이블마다 3개씩 필요 — INSERT, UPDATE, DELETE)3. Log-based 데이터베이스는 트랜잭션 수행 시마다 모든 변경사항을 트랜잭션 로그 파일(MySQL - binlog, PostgreSQL - WAL, Oracle - redo log)에 기록이 로그는 원래 복구 용도로 사용되지만, CDC 시스템이 이 로그를 읽어서 실시간으로 변경 사항을 감지할 수도 있다. 소스 데이터베이스에 부하를 거의 주지 않는다. (트랜잭션 로그를 읽기만 하므로 쿼리 실행 부담 없음) 모든 변경 유형(INSERT, UPDATE, DELETE)을 감지할 수 있다. DB 스키마를 변경할 필요가 없다. 트랜잭션 로그 형식이 표준화되어 있지 않다. 즉, DB 벤더(MySQL, Oracle, PostgreSQL 등)마다 로그 구조가 다르며, 버전별로 변경될 수 있다. 롤백된 변경 내역을 식별 및 제외해야 한다. 로그에는 커밋되지 않은 변경도 일시적으로 기록되기 때문에. 참고 자료 https://www.confluent.io/learn/change-data-capture" }, { "title": "Cubrid에서 MySQL로의 여정(0) - 개요", "url": "/posts/dbms-migration-intro/", "categories": "경험하기, 작업 노트", "tags": "MySQL", "date": "2025-10-18 23:00:00 +0900", "snippet": "프로젝트 개요 담당 서비스에서 사용하던 DBMS인 Cubrid를 걷어내고 MySQL로 전환 기존에 사용하던 Cubrid 9.3 버전은 이미 EOL(End of Life, 2023.03.31)이 지나 장기적인 유지보수가 어려운 상황이었음. DBA 팀에 따르면 상위 버전(10.x)으로의 업그레이드는 데이터 저장 방식과 구조 차이로 인해 마이그레이션 ...", "content": "프로젝트 개요 담당 서비스에서 사용하던 DBMS인 Cubrid를 걷어내고 MySQL로 전환 기존에 사용하던 Cubrid 9.3 버전은 이미 EOL(End of Life, 2023.03.31)이 지나 장기적인 유지보수가 어려운 상황이었음. DBA 팀에 따르면 상위 버전(10.x)으로의 업그레이드는 데이터 저장 방식과 구조 차이로 인해 마이그레이션 과정이 복잡하다는 판단이 있었음. 또한, DBA 팀의 인력 구성상 MySQL 운영이 더 수월하다는 현실적인 이유도 고려되었음. 서비스 유지보수 관점에서도, 레퍼런스가 풍부하고 다양한 빅테크 기업에서도 폭넓게 사용 중인 MySQL로 전환하는 것이 더 유리하다고 판단함.진행 과정1. MySQL용 쿼리 검증 MyBatis를 사용하고 있기 때문에, MySQL 운영을 위해서는 MySQL 쿼리 작성이 필수. 따라서 MySQL용 쿼리에 대한 쿼리 검증이 필수 검증 대상: 쿼리 문법 에러는 없는지 읽기의 경우, Cubrid와 동일한 결과값이 조회되는지 쓰기의 경우, Cubrid와 동일하게 데이터 변경이 발생했는지 2. Cubrid에서 MySQL로 데이터 마이그레이션 마이그레이션되는 동안 데이터 변경이 없도록 하기 위해, 서비스는 Read Only만 가능하게 사용자 요청이 가장 적은 새벽 시간대에 진행 Read Only로 운영되는 시간을 최소화하기 위해, 마이그레이션에 걸리는 시간을 최소화 하는 방향을 생각3. MySQL로 서비스 운영 혹시모를 Cubrid로의 롤백 상황을 대비해서 Cubrid에도 동일한 데이터 쌓는게 필요 양쪽 DB간의 최소한의 정합성 보장을 위해 Spring Data에서 제공하는 ChainedTransactionManager(Deprecated됨) 사용하여 분산 트랜잭션 처리4. Cubrid Fadeout MySQL로 일정 기간동안 운영 후 이슈 없을시, Cubrid Fadeout 관련 소스 코드 모두 제거 DB 서버 반납" }, { "title": "오늘 드는 생각 (251016, 은탄환)", "url": "/posts/daily-note-251016/", "categories": "끄적끄적", "tags": "", "date": "2025-10-16 22:00:00 +0900", "snippet": "개발자라는 직업을 갖고나서 “은탄환은 없다”라는 말을 많이 들었던 것 같다.왜 은탄환은 없는걸까 ?내 나름대로의 결론은, 모든 상황에는 저마다의 맥락(context)이 있기 때문인 것 같다.예를 들어, 특정 서비스의 DBMS를 다른 DBMS로 마이그레이션하는 프로젝트를 진행한다고 할 때 고려해야할 맥락은 다양하다: 서비스의 기술 스택, 데이터 규모,...", "content": "개발자라는 직업을 갖고나서 “은탄환은 없다”라는 말을 많이 들었던 것 같다.왜 은탄환은 없는걸까 ?내 나름대로의 결론은, 모든 상황에는 저마다의 맥락(context)이 있기 때문인 것 같다.예를 들어, 특정 서비스의 DBMS를 다른 DBMS로 마이그레이션하는 프로젝트를 진행한다고 할 때 고려해야할 맥락은 다양하다: 서비스의 기술 스택, 데이터 규모, 특성 (실시간성 여부, B2C인지 B2B인지, 주요 요청이 조회 중심인지, 쓰기 중심인지 등) 점검을 걸고 진행할 수 있는지 여부 (가능하다면 허용 가능한 중단 시간) 인력 구성 (투입 가능한 개발자 수, DBA 팀의 존재 여부 등) 프로젝트 기간 기타 등등 ..이뿐만 아니라 서비스가 속한 산업군에 따라 법률적 제약, 보안 정책 수준 등도 모두 다를 수 있다.그러므로 어떤 방법론이나 저명한 사람의 말, 혹은 Best Practice를 무조건 따르기보다, 현재 주어진 맥락 속에서 최선의 선택이 무엇일까 고민하는 개발자가 되고싶다." }, { "title": "오늘 드는 생각 (251015, 일과 공부)", "url": "/posts/daily-note-251015/", "categories": "끄적끄적", "tags": "", "date": "2025-10-15 22:00:00 +0900", "snippet": "현실적으로 생계를 유지하려면 내 인생에서 일은 계속 해야하지 않을까 생각한다.하루 24시간 중 일과 관련된 시간은: 출퇴근 (출근 준비시간 포함) : 2시간 회사에 있는 시간 : (8시간 근무 기준) 9시간11시간 정도된다.단순 계산으로 비율을 따져보면: 하루로 계산했을때, 약 45% 1년 워킹데이(대략 250일)로 계산했을때, 약 31%몇 살...", "content": "현실적으로 생계를 유지하려면 내 인생에서 일은 계속 해야하지 않을까 생각한다.하루 24시간 중 일과 관련된 시간은: 출퇴근 (출근 준비시간 포함) : 2시간 회사에 있는 시간 : (8시간 근무 기준) 9시간11시간 정도된다.단순 계산으로 비율을 따져보면: 하루로 계산했을때, 약 45% 1년 워킹데이(대략 250일)로 계산했을때, 약 31%몇 살까지 일을 할 수 있을지는 모르겠지만, 인생 전체를 놓고 보더라도 일은 내 삶에서 굉장히 많은 시간을 차지해왔고 앞으로도 그럴것 같다. 인생의 많은 부분을 차지하는 만큼, 이왕이면 그 시간들이 회사와 나 모두에게 가치있는 시간이면 좋겠다. 먼저, 회사 입장에서 내 시간이 가치있으려면 ? (= 나한테 주는 월급이 안 아까우려면)- 최소한의 리소스(시간, 서버 비용, 기술 선택 등)로 일을 처리한다.- 난이도 있는 업무를 잘 수행해낸다.- 많은 사람들의 시간을 불필요하게 갉아먹는 비효율적인 부분을 개선한다.- 맡은 서비스를 지속 성장 가능하도록 고민하면서 만든다.- 기타 등등..이건 평소에 많은 고민과 꾸준한 공부가 뒷받침돼야 가능하다고 생각한다. 다음으로, 나에게 가치있는 시간이란 ?1. 그 시간이 즐겁다.- 나는 무언가를 할 때 잘하면 즐거움을 느낀다.- 그리고 스스로 느끼기에 막힘이 없을 때 + 외부의 인정으로부터 잘한다는 것을 인지하게 되는 것 같다.2. 그 시간들이 쌓여서 얻는 결과가 값지다.- 도전적으로 느껴지는 것들을 해나가며 성장하는 것- 우리 가족에게 더 많은 것을 해줄수 있게되는 것- 누군가에게 도움을 줄 수 있는 것- 기타 등등..즉, 일하는 시간이 회사와 나에게 모두 가치있다는건: 평소에 많이 고민하고 꾸준히 공부한 역량을 바탕으로, 일을 잘해야하며, 조금 어렵고 힘들더라도 견디며, 그 과정을 통해 성장하는 것이라고 생각한다. 그럼, 어떤 공부를 하는게 좋을까 ?1. 업무를 진행할때 걸림돌이 되는 부분을 없애기 위한 공부 (도메인, 사용하는 기술 등)2. 동일한 업무를 다른 방식으로도 처리할 수 있을지(특히, 좀 더 어려운 상황인 경우를 가정해서)에 대한 공부 (기술 블로그, 발표 등 참고할 것들이 많다.)3. 현재 방식의 비효율적인 부분, 이슈가 될만한 부분을 찾고 개선하는데 필요한 공부 (먼저, 문제를 인식할 수 있는 감각을 키우는게 중요한 것 같다. =&gt; 셰프들이 미각을 훈련하는 것처럼)4. (별다른 고민없이) 관성적으로 사용하고있는 것들을 깊이 이해하기 위한 공부5. 현재 회사에 의존적이게 되는 것을 경계하기 위해, 이직을 위한 공부 (사실 앞서 적은 것들이 결국 이직할 때 가장 중요한 부분인 것 같긴하다.)꾸준히 공부는 해오고있지만, 명확한 방향성 없이 갈팡질팡하는 느낌을 받은 적들이 꽤 있던것 같은데앞으로는 나름대로 정리한 우선순위와 분명한 목적을 갖고 공부해보면 좋을 것 같다." }, { "title": "유지보수하기 어렵다 ?", "url": "/posts/thinking-about-maintenance/", "categories": "생각해보기, 유지보수", "tags": "", "date": "2025-10-13 22:00:00 +0900", "snippet": "나는 주로 어떤 상황에서 ‘유지보수하기 어렵다’라고 느낄까 ? 유지보수와 어렵다를 각각 그동안의 내 경험에 비추어 생각해보면유지보수1. 새로운 기능 추가 (또는 시스템 단위의 개발)2. 요구사항 변경 등으로 인한 기존 기능 수정3. 이슈 디버깅 및 조치4. 기능은 동작하지만 사용자 경험이 좋지않거나, 추후 이슈가 될 것으로 예상되는 부분 조치어렵다 ...", "content": "나는 주로 어떤 상황에서 ‘유지보수하기 어렵다’라고 느낄까 ? 유지보수와 어렵다를 각각 그동안의 내 경험에 비추어 생각해보면유지보수1. 새로운 기능 추가 (또는 시스템 단위의 개발)2. 요구사항 변경 등으로 인한 기존 기능 수정3. 이슈 디버깅 및 조치4. 기능은 동작하지만 사용자 경험이 좋지않거나, 추후 이슈가 될 것으로 예상되는 부분 조치어렵다 개인적으로 ‘어렵다 = 오래걸린다’ 라고 생각하며, 특히 위의 2,3번 유지보수 업무를 할 때 이런 느낌을 받을때가 많았던 것 같다. 걸림돌이 되었던 요소들을 생각해보면:1. 코드를 이해하기가 어렵다.2. 변경 사항이 기존 기능에 영향을 주지는 않는지 일일이 확인이 필요하다.또는, 영향을 주거나/받는다고 예상하기 어려운 범위라 확인을 못한다.3. 요구사항 변경으로 인해 고쳐야할 부분이 너무 많다.4. 이슈 추적의 단서가 될만한 로그가 제대로 남고 있지 않다. (ex : 에러 로그, 쿼리 로그, api 요청/응답 파라미터)5. 어떤 데이터의 값을 변경하는 로직이 여러 곳에 있고, 코드 repo도 분리되어 있다. (ex : 어드민, 배치, api 각각 별도 repo)따라서, 값의 변경을 추적하려면 어디서 변경한 것인지 이곳 저곳을 찾아야한다.6. 테스트를 위해 필요한 부분(데이터 세팅 등)이 많다.지금까지 이런 부분들을 느꼈을때, 흩날리듯 끄적여놨었는데앞으로는 상세하게 왜 어려웠는지, 어떻게 개선했는지 등을 잘 정리해놓으면 좋은 자산이 될 것 같다." }, { "title": "MySQL - Hash Join 알아보기", "url": "/posts/mysql-hash-join/", "categories": "지식 더하기, 이론", "tags": "MySQL", "date": "2025-10-04 00:20:00 +0900", "snippet": "Hash Join 해시 테이블을 사용해서 매칭되는 행을 찾는 조인 방식 Hash Join은 크게 build phase와 probe phase 두 단계로 구성된다.1. Build PhaseSELECT given_name, country_nameFROM personsJOIN countries ON persons.country_id = countries...", "content": "Hash Join 해시 테이블을 사용해서 매칭되는 행을 찾는 조인 방식 Hash Join은 크게 build phase와 probe phase 두 단계로 구성된다.1. Build PhaseSELECT given_name, country_nameFROM personsJOIN countries ON persons.country_id = countries.country_id; 한쪽 입력을 기반으로 in-memory 해시 테이블을 만든다. 이때 해시 테이블의 key는 조인 조건(여기서는 country_id)이다. 보통 더 작은 입력(바이트 크기 기준)을 해시 테이블로 만든다. 메모리에 올려야 하기 때문에 작은 쪽이 유리함. 예시에서 countries 테이블을 build input으로 선택했다고 가정하면: countries의 모든 country_id를 키로 해서 해시 테이블에 저장 이 과정을 마치면 build phase 종료 (출처 : https://dev.mysql.com/blog-archive/hash-join-in-mysql-8/)2. Probe Phase 이제 서버는 probe input(예제에서는 persons)을 읽기 시작한다. 각 row에 대해: persons.country_id 값을 키로 해서, 해시 테이블에서 매칭되는 row를 찾는다. 매칭되는 row가 있으면, 그 두 row를 합쳐서(joined row) 클라이언트로 보낸다. 즉, persons의 모든 row를 한 번씩만 읽고, 해시 테이블에서 상수 시간(constant time)으로 매칭을 찾게 된다. 이 방식이 잘 동작하려면 build input 전체를 메모리에 담을 수 있어야 한다. MySQL에서는 사용 가능한 메모리 크기를 join_buffer_size 시스템 변수가 제어한다. 이 값은 실행 중(runtime)에도 조정할 수 있다. (출처 : https://dev.mysql.com/blog-archive/hash-join-in-mysql-8/)Spill to disk build input을 메모리에 다 올릴 수 없는 경우, 여러 개의 파일로 나눠서 디스크에 저장1. Probe Phase에서의 Spill 처리 MySQL은 build input을 여러 개의 chunk 파일로 나누어 저장: 예를 들어 build input이 1GB인데 메모리는 256MB라면 대략 4개의 chunk 파일로 나눠서 디스크에 쓰는 식 단, MySQL은 내부적으로 chunk 파일 최대 128개까지만 만들 수 있음 어떤 row가 어떤 chunk 파일로 들어갈지는 조인 키(예: country_id)에 대해 해시 값을 계산해서 정한다. 즉, 같은 키 값을 가진 row들은 항상 같은 chunk 파일에 들어가게된다. 이때 사용하는 해시 함수는, 나중에 메모리 안에서 사용하는 해시 함수와는 다르다.2. Probe Phase에서의 Spill 처리 build input 해시 테이블에 일치하는 row가 있는지 찾아야하는데, build input의 일부가 디스크에 있기 때문에 probe input의 row도 디스크에 있는 chunk와 짝을 맞춰야 한다. 따라서, probe input도 build input을 chunk file에 분배할때 사용한 동일한 해시 함수로 계산해서 chunk 파일을 만든다. 이렇게 하면 조인 키가 같은 row들은 반드시 같은 chunk 쌍(= build chunk + probe chunk)에 들어가게 된다. 3. Chunk File 처리 모든 chunk 파일 쌍을 처리: 첫 번째 build chunk를 읽어서 메모리에 해시 테이블로 만든다. 그에 대응하는 probe chunk를 읽어서 해시 테이블과 매칭시킨다. 끝나면 두 번째 chunk 쌍으로 넘어간다. 이런 식으로 모든 chunk 파일 쌍을 순차적으로 처리한다. build chunk를 읽어서 메모리에 해시 테이블로 만들때의 해시 함수는 input을 chunk file로 분배할 때의 해시 함수와는 다르다. 같은 해시 함수를 쓰면, chunk 파일에 있는 row들이 전부 같은 해시 값에 몰려서, 해시 테이블이 한쪽으로 쏠리는 문제(hash collision)가 생긴다. (출처 : https://dev.mysql.com/blog-archive/hash-join-in-mysql-8/)Chunk File 삭제는 ? 조인 결과가 클라이언트에게 전달된 뒤 바로 삭제 모든 chunk 파일 쌍이 조인을 마치면 다음을 수행: HashJoinIterator::EndOfIteration() 호출 내부에서 cleanup 로직 실행 MySQL이 tmpdir에 만든 모든 chunk 파일을 닫고 삭제: 파일 핸들을 닫은 다음, OS 레벨에서 unlink() 호출을 수행 해시 테이블 메모리도 join_buffer_allocator가 release 사용 조건 유형 설명 사용 가능한 조인 조건 (1) Hash Equi Join build 테이블의 조인 키를 해시 테이블로 만들고,probe 테이블에서 같은 키 탐색 등가 조인 (=) 및 해당 조인 조건에 사용 가능한 인덱스가 없는 경우(물리적으로 인덱스가 있어도, 활용할 수 없는 경우도 포함) (2) Hash Join (no condition) 해시 테이블로 임시 저장 후,probe 단계에서 필터 조건(&gt;, &lt;, 등)으로 비교 비등가 조인, 조인 조건 없는 경우 예시-- 기본 Hash Join (equi-join), c1에 인덱스 없음SELECT * FROM t1 JOIN t2 ON t1.c1 = t2.c1;-- 필터 조건만 있는 Hash Join (조인 조건 없음)SELECT * FROM t1 JOIN t2WHERE t1.c2 &gt; 50;-- Inner Non-Equi Join (비등가 조인)SELECT * FROM t1 JOIN t2 ON t1.c1 &lt; t2.c1;-- Semijoin (IN 서브쿼리)SELECT * FROM t1WHERE t1.c1 IN ( SELECT t2.c2 FROM t2 );-- Antijoin (NOT EXISTS)SELECT * FROM t2WHERE NOT EXISTS (SELECT * FROM t1 WHERE t1.c1 = t2.c1);-- Left Outer JoinSELECT * FROM t1 LEFT JOIN t2 ON t1.c1 = t2.c1; 실행계획 결과 상황 Extra 출력 의미 등가조인 Using join buffer (hash join) 일반 hash join (키 기반) 비등가조인 (&lt;, &gt;, !=) Using where; Using join buffer (hash join) hash join (no condition) — 키 매칭 없음, 필터 조건으로 비교 조인 조건 없음 (카티션 곱) Using join buffer (hash join) 또는 Using where 없이 hash join (no condition) — 전 행 조합 메모리 관련 Hash Join에서 사용하는 메모리는 join_buffer_size 변수로 제어된다. Hash Join은 필요할 때마다(join 진행 중) 메모리를 점진적으로 할당한다. 따라서, join_buffer_size를 크게 설정해도 작은 쿼리가 불필요하게 큰 버퍼를 점유하지 않는다. 단, Outer Join의 경우 전체 버퍼를 한 번에 할당한다. Hash Join이 필요로 하는 메모리가 이 크기를 초과하면, MySQL은 디스크 임시 파일을 사용(spill)한다. 이때 디스크 파일 수가 open_files_limit을 넘으면 조인이 실패할 수도 있다.참고 자료 https://dev.mysql.com/blog-archive/hash-join-in-mysql-8/ https://dev.mysql.com/doc/refman/8.4/en/hash-joins.html" }, { "title": "MySQL - Nested-Loop Join 알아보기", "url": "/posts/mysql-nested-loop-join/", "categories": "지식 더하기, 이론", "tags": "MySQL", "date": "2025-10-01 21:00:00 +0900", "snippet": "Nested-Loop Join Algorithm MySQL은 테이블 간 조인을 수행할 때, 중첩 루프 조인(Nested-Loop Join, NLJ) 알고리즘(및 그 변형)을 사용한다. MySQL 8.0.18부터는 Hash Join도 지원 NLJ : 바깥(outer) 테이블에서 한 행씩 가져와 안쪽(inner) 테이블과 조...", "content": "Nested-Loop Join Algorithm MySQL은 테이블 간 조인을 수행할 때, 중첩 루프 조인(Nested-Loop Join, NLJ) 알고리즘(및 그 변형)을 사용한다. MySQL 8.0.18부터는 Hash Join도 지원 NLJ : 바깥(outer) 테이블에서 한 행씩 가져와 안쪽(inner) 테이블과 조인한다. outer : driving 테이블 / inner : driven 테이블 이라고도 불림 이 과정은 조인해야 할 모든 테이블에 대해 반복된다. MySQL 옵티마이저는 통계 정보(테이블 크기, 인덱스 카디널리티 등)를 기반으로 자동으로 조인 순서를 선택한다. 필요시 STRAIGHT_JOIN / 힌트를 이용해 강제할 수 있다.FOR each row in Outer { FOR each row in Inner matching join condition { OUTPUT if row satisfies conditions }}NLJ 비교 알고리즘 동작 방식 시간 복잡도 비고 Simple NLJ Outer 행마다 Inner 전체 스캔 O(N × M) 인덱스 없고 최적화 불가할 때 (거의 안 씀) Index NLJ Outer 행마다 Inner 인덱스 lookup O(N × logM) 가장 일반적인 형태, Inner 테이블에 조인 키 인덱스 존재 (데이터가 적은 테이블을 outer, 많은 테이블을 inner로 두는 것이 효율적) Block NLJ Outer 여러 행을 메모리에 담아 Inner 한 번 스캔 O(N × M) (buffer 최적화) Inner에 인덱스 없음, join_buffer_size 활용 Batched Key Access Outer 여러 키를 모아 batch로 인덱스 lookup O(N × logM) (batched) MySQL 5.6+, optimizer_switch='batched_key_access=on' Hybrid / Pushdown 변형 조건을 가능한 바깥 루프로 이동 N/A (상황 의존) EXISTS, IN 서브쿼리 최적화 등 ※ 참고 : Block NLJ는 더 이상 사용되지 않는다. 공식 문서 Hash Join Optimization 부분 참고 MySQL 8.0.20 버전 이후부터, Block Nested Loop가 더 이상 사용되지 않으며, 이전까지 Block Nested Loop이 사용되던 경우에는 언제나 Hash Join이 대신 사용된다.INNER JOIN과 NLJSELECT * FROM T1 INNER JOIN T2 ON T1.id = T2.id INNER JOIN T3 ON T2.key = T3.keyWHERE T1.value &gt; 100 AND T2.status = 'ACTIVE' AND T3.type = 'X';FOR each t1 in T1 such that (t1.value &gt; 100) { -- T1에서 바로 필터링 FOR each t2 in T2 such that (t1.id = t2.id AND t2.status='ACTIVE') { -- T2 루프에서 바로 필터링 FOR each t3 in T3 such that (t2.key = t3.key AND t3.type='X') { -- T3 루프에서 바로 필터링 OUTPUT t1||t2||t3 } }} INNER JOIN은 옵티마이저가 순서를 자유롭게 바꿀 수 있다. (T1 → T2 → T3 or T3 → T2 → T1 등)조건 푸시다운 (Condition Pushdown) WHERE 조건은 가능한 경우 바깥 루프로 올려 평가된다. 조건이 참조하는 컬럼이 이미 읽혀 있는 테이블에만 의존한다면, 그 조건은 더 바깥쪽 루프로 끌어올려 평가할 수 있다. 이를 통해 각 단계에서 row 수를 줄여, 내부 루프의 불필요한 반복을 줄인다. 위 쿼리에서 T1.value &gt; 100 조건은, T1 테이블의 컬럼만 조건에 있으므로 T1을 읽는 가장 바깥 루프에서 바로 필터링 가능. 푸시다운 불가능한 경우 : T1.value &gt; T2.some_val 두 테이블이 모두 읽힌 뒤에만 평가 가능. FOR each t1 in T1 { FOR each t2 in T2 such that (T1.id = T2.id AND T1.value &gt; T2.some_val) { ... }} WHERE 절에 있는 조건들을 보고, 어떤 조건을 어느 시점에서 평가해야 가장 효율적인지 INNER JOIN / OUTER JOIN의 제약 조건을 만족하면서 푸시 가능한지를 옵티마이저가 판단한다.OUTER JOIN과 NLJ OUTER JOIN에서는 NULL 보완(NULL-complemented row)이 추가된다. 외부 테이블의 각 행에 대해 내부 테이블과 매칭이 없으면 NULL로 채운 행이 생성된다. 따라서 OUTER JOIN은 조인 순서가 강제된다. (외부 테이블이 반드시 바깥 루프에 위치) 매칭이 실제 발생했을 때만 조건을 적용해야 하기 때문에(매칭 없을 때는 NULL 보완 행을 살려야 하므로), INNER JOIN처럼 조건 푸시다운을 단순 적용할 수 없다. 플래그(flag) 기반 조건부 푸시다운을 사용한다. SELECT * FROM T1 LEFT JOIN T2 ON T1.id = T2.id AND T2.status = 'ACTIVE' -- C2 LEFT JOIN T3 ON T2.key = T3.key AND T3.type = 'X' -- C3WHERE T1.value &gt; 100; -- C1FOR each t1 in T1 such that (t1.value &gt; 100) { -- C1은 T1에서 바로 평가 가능 f1 := FALSE -- t1이 T2와 매칭됐는지 여부 FOR each t2 in T2 such that (t1.id = t2.id) { f2 := FALSE -- (t1,t2)가 T3와 매칭됐는지 여부 FOR each t3 in T3 such that (t2.key = t3.key) { IF (t2.status='ACTIVE' AND t3.type='X') { -- 매칭된 경우에만 조건 검사 OUTPUT t1||t2||t3 } f2 = TRUE f1 = TRUE } IF !f2 { -- T3 매칭이 없을 때 IF (t2.status='ACTIVE') { -- T2 매칭은 있었으므로, 조건부 평가 OUTPUT t1||t2||NULL } f1 = TRUE } } IF !f1 { -- T2 매칭이 아예 없을 때 OUTPUT t1||NULL||NULL -- T2, T3 NULL 보완 행 출력 -- 단, WHERE 조건에서 C2, C3는 평가하지 않음 (NULL 보완이므로 무조건 통과) }}정리 JOIN 사용시 생각해볼만한 것들 inner 테이블을 인덱스 탐색하는지, outer / inner 순서는 적절한지 확인하자. 가능하면 푸시다운이 잘 적용(최대한 많은 행을 사전 필터링)될 수 있도록 WHERE 조건을 구성하자. 즉, WHERE 조건이 최대한 일찍(outer 루프) 평가될 수 있도록 OUTER JOIN 사용시: 이 조인에서 NULL 보완이 정말 필요한지, WHERE 조건 때문에 어차피 INNER JOIN처럼 동작하지 않는지 생각해보자. 가능하면 INNER JOIN으로 단순화해서 옵티마이저가 조인 순서 최적화, 조건 푸시다운을 최대한 활용하도록 유도. 즉, OUTER JOIN이 정말 필요한지 생각해보자. 참고 자료 https://dev.mysql.com/doc/refman/8.4/en/nested-loop-joins.html https://dev.mysql.com/doc/refman/8.4/en/nested-join-optimization.html" }, { "title": "오늘 드는 생각 (250923, 정답)", "url": "/posts/daily-note-250923/", "categories": "끄적끄적", "tags": "", "date": "2025-09-23 22:00:00 +0900", "snippet": "정답을 찾는데 급급해하지 않고, 정답이 아니면 어쩌나 불안해하지 않고, 적절한 근거와 논리에 기반한 내 생각을 갖고 살 수 있기를.", "content": "정답을 찾는데 급급해하지 않고, 정답이 아니면 어쩌나 불안해하지 않고, 적절한 근거와 논리에 기반한 내 생각을 갖고 살 수 있기를." }, { "title": "Spring Batch - Step 개념", "url": "/posts/spring-batch-step/", "categories": "지식 더하기, 이론", "tags": "Spring Batch", "date": "2025-09-18 22:00:00 +0900", "snippet": "개념Step Batch Job을 구성하는 독립된 작업 BATCH_STEP_EXECUTION 테이블에 저장됨 해당 Step을 포함하는 JobExecution 정보(JOB_EXECUTION_ID)도 기록됨 package org.springframework.batch.core;public interface Step {\tStrin...", "content": "개념Step Batch Job을 구성하는 독립된 작업 BATCH_STEP_EXECUTION 테이블에 저장됨 해당 Step을 포함하는 JobExecution 정보(JOB_EXECUTION_ID)도 기록됨 package org.springframework.batch.core;public interface Step {\tString STEP_TYPE_KEY = \"batch.stepType\";\tString getName();\tdefault boolean isAllowStartIfComplete() {\t\treturn false;\t}\tdefault int getStartLimit() {\t\treturn Integer.MAX_VALUE;\t}\tvoid execute(StepExecution stepExecution) throws JobInterruptedException;}@Beanpublic Step step1(JobRepository jobRepository, PlatformTransactionManager transactionManager, FlatFileItemReader&lt;Person&gt; reader, PersonItemProcessor processor, JdbcBatchItemWriter&lt;Person&gt; writer) { return new StepBuilder(\"step1\", jobRepository) .&lt;Person, Person&gt;chunk(2, transactionManager) .reader(reader) .processor(processor) .writer(writer) .allowStartIfComplete(true) .build();}// org.springframework.batch.core.job.SimpleJob#doExecute@Overrideprotected void doExecute(JobExecution execution)\t\tthrows JobInterruptedException, JobRestartException, StartLimitExceededException {\tStepExecution stepExecution = null;\tfor (Step step : steps) {\t\tstepExecution = handleStep(step, execution);\t\tif (stepExecution.getStatus() != BatchStatus.COMPLETED) {\t\t\t//\t\t\t// Terminate the job if a step fails\t\t\t//\t\t\tbreak;\t\t}\t}\t//\t// Update the job status to be the same as the last step\t//\tif (stepExecution != null) {\t\tif (logger.isDebugEnabled()) {\t\t\tlogger.debug(\"Upgrading JobExecution status: \" + stepExecution);\t\t}\t\texecution.upgradeStatus(stepExecution.getStatus());\t\texecution.setExitStatus(stepExecution.getExitStatus());\t}}StepExecution Step 실행 정보를 담고있는 객체 프로퍼티 정의 status 실행 상태를 나타내는 BatchStatus 객체. 실행 중이면 STARTED, 실패하면 FAILED, 성공하면 COMPLETED. startTime java.time.LocalDateTime 실행이 시작된 시간. Step이 시작되지 않았다면 비어 있음. endTime java.time.LocalDateTime 실행이 종료된 시간 (성공/실패 여부와 관계없음). Step이 아직 종료되지 않았다면 비어 있음. exitStatus 실행 결과를 나타내는 ExitStatus. 종료 코드를 포함해 호출자에게 반환됨. Job이 아직 종료되지 않았다면 비어 있음. executionContext 실행 간에 유지해야 할 사용자 데이터를 담는 “property bag”. readCount 성공적으로 읽은 아이템 수. writeCount 성공적으로 기록(write)한 아이템 수. commitCount 이 실행에서 커밋된 트랜잭션 수. rollbackCount Step이 제어하는 트랜잭션이 롤백된 횟수. readSkipCount 읽기 실패로 인해 스킵된 횟수. processSkipCount 처리 실패로 인해 스킵된 횟수. filterCount ItemProcessor에 의해 “필터링”된 아이템 수. writeSkipCount 쓰기 실패로 인해 스킵된 횟수. BatchStatus vs ExitStatus 보통은 BatchStatus와 ExitStatus가 모두 COMPLETED이거나 FAILED Job은 정상 수행되었지만, 아무 처리도 하지 않은 경우에는 BatchStatus는 COMPLETED, ExitStatus는 NOOP일수도 있다. 좀 더 세부적으로 처리가 필요한 경우 ExitStatus는 커스터마이징이 가능하다. BatchStatus enum, ExitStatus는 class package org.springframework.batch.core;public enum BatchStatus { COMPLETED, STARTING, STARTED, STOPPING, STOPPED, FAILED, ABANDONED, UNKNOWN; ...}package org.springframework.batch.core;...public class ExitStatus implements Serializable, Comparable&lt;ExitStatus&gt; { public static final ExitStatus UNKNOWN = new ExitStatus(\"UNKNOWN\"); public static final ExitStatus EXECUTING = new ExitStatus(\"EXECUTING\"); public static final ExitStatus COMPLETED = new ExitStatus(\"COMPLETED\"); public static final ExitStatus NOOP = new ExitStatus(\"NOOP\"); public static final ExitStatus FAILED = new ExitStatus(\"FAILED\"); public static final ExitStatus STOPPED = new ExitStatus(\"STOPPED\"); private final String exitCode; private final String exitDescription; public ExitStatus(String exitCode) { this(exitCode, \"\"); } public ExitStatus(String exitCode, String exitDescription) { super(); this.exitCode = exitCode; this.exitDescription = exitDescription == null ? \"\" : exitDescription; } ...} 예: 상황 BatchStatus ExitStatus 설명 정상 처리 완료 COMPLETED COMPLETED 아무 문제 없음 처리 중 실패 FAILED FAILED 시스템 오류 등으로 실패 데이터 없음 COMPLETED NO DATA 시스템 입장에서는 성공적으로 끝났지만, 처리할 데이터가 없음 일부 스킵 발생 COMPLETED COMPLETED WITH SKIPS 프레임워크는 정상 종료로 보지만, 비즈니스상 경고가 필요함 재처리 필요 COMPLETED RETRY NEEDED 실행 자체는 완료되었지만, 특정 조건으로 인해 다시 돌려야 함 Tasklet Chunk 기반(reader → processor → writer) 처리가 굳이 필요하지 않은 경우 (예: DB stored procedure 호출, 스크립트 실행, 단순 SQL update/delete batch 등) Tasklet 기반 Step을 사용하면 Tasklet 인터페이스를 구현한 객체의 execute 메서드를 실행한다.// org.springframework.batch.core.step.tasklet.Tasklet@FunctionalInterfacepublic interface Tasklet {\t@Nullable\tRepeatStatus execute(StepContribution contribution, ChunkContext chunkContext) throws Exception;} MethodInvokingTaskletAdapter 사용@Beanpublic MethodInvokingTaskletAdapter myTasklet() {\tMethodInvokingTaskletAdapter adapter = new MethodInvokingTaskletAdapter();\tadapter.setTargetObject(fooDao());\tadapter.setTargetMethod(\"updateFoo\");\treturn adapter;} Tasklet 인터페이스 구현public class DemoTasklet implements Tasklet { @Override public RepeatStatus execute(StepContribution contribution, ChunkContext chunkContext) throws Exception { // doSomething return null; }}코드 살펴보기 TaskletStepBuilder 사용 (아래에서 실펴볼 Chunk 기반은 SimpleStepBuilder 사용)@Beanpublic Step step1(JobRepository jobRepository, PlatformTransactionManager transactionManager, Tasklet demoTasklet) { return new StepBuilder(\"step1\", jobRepository) .tasklet(demoTasklet, transactionManager) .build();}// org.springframework.batch.core.step.builder.StepBuilder#taskletpublic TaskletStepBuilder tasklet(Tasklet tasklet, PlatformTransactionManager transactionManager) {\treturn new TaskletStepBuilder(this).tasklet(tasklet, transactionManager);}// org.springframework.batch.core.step.builder.TaskletStepBuilder#createTasklet@Overrideprotected Tasklet createTasklet() {\treturn tasklet;}Chunk 청크 지향 처리란, 데이터를 한 번에 하나씩(ItemReader) 읽고, 읽은 데이터를 모아서 청크(chunk)를 만드는 방식. 이렇게 모은 청크는 하나의 트랜잭션 경계 안에서 ItemWriter를 통해 한번에 쓰여진다. 읽은 아이템의 개수가 커밋 간격(commit interval)에 도달하면, ItemWriter가 청크 전체를 기록하고, 그 시점에 트랜잭션이 커밋된다. Reader -&gt; Processor -&gt; Writer 의사 코드List items = new Arraylist();for(int i = 0; i &lt; commitInterval; i++){ Object item = itemReader.read(); if (item != null) { items.add(item); }}List processedItems = new Arraylist();for(Object item: items){ Object processedItem = itemProcessor.process(item); if (processedItem != null) { processedItems.add(processedItem); }}itemWriter.write(processedItems);코드 살펴보기 SimpleStepBuilder 사용@Beanpublic Step step1(JobRepository jobRepository, PlatformTransactionManager transactionManager, FlatFileItemReader&lt;Person&gt; reader, PersonItemProcessor processor, JdbcBatchItemWriter&lt;Person&gt; writer) { return new StepBuilder(\"step1\", jobRepository) .&lt;Person, Person&gt;chunk(2, transactionManager) .reader(reader) .processor(processor) .writer(writer) .allowStartIfComplete(true) .build();}// org.springframework.batch.core.step.builder.SimpleStepBuilder#chunk(int)public SimpleStepBuilder&lt;I, O&gt; chunk(int chunkSize) {\tAssert.state(completionPolicy == null || chunkSize == 0,\t\t\t\"You must specify either a chunkCompletionPolicy or a commitInterval but not both.\");\tthis.chunkSize = chunkSize;\treturn this;} FaultTolerantStepBuilder 추가 가능@Beanpublic Step step1(JobRepository jobRepository, PlatformTransactionManager transactionManager, FlatFileItemReader&lt;Person&gt; reader, PersonItemProcessor processor, JdbcBatchItemWriter&lt;Person&gt; writer) { Step step1 = new StepBuilder(\"step1\", jobRepository) .&lt;Person, Person&gt;chunk(2, transactionManager) .reader(reader) .processor(processor) .writer(writer) .allowStartIfComplete(true) .faultTolerant() .skip() .retry() .reader() .build(); return step1;}// org.springframework.batch.core.step.builder.SimpleStepBuilder#faultTolerantpublic FaultTolerantStepBuilder&lt;I, O&gt; faultTolerant() {\treturn new FaultTolerantStepBuilder&lt;&gt;(this);} ChunkOrientedTasklet 구현체 생성// org.springframework.batch.core.step.builder.AbstractTaskletStepBuilderprotected abstract Tasklet createTasklet();public TaskletStep build() {\tregisterStepListenerAsChunkListener();\tTaskletStep step = new TaskletStep(getName());\tsuper.enhance(step);\tstep.setChunkListeners(chunkListeners.toArray(new ChunkListener[0]));\tif (this.transactionManager != null) {\t\tstep.setTransactionManager(this.transactionManager);\t}\tif (transactionAttribute != null) {\t\tstep.setTransactionAttribute(transactionAttribute);\t}\tif (stepOperations == null) {\t\tstepOperations = new RepeatTemplate();\t\tif (taskExecutor != null) {\t\t\tTaskExecutorRepeatTemplate repeatTemplate = new TaskExecutorRepeatTemplate();\t\t\trepeatTemplate.setTaskExecutor(taskExecutor);\t\t\trepeatTemplate.setThrottleLimit(throttleLimit);\t\t\tstepOperations = repeatTemplate;\t\t}\t\t((RepeatTemplate) stepOperations).setExceptionHandler(exceptionHandler);\t}\tstep.setStepOperations(stepOperations);\tstep.setTasklet(createTasklet()); // Tasklet 구현체 만들기\tstep.setStreams(streams.toArray(new ItemStream[0]));\ttry {\t\tstep.afterPropertiesSet();\t}\tcatch (Exception e) {\t\tthrow new StepBuilderException(e);\t}\treturn step;}// org.springframework.batch.core.step.builder.SimpleStepBuilder#createTasklet@Overrideprotected Tasklet createTasklet() {\tAssert.state(reader != null, \"ItemReader must be provided\");\tAssert.state(writer != null, \"ItemWriter must be provided\");\tRepeatOperations repeatOperations = createChunkOperations();\tSimpleChunkProvider&lt;I&gt; chunkProvider = new SimpleChunkProvider&lt;&gt;(getReader(), repeatOperations);\tSimpleChunkProcessor&lt;I, O&gt; chunkProcessor = new SimpleChunkProcessor&lt;&gt;(getProcessor(), getWriter());\tchunkProvider.setListeners(new ArrayList&lt;&gt;(itemListeners));\tchunkProvider.setMeterRegistry(this.meterRegistry);\tchunkProcessor.setListeners(new ArrayList&lt;&gt;(itemListeners));\tchunkProcessor.setMeterRegistry(this.meterRegistry);\tChunkOrientedTasklet&lt;I&gt; tasklet = new ChunkOrientedTasklet&lt;&gt;(chunkProvider, chunkProcessor);\ttasklet.setBuffering(!readerTransactionalQueue);\treturn tasklet;} ChunkOrientedTasklet도 결국 Tasklet을 구현한 클래스public class ChunkOrientedTasklet&lt;I&gt; implements Tasklet { ... @Nullable @Override public RepeatStatus execute(StepContribution contribution, ChunkContext chunkContext) throws Exception { @SuppressWarnings(\"unchecked\") Chunk&lt;I&gt; inputs = (Chunk&lt;I&gt;) chunkContext.getAttribute(INPUTS_KEY); if (inputs == null) { inputs = chunkProvider.provide(contribution); if (buffering) { chunkContext.setAttribute(INPUTS_KEY, inputs); } } chunkProcessor.process(contribution, inputs); chunkProvider.postProcess(contribution, inputs); // Allow a message coming back from the processor to say that we // are not done yet if (inputs.isBusy()) { logger.debug(\"Inputs still busy\"); return RepeatStatus.CONTINUABLE; } chunkContext.removeAttribute(INPUTS_KEY); chunkContext.setComplete(); if (logger.isDebugEnabled()) { logger.debug(\"Inputs not busy, ended: \" + inputs.isEnd()); } return RepeatStatus.continueIf(!inputs.isEnd()); }}실행 흐름// org.springframework.batch.core.job.SimpleStepHandler#handleStep@Overridepublic StepExecution handleStep(Step step, JobExecution execution)\t\tthrows JobInterruptedException, JobRestartException, StartLimitExceededException { ...\tif (shouldStart(lastStepExecution, execution, step)) {\t\tcurrentStepExecution = execution.createStepExecution(step.getName()); ...\t\ttry {\t\t\tstep.execute(currentStepExecution);\t\t\tcurrentStepExecution.getExecutionContext().put(\"batch.executed\", true);\t\t} ...\t}\treturn currentStepExecution;}// org.springframework.batch.core.step.AbstractStep#execute@Overridepublic final void execute(StepExecution stepExecution)\t\tthrows JobInterruptedException, UnexpectedJobExecutionException { ...\ttry (Observation.Scope scope = observation.openScope()) {\t\tgetCompositeListener().beforeStep(stepExecution);\t\topen(stepExecution.getExecutionContext());\t\ttry {\t\t\tdoExecute(stepExecution);\t\t}\t\tcatch (RepeatException e) {\t\t\tthrow e.getCause();\t\t}\t\texitStatus = ExitStatus.COMPLETED.and(stepExecution.getExitStatus()); ...\t}}protected abstract void doExecute(StepExecution stepExecution) throws Exception; Tasklet.execute는 하나의 트랜잭션 내에서 실행된다. (TransactionTemplate.execute의 콜백 메서드로 호출되는 ChunkTransactionCallback#doInTransaction내에서 Tasklet.execute가 호출됨) stepOperations.iterate(new StepContextRepeatCallback(stepExecution) { ... }를 통해 chunk 단위로 반복 (따라서, chunk 단위로 트랜잭션 보장됨)// org.springframework.batch.core.step.tasklet.TaskletStep#doExecute@Overrideprotected void doExecute(StepExecution stepExecution) throws Exception { ...\t// Shared semaphore per step execution, so other step executions can run\t// in parallel without needing the lock\tfinal Semaphore semaphore = createSemaphore();\tstepOperations.iterate(new StepContextRepeatCallback(stepExecution) {\t\t@Override\t\tpublic RepeatStatus doInChunkContext(RepeatContext repeatContext, ChunkContext chunkContext)\t\t\t\tthrows Exception {\t\t\tStepExecution stepExecution = chunkContext.getStepContext().getStepExecution();\t\t\t// Before starting a new transaction, check for\t\t\t// interruption.\t\t\tinterruptionPolicy.checkInterrupted(stepExecution);\t\t\tRepeatStatus result;\t\t\ttry {\t\t\t\tresult = new TransactionTemplate(transactionManager, transactionAttribute)\t\t\t\t\t.execute(new ChunkTransactionCallback(chunkContext, semaphore));\t\t\t}\t\t\tcatch (UncheckedTransactionException e) {\t\t\t\t// Allow checked exceptions to be thrown inside callback\t\t\t\tthrow (Exception) e.getCause();\t\t\t}\t\t\tchunkListener.afterChunk(chunkContext);\t\t\t// Check for interruption after transaction as well, so that\t\t\t// the interrupted exception is correctly propagated up to\t\t\t// caller\t\t\tinterruptionPolicy.checkInterrupted(stepExecution);\t\t\treturn result == null ? RepeatStatus.FINISHED : result;\t\t}\t});}// org.springframework.batch.core.step.tasklet.TaskletStep.ChunkTransactionCallback#doInTransaction@Overridepublic RepeatStatus doInTransaction(TransactionStatus status) {\tTransactionSynchronizationManager.registerSynchronization(this);\tRepeatStatus result = RepeatStatus.CONTINUABLE;\tStepContribution contribution = stepExecution.createStepContribution();\tchunkListener.beforeChunk(chunkContext);\t// In case we need to push it back to its old value\t// after a commit fails...\toldVersion = new StepExecution(stepExecution.getStepName(), stepExecution.getJobExecution());\tcopy(stepExecution, oldVersion);\ttry {\t\ttry {\t\t\ttry {\t\t\t\tresult = tasklet.execute(contribution, chunkContext);\t\t\t\tif (result == null) {\t\t\t\t\tresult = RepeatStatus.FINISHED;\t\t\t\t}\t\t\t}\t\t\tcatch (Exception e) {\t\t\t\tif (transactionAttribute.rollbackOn(e)) {\t\t\t\t\tchunkContext.setAttribute(ChunkListener.ROLLBACK_EXCEPTION_KEY, e);\t\t\t\t\tthrow e;\t\t\t\t}\t\t\t}\t\t}\t\tfinally {\t\t\t// If the step operations are asynchronous then we need\t\t\t// to synchronize changes to the step execution (at a\t\t\t// minimum). Take the lock *before* changing the step\t\t\t// execution.\t\t\ttry {\t\t\t\tsemaphore.acquire();\t\t\t\tlocked = true;\t\t\t}\t\t\tcatch (InterruptedException e) {\t\t\t\tlogger.error(\"Thread interrupted while locking for repository update\");\t\t\t\tstepExecution.setStatus(BatchStatus.STOPPED);\t\t\t\tstepExecution.setTerminateOnly();\t\t\t\tThread.currentThread().interrupt();\t\t\t}\t\t\t// Apply the contribution to the step\t\t\t// even if unsuccessful\t\t\tif (logger.isDebugEnabled()) {\t\t\t\tlogger.debug(\"Applying contribution: \" + contribution);\t\t\t}\t\t\tstepExecution.apply(contribution);\t\t}\t\tstepExecutionUpdated = true;\t\tstream.update(stepExecution.getExecutionContext());\t\ttry {\t\t\t// Going to attempt a commit. If it fails this flag will\t\t\t// stay false and we can use that later.\t\t\tif (stepExecution.getExecutionContext().isDirty()) {\t\t\t\tgetJobRepository().updateExecutionContext(stepExecution);\t\t\t}\t\t\tstepExecution.incrementCommitCount();\t\t\tif (logger.isDebugEnabled()) {\t\t\t\tlogger.debug(\"Saving step execution before commit: \" + stepExecution);\t\t\t}\t\t\tgetJobRepository().update(stepExecution);\t\t}\t\tcatch (Exception e) {\t\t\t// If we get to here there was a problem saving the step\t\t\t// execution and we have to fail.\t\t\tString msg = \"JobRepository failure forcing rollback\";\t\t\tlogger.error(msg, e);\t\t\tthrow new FatalStepExecutionException(msg, e);\t\t}\t}\tcatch (Error e) {\t\tif (logger.isDebugEnabled()) {\t\t\tlogger.debug(\"Rollback for Error: \" + e.getClass().getName() + \": \" + e.getMessage());\t\t}\t\trollback(stepExecution);\t\tthrow e;\t}\tcatch (RuntimeException e) {\t\tif (logger.isDebugEnabled()) {\t\t\tlogger.debug(\"Rollback for RuntimeException: \" + e.getClass().getName() + \": \" + e.getMessage());\t\t}\t\trollback(stepExecution);\t\tthrow e;\t}\tcatch (Exception e) {\t\tif (logger.isDebugEnabled()) {\t\t\tlogger.debug(\"Rollback for Exception: \" + e.getClass().getName() + \": \" + e.getMessage());\t\t}\t\trollback(stepExecution);\t\t// Allow checked exceptions\t\tthrow new UncheckedTransactionException(e);\t}\treturn result;} 반복 호출 (stepOperations.iterate) 내부// org.springframework.batch.repeat.support.RepeatTemplate#iterate@Overridepublic RepeatStatus iterate(RepeatCallback callback) { RepeatContext outer = RepeatSynchronizationManager.getContext(); RepeatStatus result = RepeatStatus.CONTINUABLE; try { // This works with an asynchronous TaskExecutor: the // interceptors have to wait for the child processes. result = executeInternal(callback); } finally { RepeatSynchronizationManager.clear(); if (outer != null) { RepeatSynchronizationManager.register(outer); } } return result;}private RepeatStatus executeInternal(final RepeatCallback callback) { // 반복 실행을 위한 RepeatContext 생성 (루프 상태 추적: 완료 여부, 반복 횟수 등) RepeatContext context = start(); // 이미 complete 표시된 상태라면 실행 안 함 boolean running = !isMarkedComplete(context); // 모든 리스너(open) 호출 (루프 시작 시 1회 실행) for (RepeatListener interceptor : listeners) { interceptor.open(context); // 리스너가 complete 시킬 수도 있으므로 다시 체크 running = running &amp;&amp; !isMarkedComplete(context); if (!running) break; } // 기본 결과 상태는 CONTINUABLE (계속 반복 가능) RepeatStatus result = RepeatStatus.CONTINUABLE; // 반복 상태 추적 객체 (예외 모음, 비동기 결과 등) RepeatInternalState state = createInternalState(context); Collection&lt;Throwable&gt; throwables = state.getThrowables(); // 실행 중 발생한 예외 모음 Collection&lt;Throwable&gt; deferred = new ArrayList&lt;&gt;(); // 나중에 재던질 치명적 예외 try { // 반복 루프 시작 (조건이 만족할 때까지 계속 콜백 실행) while (running) { // 각 반복(=청크 실행) 직전에 before 리스너 호출 for (RepeatListener interceptor : listeners) { interceptor.before(context); running = running &amp;&amp; !isMarkedComplete(context); // 중간에 complete 가능 } // 여전히 running 상태일 때만 콜백 실행 if (running) { try { // 콜백 실행 → 여기서 실제로 doInChunkContext 호출 // 즉, 트랜잭션 열고 청크 단위로 읽기/처리/쓰기/커밋 진행 result = getNextResult(context, callback, state); // after 리스너 호출 executeAfterInterceptors(context, result); } catch (Throwable throwable) { // 콜백 중 예외 발생 시 처리 (재시도/스킵 정책 등 적용 가능) doHandle(throwable, context, deferred); } // 반복 종료 조건: // 1) 결과가 FINISHED // 2) context가 complete 상태 // 3) deferred 예외 존재 if (isComplete(context, result) || isMarkedComplete(context) || !deferred.isEmpty()) { running = false; } } } // 비동기 실행 시 결과 기다리기 result = result.and(waitForResults(state)); // throwables 모아둔 것 처리 for (Throwable throwable : throwables) { doHandle(throwable, context, deferred); } // 내부 상태 해제 state = null; } finally { try { // deferred 예외가 남아 있다면 fatal → 그대로 재던짐 if (!deferred.isEmpty()) { Throwable throwable = deferred.iterator().next(); if (logger.isDebugEnabled()) { logger.debug(\"Handling fatal exception explicitly (rethrowing first of \" + deferred.size() + \"): \" + throwable.getClass().getName() + \": \" + throwable.getMessage()); } rethrow(throwable); } } finally { try { // 모든 리스너 close 호출 (루프 종료 시점) for (int i = listeners.length; i-- &gt; 0;) { RepeatListener interceptor = listeners[i]; interceptor.close(context); } } finally { // context 자원 정리 context.close(); } } } // 최종 결과 반환 (CONTINUABLE or FINISHED) return result;} 더이상 읽을게 없으면, RepeatStatus가 FINISHED로 된다.// org.springframework.batch.core.step.item.ChunkOrientedTasklet#execute@Nullable@Overridepublic RepeatStatus execute(StepContribution contribution, ChunkContext chunkContext) throws Exception {\t@SuppressWarnings(\"unchecked\")\tChunk&lt;I&gt; inputs = (Chunk&lt;I&gt;) chunkContext.getAttribute(INPUTS_KEY);\tif (inputs == null) {\t\tinputs = chunkProvider.provide(contribution);\t\tif (buffering) {\t\t\tchunkContext.setAttribute(INPUTS_KEY, inputs);\t\t}\t} ...\treturn RepeatStatus.continueIf(!inputs.isEnd());}// org.springframework.batch.repeat.RepeatStatuspublic static RepeatStatus continueIf(boolean continuable) {\treturn continuable ? CONTINUABLE : FINISHED;}적절한 상황 생각해보기 : Tasklet 기반 구성 vs Chunk 기반 구성 각각의 특징을 고려해서 생각해보자   Tasklet 기반 Chunk 기반 특징 - Tasklet의 구현 클래스를 직접 만들어서 처리 - Tasklet.execute가 하나의 트랜잭션으로 처리됨 - 예외 상황에서의 재시도, skip 처리 등을 직접 구현해야함 - Tasklet의 구현 클래스를 ChunkOrientedTasklet로 이미 만들어놓음 - 해당 구현체를 사용하기 위한 구성요소 필요 (reader, processor, writer) - chunkSize 단위로 트랜잭션 나눠서 처리됨 - 예외 상황에서 재시도, skip 등의 처리들을 설정(skip, retry, retryLimit, backOffPolicy 등)으로 편리하게 처리 가능 적절할 것 같은 상황 - 하나의 트랜잭션으로 처리해도 DB에 부하 및 락으로 인한 영향도가 크지 않은 경우 - 굳이 chunk 단위로 작업할 필요가 없는 경우 (특정 일자의 파일 생성/삭제, 스크립트 실행 등) - Job 실패시, 처음부터 다시 재처리해도 되는 경우(데이터가 매우 적거나, 상태값 같은게 있어서 이미 처리된건 다시 처리되지 않는 경우) - 하나의 트랜잭션으로 처리하기엔 DB에 부하 및 락으로 인한 영향도가 큰 경우 - 예외 상황 발생시 retry, skip 등의 처리가 필요한 경우 - Job 실패시, 실패 지점부터 다시 시작하는게 효율적인 경우 참고 자료 https://docs.spring.io/spring-batch/reference/domain.html https://docs.spring.io/spring-batch/reference/step/tasklet.html https://docs.spring.io/spring-batch/reference/step/chunk-oriented-processing.html https://docs.spring.io/spring-batch/reference/step/controlling-flow.html#batchStatusVsExitStatus" }, { "title": "오늘 드는 생각 (250910, 환경)", "url": "/posts/daily-note-250910/", "categories": "끄적끄적", "tags": "", "date": "2025-09-10 22:00:00 +0900", "snippet": "현재의 환경에서 좋지않은 점에 집중하여 비관적인 태도를 가지면 시간만 낭비하게되는 것 같다.주어진 환경에서 할 수 있는 최선을 다하자.기여할 부분이 보이지 않는다면, 내 역량이 부족한 것이다.", "content": "현재의 환경에서 좋지않은 점에 집중하여 비관적인 태도를 가지면 시간만 낭비하게되는 것 같다.주어진 환경에서 할 수 있는 최선을 다하자.기여할 부분이 보이지 않는다면, 내 역량이 부족한 것이다." }, { "title": "Spring Batch - Job 개념", "url": "/posts/spring-batch-job/", "categories": "지식 더하기, 이론", "tags": "Spring Batch", "date": "2025-09-05 21:25:00 +0900", "snippet": "개념Job 배치 작업(출처 : https://docs.spring.io/spring-batch/reference/domain.html) 하나의 Job은 하나 이상의 Step을 가지며, 각각의 Step은 하나의 ItemReader, 하나의 ItemProcessor, 그리고 하나의 ItemWriter를 가진다. 즉, Job은 Step 인스턴스들을 담...", "content": "개념Job 배치 작업(출처 : https://docs.spring.io/spring-batch/reference/domain.html) 하나의 Job은 하나 이상의 Step을 가지며, 각각의 Step은 하나의 ItemReader, 하나의 ItemProcessor, 그리고 하나의 ItemWriter를 가진다. 즉, Job은 Step 인스턴스들을 담는 컨테이너이다. Job은 JobLauncher를 통해 실행되며, 현재 실행 중인 프로세스에 대한 메타데이터는 JobRepository에 저장된다. JobRepository : Spring Batch에서 JobExecution과 StepExecution 같은 다양한 영속화된 도메인 객체들에 대해 기본적인 CRUD(생성, 조회, 수정, 삭제) 작업을 수행하기 위한 저장소 JobInstance 배치 작업의 실행 단위(출처 : https://docs.spring.io/spring-batch/reference/domain.html) 예를 들어 (위 그림의) EndOfDay Job의 경우 하루마다 하나의 논리적 JobInstance가 존재한다. 만약 1월 1일 실행이 처음에 실패하고 다음 날 다시 실행된다 하더라도, 그것은 여전히 1월 1일 실행이다. 실행은 보통 처리되는 데이터와 대응된다. 즉, 1월 1일 실행은 1월 1일 데이터를 처리한다는 의미이다. 즉, JobInstance는 여러 번의 실행(JobExecution)을 가질 수 있다. 특정 Job과 그것을 식별하는 JobParameters에 대응되는 JobInstance은 동시에 하나만(또는 한번만) 실행될 수 있다. JobInstance = Job + 식별용 JobParameters allowStartIfComplete 속성을 활용하면 완료된 Job을 재실행할 수 있다. BATCH_JOB_INSTANCE의 JOB_KEY가 파라미터의 해시값 파라미터 없는 경우에도 해시값 만들어짐 새로운 JobInstance를 사용하는 것은 “처음부터 시작”을 의미하며, 기존 인스턴스를 사용하는 것은 일반적으로(ExecutionContext을 활용하여) “이전 실행 지점부터 이어서 시작”을 의미한다.JobExecution ‘실행 시도’를 나타내는 개념 해당 실행과 연결된 JobInstance는 실행이 성공적으로 완료될 때까지 완료된 것으로 간주되지 않는다. 예를 들어 앞서 설명한 EndOfDay Job을 기준으로, 2017년 1월 1일의 JobInstance가 첫 실행에서 실패했다고 가정해 보자. 동일한 식별 JobParameters(2017-01-01)로 다시 실행하면 새로운 JobExecution이 생성된다. 그러나 여전히 JobInstance는 하나뿐이다.예시 상황 EndOfDayJob 2017-01-01 실행 (21:00 시작 → 21:30 실패) 다음날 같은 파라미터로 재실행 → 성공 (21:00 → 21:30) 이어서 2017-01-02 실행 → 성공 (21:31 → 22:29) BATCH_JOB_INSTANCE 테이블 JOB_INST_ID JOB_NAME 1 EndOfDayJob 2 EndOfDayJob BATCH_JOB_EXECUTION 테이블 JOB_EXEC_ID JOB_INST_ID START_TIME END_TIME STATUS 1 1 2017-01-01 21:00 2017-01-01 21:30 FAILED 2 1 2017-01-02 21:00 2017-01-02 21:30 COMPLETED 3 2 2017-01-02 21:31 2017-01-02 22:29 COMPLETED 코드 살짝 들여다보기Job 계층구조Job (Interface) └── AbstractJob (Abstract Class) ├── SimpleJob (Concrete Class) └── FlowJob (Concrete Class)// org.springframework.batch.core.job.AbstractJobabstract protected void doExecute(JobExecution execution) throws JobExecutionException;@Overridepublic final void execute(JobExecution execution) {\tAssert.notNull(execution, \"jobExecution must not be null\");\texecution.getExecutionContext().put(SpringBatchVersion.BATCH_VERSION_KEY, SpringBatchVersion.getVersion()); ...\ttry (Observation.Scope scope = observation.openScope()) {\t\tjobParametersValidator.validate(execution.getJobParameters());\t\tif (execution.getStatus() != BatchStatus.STOPPING) {\t\t\texecution.setStartTime(LocalDateTime.now());\t\t\tupdateStatus(execution, BatchStatus.STARTED);\t\t\tlistener.beforeJob(execution);\t\t\ttry {\t\t\t\tdoExecute(execution); ...// org.springframework.batch.core.job.SimpleJob#doExecute@Overrideprotected void doExecute(JobExecution execution) throws JobInterruptedException, JobRestartException, StartLimitExceededException { StepExecution stepExecution = null; for (Step step : steps) { stepExecution = handleStep(step, execution); if (stepExecution.getStatus() != BatchStatus.COMPLETED) { // // Terminate the job if a step fails // break; } } // // Update the job status to be the same as the last step // if (stepExecution != null) { if (logger.isDebugEnabled()) { logger.debug(\"Upgrading JobExecution status: \" + stepExecution); } execution.upgradeStatus(stepExecution.getStatus()); execution.setExitStatus(stepExecution.getExitStatus()); }}JobInstance는 JobParameters로 식별// org.springframework.batch.core.repository.dao.JdbcJobInstanceDao#createJobInstance@Overridepublic JobInstance createJobInstance(String jobName, JobParameters jobParameters) { Assert.notNull(jobName, \"Job name must not be null.\"); Assert.notNull(jobParameters, \"JobParameters must not be null.\"); Assert.state(getJobInstance(jobName, jobParameters) == null, \"JobInstance must not already exist\"); Long jobInstanceId = jobInstanceIncrementer.nextLongValue(); JobInstance jobInstance = new JobInstance(jobInstanceId, jobName); jobInstance.incrementVersion(); Object[] parameters = new Object[] { jobInstanceId, jobName, jobKeyGenerator.generateKey(jobParameters), jobInstance.getVersion() }; getJdbcTemplate().update(getQuery(CREATE_JOB_INSTANCE), parameters, new int[] { Types.BIGINT, Types.VARCHAR, Types.VARCHAR, Types.INTEGER }); return jobInstance;}특정 Job과 그것을 식별하는 JobParameters에 대응되는 JobInstance은 한번만 실행 가능 관련 에러 메세지batch.core.job.SimpleStepHandler : Step already complete or not restartable, so no action to execute: StepExecution: id=10, version=5, name=step1, status=COMPLETED, exitStatus=COMPLETED, ... 호출 흐름 job이름 , 파라미터로 실행 내역을 찾는다.// org.springframework.batch.core.launch.support.TaskExecutorJobLauncher#runjobExecution = jobRepository.createJobExecution(job.getName(), jobParameters);\t\ttry {\t\t\ttaskExecutor.execute(new Runnable() {\t\t\t\t@Override\t\t\t\tpublic void run() { ...\t\t\t\t\t\tjob.execute(jobExecution); 찾은 실행 내역에서 JobInstance를 가져온다.// org.springframework.batch.core.job.SimpleStepHandler#handleStepJobInstance jobInstance = execution.getJobInstance();StepExecution lastStepExecution = jobRepository.getLastStepExecution(jobInstance, step.getName());if (stepExecutionPartOfExistingJobExecution(execution, lastStepExecution)) {\t// If the last execution of this step was in the same job, it's\t// probably intentional so we want to run it again...\tif (logger.isInfoEnabled()) {\t\tlogger.info(String.format(\t\t\t\t\"Duplicate step [%s] detected in execution of job=[%s]. \"\t\t\t\t\t\t+ \"If either step fails, both will be executed again on restart.\",\t\t\t\tstep.getName(), jobInstance.getJobName()));\t}\tlastStepExecution = null;}StepExecution currentStepExecution = lastStepExecution;if (shouldStart(lastStepExecution, execution, step)) { ...} jobRepository.getLastStepExecution 호출로 실행되는 쿼리-- org.springframework.batch.core.repository.dao.JdbcStepExecutionDao#GET_LAST_STEP_EXECUTIONSELECT ...FROM BATCH_JOB_EXECUTION JE JOIN BATCH_STEP_EXECUTION SE ON SE.JOB_EXECUTION_ID = JE.JOB_EXECUTION_IDWHERE JE.JOB_INSTANCE_ID = ? AND SE.STEP_NAME = ? StepExecution의 stepStatus 가 COMPLETED인데 allowStartIfComplete 속성이 false이거나 stepStatus가 ABANDONED 이면 특정 job을 동일한 파라미터로 재실행 불가능// org.springframework.batch.core.job.SimpleStepHandler#shouldStartprotected boolean shouldStart(StepExecution lastStepExecution, JobExecution jobExecution, Step step)\t\tthrows JobRestartException, StartLimitExceededException {\tBatchStatus stepStatus;\tif (lastStepExecution == null) {\t\tstepStatus = BatchStatus.STARTING;\t}\telse {\t\tstepStatus = lastStepExecution.getStatus();\t} ...\tif ((stepStatus == BatchStatus.COMPLETED &amp;&amp; !step.isAllowStartIfComplete())\t\t\t|| stepStatus == BatchStatus.ABANDONED) {\t\t// step is complete, false should be returned, indicating that the\t\t// step should not be started\t\tif (logger.isInfoEnabled()) {\t\t\tlogger.info(\"Step already complete or not restartable, so no action to execute: \" + lastStepExecution);\t\t}\t\treturn false;\t} ...}특정 Job과 그것을 식별하는 JobParameters에 대응되는 JobInstance은 동시에 하나만 실행 가능 관련 에러 메세지Caused by: org.springframework.batch.core.repository.JobExecutionAlreadyRunningException: A jobexecution for this job is already running: JobInstance: id=1, ...// org.springframework.batch.core.repository.support.SimpleJobRepository#createJobExecution@Overridepublic JobExecution createJobExecution(String jobName, JobParameters jobParameters)\t\tthrows JobExecutionAlreadyRunningException, JobRestartException, JobInstanceAlreadyCompleteException {\tAssert.notNull(jobName, \"Job name must not be null.\");\tAssert.notNull(jobParameters, \"JobParameters must not be null.\");\tJobInstance jobInstance = jobInstanceDao.getJobInstance(jobName, jobParameters);\tExecutionContext executionContext;\tif (jobInstance != null) {\t\tList&lt;JobExecution&gt; executions = jobExecutionDao.findJobExecutions(jobInstance);\t\tif (executions.isEmpty()) {\t\t\tthrow new IllegalStateException(\"Cannot find any job execution for job instance: \" + jobInstance);\t\t}\t\tfor (JobExecution execution : executions) {\t\t\tif (execution.isRunning()) {\t\t\t\tthrow new JobExecutionAlreadyRunningException(\t\t\t\t\t\t\"A job execution for this job is already running: \" + jobInstance);\t\t\t} ...\t\t}\t\texecutionContext = ecDao.getExecutionContext(jobExecutionDao.getLastJobExecution(jobInstance));\t} ...\treturn jobExecution;}// org.springframework.batch.core.BatchStatuspublic enum BatchStatus { /** * The batch job has successfully completed its execution. */ COMPLETED, /** * Status of a batch job prior to its execution. */ STARTING, /** * Status of a batch job that is running. */ STARTED, /** * Status of batch job waiting for a step to complete before stopping the batch job. */ STOPPING, /** * Status of a batch job that has been stopped by request. */ STOPPED, /** * Status of a batch job that has failed during its execution. */ FAILED, /** * Status of a batch job that did not stop properly and can not be restarted. */ ABANDONED, /** * Status of a batch job that is in an uncertain state. */ UNKNOWN; public boolean isRunning() { return this == STARTING || this == STARTED || this == STOPPING; } ...}참고 자료 https://docs.spring.io/spring-batch/reference/domain.html https://docs.spring.io/spring-batch/reference/job/configuring-repository.html" }, { "title": "Gradle 알아보기", "url": "/posts/what-is-gradle/", "categories": "지식 더하기, 이론", "tags": "Gradle", "date": "2025-08-26 21:25:00 +0900", "snippet": "Gradle 프로젝트 구조project ├── gradle/ (1) │ ├── libs.versions.toml (2) │ └── wrapper/ │ ├── gradle-wrapper.jar │ └── gradle-wrapper.properties │ ├── gradle...", "content": "Gradle 프로젝트 구조project ├── gradle/ (1) │ ├── libs.versions.toml (2) │ └── wrapper/ │ ├── gradle-wrapper.jar │ └── gradle-wrapper.properties │ ├── gradlew (3) ├── gradlew.bat (3) ├── settings.gradle(.kts) (4) ├── subproject-a/ │ ├── build.gradle(.kts) (5) │ └── src/ (6) └── subproject-b/ ├── build.gradle(.kts) (5) └── src/ (6)(1) : gradle/ Gradle 디렉터리, wrapper 파일과 기타 파일 저장소.(2) : libs.versions.toml Gradle version catalog 파일, 의존성 관리용. 라이브러리 버전을 한 곳에서 관리하는 카탈로그. 예시:[versions]spring-boot = \"3.2.0\"junit = \"5.10.0\"[libraries]spring-boot-starter-web = { module = \"org.springframework.boot:spring-boot-starter-web\", version.ref = \"spring-boot\" }junit-jupiter = { module = \"org.junit.jupiter:junit-jupiter\", version.ref = \"junit\" } 이렇게 정의해 두면 각 build.gradle에서 버전을 직접 쓰지 않고 이름으로 불러올 수 있음(3) : gradlew, gradlew.bat Gradle wrapper 스크립트 gradlew (리눅스/맥), gradlew.bat (윈도우) 로컬에 Gradle이 없어도, 프로젝트가 지정한 Gradle 버전을 자동으로 다운로드해서 실행 가능. 즉, 모든 개발자가 같은 버전의 Gradle을 쓰게 보장.(4) : settings.gradle 루트 프로젝트 이름과 서브프로젝트를 정의하는 Gradle 설정 파일. 멀티 프로젝트의 구성도 같은 것. 예시: rootProject.name = \"my-multi-app\"include(\"subproject-a\", \"subproject-b\") 이렇게 해야 Gradle이 subproject-a, subproject-b를 인식. 멀티 모듈 프로젝트에서는 해당 파일이 반드시 필요함 (단일 모듈 프로젝트에서는 없어도됨)(5) : build.gradle 각 서브프로젝트의 빌드 스크립트 각 모듈(예: subproject-a, subproject-b)이 어떤 언어, 어떤 의존성, 어떤 플러그인을 쓰는지 등을 정의plugins { id(\"java\")}dependencies { implementation(libs.spring.boot.starter.web) testImplementation(libs.junit.jupiter)}(6) : src/ 프로젝트의 소스 코드 및 기타 파일.src/ ├── main/java/... # 실제 애플리케이션 코드 ├── main/resources/ # 설정 파일, 정적 리소스 └── test/java/... # 테스트 코드핵심 개념 Projects, Build Scripts, Dependencies, Tasks, Plugins1. Projects Gradle 프로젝트는 애플리케이션이나 라이브러리처럼 빌드할 수 있는 소프트웨어 조각이다. 단일 프로젝트 빌드는 루트 프로젝트라 불리는 단일 프로젝트를 포함한다. 멀티 프로젝트 빌드는 하나의 루트 프로젝트와 여러 개의 하위 프로젝트를 포함한다.my-app/ └── build.gradlemy-multi-app/ ├── build.gradle # 루트 프로젝트 ├── settings.gradle # 어떤 서브프로젝트 있는지 선언 ├── app/ # 서브 프로젝트1 │ └── build.gradle └── library/ # 서브 프로젝트2 └── build.gradle// settings.gradlerootProject.name = \"my-multi-app\"include(\"app\", \"library\")2. Build Scripts 빌드 스크립트는 Gradle에게 프로젝트를 빌드하기 위해 어떤 단계를 수행해야 하는지를 알려준다. 각 프로젝트는 하나 이상의 빌드 스크립트를 포함할 수 있다. 예 : “이 프로젝트는 자바야, 이 라이브러리 필요해”라고 Gradle에게 알려주는 설명서.// build.gradleplugins { id 'java' // 자바 관련 Task들 자동 등록}group = 'com.example'version = '1.0.0'repositories { mavenCentral() // 외부 라이브러리를 여기서 가져옴}dependencies { implementation 'org.springframework.boot:spring-boot-starter-web:3.2.0' testImplementation 'org.junit.jupiter:junit-jupiter:5.10.0'}3. Dependencies and Dependency Management 프로젝트에 필요한 외부 리소스(=의존성)를 선언하고 해결하는 자동화된 기법 의존성(Dependencies)에는 JAR, 라이브러리, 소스 코드 등이 포함되며, 이는 프로젝트 빌드를 지원한다. Gradle은 이러한 의존성을 자동으로 다운로드하고, 캐싱하고, 해결해주기 때문에 개발자가 직접 관리할 필요가 없다. 또한 Gradle은 버전 충돌 처리를 해주고, 유연한 버전 선언 방식도 지원한다.dependencies { implementation 'org.springframework.boot:spring-boot-starter-web:3.2.0'} Gradle에서는 의존성을 Configuration(구성) 별로 그룹화하여, 언제/어떻게 사용되는지를 정의한다. implementation: 프로덕션 코드의 컴파일 및 실행에 필요한 의존성 api: 라이브러리 소비자에게도 공개되어야 하는 의존성 (즉, 라이브러리를 쓰는 쪽에서도 접근 가능해야 하는 경우) 의존성 트리를 확인하려면 dependencies 작업을 실행할 수 있다. 예: :app 프로젝트의 의존성을 보려면 $ ./gradlew :app:dependencies 버전 카탈로그 빌드 전체에서 의존성 좌표(group:name:version)와 버전을 중앙 집중적이고 일관되게 관리하는 방법을 제공한다.각 build.gradle(.kts) 파일마다 버전을 직접 적는 대신, libs.versions.toml 파일에 한 번만 정의하고 공통으로 가져다 쓸 수 있다. 이를 통해 다음이 쉬워진다: 여러 서브프로젝트 간 공통 의존성 선언 공유 중복 및 버전 불일치 방지 대규모 프로젝트에서 의존성과 플러그인 버전 강제 일반적으로 libs.versions.toml 파일에는 4개의 섹션이 있다: [versions] → 플러그인 및 라이브러리가 참조할 버전 번호 선언 [libraries] → 빌드 파일에서 사용할 라이브러리 정의 [bundles] → 여러 의존성을 묶어서 세트로 정의 [plugins] → 플러그인 정의 예시:[versions]guava = \"32.1.2-jre\"juneau = \"8.2.0\"[libraries]guava = { group = \"com.google.guava\", name = \"guava\", version.ref = \"guava\" }juneau-marshall = { group = \"org.apache.juneau\", name = \"juneau-marshall\", version.ref = \"juneau\" }// build.gradledependencies { implementation(libs.guava) api(libs.juneau.marshall)}Dependency Configurations Gradle 프로젝트에서 선언되는 모든 의존성은 특정 범위(scope)에 적용된다. Dependency Configuration(의존성 구성)은 프로젝트 안에서 서로 다른 목적에 맞게 의존성을 분리하여 정의하는 방법이다. 이것은 빌드 과정의 여러 단계(컴파일, 실행, 테스트 등)에서 의존성이 언제, 어떻게 사용될지를 결정한다. Configurations는 Gradle의 의존성 해석(dependency resolution)에서 기본적인 역할을 한다. Java Library Plugin을 적용하면 프로젝트가 Java 라이브러리를 생산할 수 있고, 이 플러그인은 여러 Configuration을 자동으로 추가한다. 예: 소스 코드 컴파일, 테스트 실행 등 다양한 classpath에 필요한 의존성 그룹. Configuration 설명 비고 api 컴파일 &amp; 실행 시 모두 필요한 의존성. 또한 배포된 API에 포함된다. java-library 플러그인 전용 implementation 컴파일 &amp; 실행 시 필요한 의존성. 하지만 API로 노출되지는 않는다. 일반적인 경우 가장 많이 사용 compileOnly 컴파일 시에만 필요한 의존성. 실행(runtime)이나 배포 시에는 포함되지 않는다. 예: Lombok, Annotation Processor compileOnlyApi 컴파일 시에만 필요하지만, 배포된 API에는 포함된다.   runtimeOnly 실행 시에만 필요한 의존성. 컴파일 classpath에는 없음. 예: JDBC Driver, 로깅 구현체 testImplementation 테스트 코드 컴파일 &amp; 실행에 필요한 의존성. 예: JUnit testCompileOnly 테스트 코드 컴파일에만 필요한 의존성. 실행에는 불필요.   testRuntimeOnly 테스트 실행 시에만 필요한 의존성.   annotationProcessor는 목록에 없을까? 이유는, annotationProcessor는 Java 플러그인/Java Library 플러그인에서 추가적으로 정의한 특수 Configuration이기 때문 원래 Gradle 기본 문서에서 “주요 Configuration”만 소개했을 때는 annotationProcessor가 빠져 있음 annotationProcessor는 다음을 위해 사용됨: 컴파일 시에 애노테이션 프로세서를 실행해야 할 때 (예: Lombok, MapStruct) 실행(runtime)이나 API 배포와는 무관하게, 오직 컴파일 단계에서만 필요 4. Tasks 작업(Task)은 코드를 컴파일하거나 테스트를 실행하는 것 같은 독립적인 작업 단위이다. 일반적인 작업(Task)의 유형에는 다음이 포함된다: 소스 코드 컴파일 테스트 실행 출력물 패키징 (예: JAR 또는 APK 생성) 문서 생성 (예: Javadoc) 빌드 산출물을 저장소에 배포 각 작업은 독립적이지만, 다른 작업이 먼저 실행되어야만 수행될 수도 있다. Gradle은 이 정보를 사용해서 가장 효율적인 순서로 작업들을 실행하며, 이미 최신 상태인 작업은 건너뛴다. Gradle 플러그인과 빌드 스크립트는 프로젝트에서 사용할 수 있는 작업들을 정의한다. $ ./gradlew tasks로 확인 가능 Task 실행 ./gradlew &lt;Task명&gt; 작업을 실행하려면 프로젝트 루트 디렉터리에서 Gradle Wrapper를 사용한다. 예를 들어, build 작업을 실행하려면: ./gradlew build 이 명령은 build 작업과 그에 필요한 모든 의존 작업들을 실행한다. Gradle은 어떤 작업이 다른 어떤 작업에 의존하는지 알고 있으며, 올바른 순서로 자동으로 실행한다.$ ./gradlew build&gt; Task :app:compileJava&gt; Task :app:processResources NO-SOURCE&gt; Task :app:classes&gt; Task :app:jar&gt; Task :app:startScripts&gt; Task :app:distTar&gt; Task :app:distZip&gt; Task :app:assemble&gt; Task :app:check&gt; Task :app:buildBUILD SUCCESSFUL in 764ms7 actionable tasks: 7 executed5. Plugins Gradle 빌드 시스템에 추가 기능을 제공하는 재사용 가능한 소프트웨어 조각 추가 기능 예: 새로운 작업(Task)을 빌드에 추가한다 (예: compileJava, test) 새로운 설정(Configuration)을 추가한다 (예: implementation, runtimeOnly) 새로운 DSL(Domain Specific Language, 빌드 스크립트에서 쓸 수 있는 전용 문법) 요소를 제공한다 (예: application {}, publishing {}) 즉, Gradle은 기본적으로 최소 기능만 있고 플러그인을 붙이면 기능이 늘어남. 예시:plugins { id(\"java\") // java 플러그인을 적용}dependencies { implementation(\"org.springframework.boot:spring-boot-starter-web\") // 코드 컴파일 시 필요한 라이브러리 testImplementation(\"org.junit.jupiter:junit-jupiter:5.10.0\") // 테스트 전용 라이브러리 runtimeOnly(\"com.h2database:h2\") // 실행 시에만 필요한 라이브러리} java 플러그인을 적용하기 전에는 implementation, testImplementation, runtimeOnly 같은 Configuration이 없음. 플러그인을 적용하면서 Gradle이 이런 “의존성 그룹”을 자동으로 만들어줌.plugins { id(\"org.springframework.boot\") version \"3.2.0\"}tasks.named&lt;org.springframework.boot.gradle.tasks.run.BootRun&gt;(\"bootRun\") { args(\"--spring.profiles.active=dev\")} 플러그인은 특정 도메인이나 워크플로우에 필요한 모든 로직을 가져온다. 도메인 = “특정한 문제 영역” (예: Java 빌드, Spring Boot 실행, Android 앱 패키징 등) 플러그인은 그 도메인에 필요한 Task, Configuration, DSL 블록을 한꺼번에 제공한다. 즉, 어떤 워크플로우(흐름)를 실행하기 위해 필요한 것들을 전부 제공한다. Spring Boot 플러그인을 적용하면: bootRun, bootJar 같은 Task 자동 추가 developmentOnly, runtimeClasspath 같은 Configuration 추가 springBoot {} 같은 DSL 블록 제공 결과적으로 Spring Boot 앱 빌드/실행/배포에 필요한 전 과정을 자동화할 수 있음. 플러그인 종류 Gradle은 세 가지 타입의 플러그인을 지원한다1. 스크립트 플러그인 (Script plugins) 다른 Gradle 스크립트 파일(.gradle / .gradle.kts)을 불러다 쓰는 방식. apply from: 문법으로 적용. 흔히 공통 설정을 common.gradle 같은 파일로 분리해서 여러 프로젝트에서 재사용할 때 사용.// common.gradletasks.register(\"hello\") { doLast { println(\"Hello from common.gradle\") }}// build.gradleapply from: \"common.gradle\"2. 사전 컴파일된 플러그인 (Pre-compiled plugins) Kotlin 또는 Groovy 코드로 패키징된 플러그인. 직접 플러그인 클래스를 작성해서 프로젝트 내에서 재사용. buildSrc/ 디렉터리에 넣으면 자동으로 인식됨.project-root/ ├── buildSrc/ │ └── src/main/kotlin/MyConventionPlugin.kt └── build.gradle.kts// buildSrc/src/main/kotlin/MyConventionPlugin.ktimport org.gradle.api.Pluginimport org.gradle.api.Projectclass MyConventionPlugin : Plugin&lt;Project&gt; { override fun apply(project: Project) { project.tasks.register(\"greet\") { doLast { println(\"Hello from MyConventionPlugin!\") } } }}// build.gradle.ktsplugins { id(\"my-convention-plugin\") // 클래스 이름 기반으로 자동 인식}3. 바이너리 플러그인 (Binary plugins) 패키징·배포된 플러그인(Plugin Portal 또는 Maven에 올라온 것). 대부분 우리가 쓰는 유명한 플러그인들 (예: java, application, spring-boot, spotless)plugins { id(\"java-library\") // Gradle 코어 플러그인 (버전 지정 필요 없음) id(\"org.springframework.boot\") version \"3.2.0\" // 커뮤니티 플러그인 id(\"com.diffplug.spotless\") version \"6.25.0\" // 코드 포맷팅 플러그인}플러그인 배포 경로 Gradle 플러그인은 여러 소스에서 제공되며, 상황에 맞게 선택할 수 있다.1. 코어 플러그인 (Core Plugins) Gradle 배포판에 포함된 플러그인. (목록) Gradle 팀에서 관리한다. 예: java-libraryplugins { id(\"java-library\")}2. 커뮤니티 플러그인 (Community Plugins, Plugin Portal) Gradle 커뮤니티가 만든 플러그인. Gradle Plugin Portal에 공개되어 있고, ID와 버전으로 적용 가능.plugins { id(\"org.springframework.boot\").version(\"3.1.5\")}3. 로컬/커스텀 플러그인 (Local or Custom Plugins) 직접 만든 플러그인. 단일 프로젝트에서만 쓰거나 여러 프로젝트에서 공유할 수 있다. 가장 흔한 형태는 Convention Plugin. buildSrc/ 디렉토리나 별도 build-logic 모듈 안에 위치. Kotlin/Groovy로 작성. Gradle Wrapper Gradle Wrapper는 항상 신뢰할 수 있고, 제어되고, 표준화된 빌드 실행을 보장하기 때문에 권장되는 Gradle 빌드 방식이다. Wrapper 스크립트는 선언된 Gradle 버전을 호출하며, 필요하다면 먼저 그것을 다운로드한다. 프로젝트 루트 디렉터리에서 gradlew 또는 gradlew.bat 파일로 제공된다:root├── gradlew // THE WRAPPER├── gradlew.bat // THE WRAPPER└── ... 프로젝트에 이 파일들이 없다면, 그것은 아마도 Gradle 프로젝트가 아니거나, Wrapper가 아직 설정되지 않은 것일 수 있다. Wrapper는 인터넷에서 직접 다운로드하는 것이 아니다. Gradle이 설치된 머신에서 gradle wrapper 명령을 실행하여 생성해야 한다. 출처 : https://docs.gradle.org/current/userguide/gradle_wrapper_basics.htmlWrapper가 제공하는 이점 지정된 Gradle 버전을 자동으로 다운로드하고 사용한다. 프로젝트를 특정 Gradle 버전에 표준화한다. 서로 다른 사용자 및 환경(IDE, CI 서버 등)에 동일한 Gradle 버전을 제공한다. Gradle을 직접 수동 설치하지 않고도 Gradle 빌드를 쉽게 실행할 수 있게 해준다.만약 실행 환경별로 버전이 다르면 ?1. 빌드 스크립트 문법 차이 Gradle은 계속 발전하면서 DSL(Groovy/Kotlin) 문법이 추가·변경·Deprecated 됨 예: 예전 버전에서 쓰던 compile-&gt; implementation으로 변경 Gradle 4.x를 쓰면 compile이 잘 동작하지만, Gradle 7.x에선 에러 발생2. 플러그인 호환성 문제 Spring Boot, Kotlin, Android 등은 Gradle 버전에 맞춰 플러그인을 업데이트 특정 버전의 Gradle에서는 플러그인이 아예 동작하지 않거나 경고/에러가 발생할 수 있음. 예: 최신 Android Gradle Plugin은 최소 Gradle 7.x 이상을 요구. 누군가는 6.x를 쓰면 빌드 자체가 불가능. CI/CD 서버나 다른 개발자 환경에서 빌드 깨짐.3. 빌드 결과물 차이 같은 코드를 빌드해도 Gradle 내부 동작(캐싱, 병렬 빌드 방식, dependency resolution 방식)이 버전에 따라 달라집니다. 예: Gradle 6.x와 7.x는 의존성 중복 처리 방식이 달라서, 한쪽은 충돌 나고 다른 한쪽은 자동 해결될 수도 있음. 로컬에서 테스트한 JAR/WAR는 잘 돌아가는데, 서버에서 빌드된 건 오류가 날 수 있음.4. CI/CD 환경과 로컬 환경 불일치 로컬 개발자는 Gradle 8.x, CI 서버는 Gradle 7.x → 서버에서만 빌드 에러 발생. 빌드 실패 원인을 파악하기 어려워지고, 시간을 허비하게 됨.Wrapper 구성 요소.├── gradle│ └── wrapper│ ├── gradle-wrapper.jar│ └── gradle-wrapper.properties├── gradlew└── gradlew.bat gradle-wrapper.jar 작은 JAR 파일로, Gradle Wrapper 코드를 담고 있다. 프로젝트에 필요한 Gradle 버전이 설치되어 있지 않으면 이를 다운로드하고 설치하는 역할을 한다. gradle-wrapper.properties Gradle Wrapper에 대한 설정 속성을 담고 있는 파일이다. 예: Gradle을 어디서 다운로드할지(배포 URL), 배포 유형(ZIP 또는 TARBALL). gradlew Unix 계열 시스템에서 사용하는 셸 스크립트. gradle-wrapper.jar를 감싸는 역할을 하며, Gradle을 수동 설치하지 않고도 Gradle 작업을 실행할 수 있다. gradlew.bat Windows에서 사용하는 배치 스크립트. 목적은 gradlew와 동일하다. 실행 흐름 예시 ./gradlew build gradlew (shell script) 동작: CLASSPATH=$APP_HOME/gradle/wrapper/gradle-wrapper.jar 설정 Java 실행기로 gradle-wrapper.jar를 실행 이때, gradle-wrapper.properties 파일도 읽어서 “Gradle 몇 버전, 어디서 받을지” 확인 gradle-wrapper.jar 동작: 지정된 버전의 Gradle 설치 여부 확인 없으면 다운로드 (ZIP이나 TAR) 해당 Gradle 런처를 호출해서 실제 빌드 실행 주의 사항 이 파일들은 절대 직접 수정해서는 안 된다. Gradle 버전을 확인하거나 업데이트하려면 명령줄에서 다음 명령을 사용한다:$ ./gradlew --version$ ./gradlew wrapper --gradle-version 7.2참고 자료 https://docs.gradle.org/current/userguide/gradle_basics.html https://docs.gradle.org/current/userguide/dependency_management_basics.html https://docs.gradle.org/current/userguide/dependency_configurations.html https://docs.gradle.org/current/userguide/task_basics.html https://docs.gradle.org/current/userguide/task_basics.html" }, { "title": "Spring Batch - Multi-threaded Step 알아보기", "url": "/posts/spring-batch-multi-threaded-step/", "categories": "지식 더하기, 이론", "tags": "Spring Batch", "date": "2025-08-23 12:25:00 +0900", "snippet": "개요 Spring Batch는 병렬 처리를 위한 다양한 옵션을 제공한다. 큰 틀에서 병렬 처리는 두 가지 방식이 있다: Single-process Multi-threaded Step Parallel Steps Multi-process ...", "content": "개요 Spring Batch는 병렬 처리를 위한 다양한 옵션을 제공한다. 큰 틀에서 병렬 처리는 두 가지 방식이 있다: Single-process Multi-threaded Step Parallel Steps Multi-process Remote Chunking of Step Partitioning a Step (Single or Multi-process) Multi-threaded Step 병렬 처리를 시작하는 가장 단순한 방법은 Step 설정에 TaskExecutor를 추가하는 것@Beanpublic TaskExecutor taskExecutor() { return new SimpleAsyncTaskExecutor(\"spring_batch\");}@Beanpublic Step sampleStep(TaskExecutor taskExecutor, JobRepository jobRepository, PlatformTransactionManager transactionManager) {\treturn new StepBuilder(\"sampleStep\", jobRepository)\t\t\t\t.&lt;String, String&gt;chunk(10, transactionManager)\t\t\t\t.reader(itemReader())\t\t\t\t.writer(itemWriter())\t\t\t\t.taskExecutor(taskExecutor) .throttleLimit(20) // default : 4 (스프링 배치 5.0부터는 deprecated) .build();} 위 설정을 통해, Step이 실행될 때 각 청크(커밋 단위)를 별도의 스레드에서 읽고, 처리하고, 쓰게 된다. 따라서 아이템들이 순차적으로 처리되지 않을 수 있으며, 단일 스레드의 경우와 달리 한 청크에 연속되지 않는 아이템이 포함될 수도 있습니다. 추가로, taskExecutor 자체의 제한(예: 스레드 풀이냐 아니냐) 외에도, tasklet 설정에는 throttleLimit(기본값: 4)이 존재합니다. 즉, 동시에 최대 4개의 청크만 병렬 실행 스레드 풀을 충분히 활용하려면 이 제한을 늘려야 할 수도 있습니다.Throttle limit 폐기 버전 5.0부터 throttle limit은 deprecated 되었으며 대체제가 없습니다. 현재 기본 TaskExecutorRepeatTemplate에서 쓰이는 쓰로틀링 메커니즘을 바꾸고 싶다면, bounded task queue를 가진 TaskExecutor 기반의 커스텀 RepeatOperations 구현체를 만들어서, StepBuilder#stepOperations에 넣어야 합니다:@Beanpublic Step sampleStep(RepeatOperations customRepeatOperations, JobRepository jobRepository, PlatformTransactionManager transactionManager) { return new StepBuilder(\"sampleStep\", jobRepository) .&lt;String, String&gt;chunk(10, transactionManager) .reader(itemReader()) .writer(itemWriter()) .stepOperations(customRepeatOperations) .build();}주의사항 Step의 많은 구성 요소(예: Reader, Writer)는 stateful 합니다. FlatFileItemReader는 파일 포인터(cursor)를 갖고 있어서 여러 스레드가 동시에 접근하면 꼬임. JdbcCursorItemReader도 DB 커서 기반이라 멀티스레드에 적합하지 않음. 반면, JdbcPagingItemReader는 Stateless Reader 만약 이 상태가 스레드별로 분리되지 않는다면, 멀티스레드 Step에서는 사용할 수 없습니다. 특히, Spring Batch가 제공하는 대부분의 Reader와 Writer는 멀티스레드 용도로 설계되지 않았습니다. 그러나, stateless 또는 thread-safe한 Reader/Writer를 사용한다면 가능하며, Spring Batch 샘플에서는 DB 입력 테이블에 처리 여부 플래그(process indicator)를 두어 어떤 아이템이 이미 처리되었는지 추적하는 방법을 보여줍니다. Spring Batch는 일부 ItemWriter, ItemReader 구현을 제공합니다. Javadoc에 thread-safe 여부 또는 동시성 환경에서 피해야 할 문제가 명시되어 있는 경우가 많습니다. 정보가 없다면 구현체를 직접 확인해 상태(state)가 있는지 살펴야 합니다. 만약 Reader가 thread-safe하지 않다면, SynchronizedItemStreamReader로 감싸거나, 직접 동기화 래퍼를 구현할 수 있습니다. read() 호출을 동기화하면, 비록 읽기 자체는 직렬화되더라도, 처리(processing)와 쓰기(writing) 단계가 청크 내에서 가장 비용이 큰 부분이라면, 전체 Step은 여전히 싱글스레드보다 훨씬 빠르게 끝날 수 있습니다.예시@Configuration@EnableBatchProcessingpublic class OrderBatchConfig { private final DataSource dataSource; public OrderBatchConfig(DataSource dataSource) { this.dataSource = dataSource; } // 멀티스레드용 TaskExecutor @Bean public ThreadPoolTaskExecutor taskExecutor() { ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setCorePoolSize(8); // 동시에 실행할 스레드 수 executor.setMaxPoolSize(8); // 최대 스레드 수도 동일하게 설정 executor.setQueueCapacity(0); // bounded queue → 무한 큐 방지 executor.setThreadNamePrefix(\"spring_batch-\"); executor.initialize(); return executor; } // Reader (페이징 기반, 멀티스레드 안전) @Bean public JdbcPagingItemReader&lt;Order&gt; orderReader() throws Exception { JdbcPagingItemReader&lt;Order&gt; reader = new JdbcPagingItemReader&lt;&gt;(); reader.setDataSource(dataSource); reader.setPageSize(100); SqlPagingQueryProviderFactoryBean queryProvider = new SqlPagingQueryProviderFactoryBean(); queryProvider.setDataSource(dataSource); queryProvider.setSelectClause(\"id, customer_id, status\"); queryProvider.setFromClause(\"from orders\"); queryProvider.setWhereClause(\"where status = 'NEW'\"); queryProvider.setSortKeys(Map.of(\"id\", Order.ASCENDING)); reader.setQueryProvider(queryProvider.getObject()); reader.setRowMapper((rs, rowNum) -&gt; new Order( rs.getLong(\"id\"), rs.getLong(\"customer_id\"), rs.getString(\"status\") )); return reader; } // Processor (상태 변경) @Bean public ItemProcessor&lt;Order, Order&gt; orderProcessor() { return order -&gt; { order.setStatus(\"PROCESSED\"); return order; }; } // Writer (Batch update) @Bean public JdbcBatchItemWriter&lt;Order&gt; orderWriter() { return new JdbcBatchItemWriterBuilder&lt;Order&gt;() .dataSource(dataSource) .sql(\"UPDATE orders SET status = :status WHERE id = :id\") .beanMapped() .build(); } // Step 정의 (멀티스레드 적용) @Bean public Step processOrdersStep(JobRepository jobRepository, PlatformTransactionManager transactionManager) throws Exception { return new StepBuilder(\"processOrdersStep\", jobRepository) .&lt;Order, Order&gt;chunk(100, transactionManager) .reader(orderReader()) .processor(orderProcessor()) .writer(orderWriter()) .taskExecutor(taskExecutor()) // 멀티스레드 실행 .build(); } // Job 정의 @Bean public Job orderJob(JobRepository jobRepository, Step processOrdersStep) { return new JobBuilder(\"orderJob\", jobRepository) .start(processOrdersStep) .build(); }}페이징 처리 방식 JdbcPagingItemReader는 pageSize 단위로 쿼리를 실행해서 데이터를 가져옵니다. 예를 들어, 전체 조회 대상이 10000개이고 pageSize=100이면: 1차 쿼리 : where status='NEW' order by id limit 100 offset 0 2차 쿼리 : where status='NEW' order by id limit 100 offset 100 이런 식으로 쿼리를 날려서 10000개를 끊어서 가져옵니다. 따라서 중복 없이 전체 10000개를 다 읽습니다.(단, 여기서 정렬 기준 sortKeys를 반드시 지정해야 안정적으로 paging이 됩니다. 보통 id ASC 같은 단일 PK 기준을 많이 씁니다.) pageSize &gt; chunkSize 이면 ? 총 대상 데이터 : 10,000개 (status=NEW) chunkSize = 100 pageSize = 1000 Reader DB에서 1000개를 한 번에 select 해와서 메모리에 들고 있음. 예: id 1 ~ 1000 Processor &amp; Writer 읽어온 1000개 중 100개씩 잘라서 Processor → Writer → commit 실행. 즉, 같은 1000개 데이터를 내부 버퍼에서 10번(=1000/100) 반복해서 처리. 다음 Reader 호출 Reader가 다시 DB에 쿼리 → 다음 1000개(id 1001~2000) 가져옴. 그 안에서도 chunk 단위(100)로 나눠서 10번 commit. 결과 DB는 10번(=10000 / 1000)만 조회함. Processor/Writer는 여전히 100개 단위로 동작해서 총 100번 commit(=10000 / 100). 즉, pageSize는 “DB fetch 단위”, chunkSize는 “트랜잭션 처리 단위”라는 점이 드러남. 참고 자료 https://docs.spring.io/spring-batch/reference/scalability.html" }, { "title": "Spring Batch 알아보기", "url": "/posts/what-is-spring-batch/", "categories": "지식 더하기, 이론", "tags": "Spring Batch", "date": "2025-08-22 21:25:00 +0900", "snippet": "Spring Batch Spring Batch는 엔터프라이즈 시스템의 핵심 일상 업무(vital for the daily operations)를 위한 견고한 배치 애플리케이션 개발을 지원하는 경량·종합 프레임워크 Spring Batch는 생산성, POJO 기반 개발, 쉬운 사용성 같은 스프링 특성을 계승하면서, 필요 시 고급 엔터프라이즈 서비스도 ...", "content": "Spring Batch Spring Batch는 엔터프라이즈 시스템의 핵심 일상 업무(vital for the daily operations)를 위한 견고한 배치 애플리케이션 개발을 지원하는 경량·종합 프레임워크 Spring Batch는 생산성, POJO 기반 개발, 쉬운 사용성 같은 스프링 특성을 계승하면서, 필요 시 고급 엔터프라이즈 서비스도 쉽게 활용할 수 있도록 한다. Spring Batch는 스케줄링 프레임워크가 아니다. 상용·오픈소스 영역에는 Quartz, Tivoli, Control-M 등 훌륭한 스케줄러가 이미 있다. 따라서, Spring Batch는 스케줄러를 대체하는 게 아니라 함께 동작하도록 설계되었다. Spring Batch는 대규모 데이터 처리에 필수적인 재사용 가능한 기능(로그/추적, 트랜잭션 관리, 실행 통계, 재시작, 스킵, 리소스 관리)을 제공한다. 또한 최적화·파티셔닝 기법을 통해 초대규모·고성능 배치 작업을 가능하게 하는 고급 기능도 제공한다. 즉, 대규모 데이터 처리 작업 및 운영을 안정적이고 편리하게 만들어주는 프레임워크이다.주요 개념Chunk-based processing 대용량 데이터를 효율적으로 처리하는 핵심 패턴 데이터를 일정 단위(예: 1000건)로 묶어 처리 메모리 절약, 안정적 처리, 재시작 용이Transaction management 배치 처리에서 데이터 일관성을 보장 Chunk 단위 트랜잭션 보장 (실패 시 해당 Chunk만 롤백 가능)Declarative I/O 복잡한 I/O 로직을 간단한 설정으로 처리 ItemReader, ItemWriter 같은 표준 컴포넌트 제공 ItemReader: 데이터베이스, 파일, 메시지 큐 등 다양한 소스에서 데이터 읽기 ItemWriter: 다양한 형태로 데이터 출력 (DB 저장, 파일 쓰기, API 호출 등) Start / Stop / Restart 배치 작업의 운영 편의성을 제공 JobRepository: 작업 실행 상태와 메타데이터를 저장 실행한 Job/Step의 상태, 시작·종료 시간, 처리 건수, 실패 원인 등 Job의 이력 관리 가능 (언제 어떤 데이터가 처리되었는지 추적) 재시작 지원: 실패 지점부터 다시 시작 가능 (이미 처리된 부분은 건너뜀) 중단 후 재개: 안전하게 작업을 중단하고 나중에 이어서 실행Retry / Skip 오류 상황에 대한 유연한 처리 정책을 제공 Retry: 일시적 오류(네트워크 끊김 등) 발생 시 지정된 횟수만큼 재시도 Skip: 특정 오류는 무시하고 다음 데이터로 넘어가기 정책 설정: 어떤 예외에 대해 몇 번 재시도할지, 어떤 예외는 건너뛸지 세밀하게 제어Job / Step 구조 Job: 하나의 배치 작업 단위 (예: “월말 정산 작업”) Step: Job을 구성하는 세부 단계 (예: “DB에서 데이터 읽기 → 가공 → 결과 저장”) 이를 통해, 구조화된 모델 덕분에 작업을 잘게 나누고, 재사용·조합이 가능해짐. 예: 동일한 DB 읽기 Step을 다른 Job에서도 재사용할 수 있음. Scalability 멀티스레드 Step: 한 Step을 여러 스레드로 분할 처리 Partitioning: 데이터를 여러 파티션으로 나누어 병렬 처리 Remote Chunking: “읽기/처리”와 “쓰기”를 분산 노드에서 실행 Remote Partitioning: Step 자체를 원격 워커들에 분산 대규모 데이터도 분산·병렬 환경에서 처리 가능Tasklet Step 안에서 한 번에 실행할 작업(Task)을 구현하는 방식 인터페이스: Tasklet → RepeatStatus execute(StepContribution, ChunkContext) 메서드 구현 반복 처리(Chunk 기반)가 필요 없는 단순 로직에 적합 예: 특정 테이블 초기화, 파일 삭제, 외부 API 한 번 호출 선언형 Chunk 모델보다 Spring MVC + @Service 스타일과 유사 하지만 Spring Batch 실행 모델(Job/Step, JobRepository, 재시작 지원 등) 위에서 동작하므로, 단순 로직도 운영 메타데이터 관리, 재시작, 스케줄링 연동 혜택을 받음 예시: 아래 코드 실행 중 외부 API 호출이 네트워크 문제로 실패했다면? Job 상태가 FAILED로 기록됨 restart 하면 해당 Step만 다시 실행 → 중간부터 이어서 처리 가능 @Beanpublic Step apiStep(JobRepository jobRepository, PlatformTransactionManager transactionManager) { return new StepBuilder(\"apiStep\", jobRepository) .tasklet((contribution, chunkContext) -&gt; { externalApi.call(...); // 외부 API 호출 return RepeatStatus.FINISHED; }, transactionManager) .allowStartIfComplete(false) // 이미 성공한 Step은 재실행 안 함 .build();} Tasklet vs Chunk 구분 Chunk 기반 처리 Tasklet 기반 처리 처리 방식 데이터를 읽기-처리-쓰기(Reader/Processor/Writer) 패턴으로 반복 실행 execute() 메서드 안에서 단일 작업 수행 적합한 시나리오 - 대량 데이터(수천~수억 건)- 주기적인 정산/집계- DB → 파일, 파일 → DB 적재- 데이터 변환, ETL 파이프라인 - 단순 작업(한두 번 실행)- 테이블 초기화/백업- 외부 API 호출 1회- 파일 압축/삭제 트랜잭션 관리 Chunk 단위 트랜잭션 (예: 1000건씩 커밋) → 실패 시 해당 Chunk만 롤백 보통 전체 Step이 1 트랜잭션 → 실패 시 전체 롤백 재시작(Checkpoint) 마지막 커밋된 Chunk 위치부터 이어서 실행 가능 Step 자체가 단일 실행 단위 → 실패 시 Step 전체 재실행 Retry/Skip 선언적으로 설정 가능 (.retry(), .skip()) → 데이터 단위 예외 처리 별도 로직 작성 필요 (try-catch 안에서 수동 구현) 운영 편의성 대량 데이터 처리 시 안정성, 확장성 뛰어남 단순 로직도 Job/Step 메타데이터에 기록되므로 운영 추적 가능 비교 비유 “엑셀에서 100만 행 데이터를 1000행씩 끊어서 처리” “버튼 한 번 누르면 테이블 삭제” 같은 단일 태스크 비교 : Spring MVC vs Spring Batch 예시 상황 : 대량의 주문 정보 업데이트가 필요, 업데이트 하려면 외부 api 호출 필요. Spring MVC 방식@Servicepublic class OrderBatchService { @Transactional public void processOrders(List&lt;Order&gt; orders) { for (Order order : orders) { try { ApiResult result = externalApi.call(...); // 외부 API 호출 order.doSomething(result); // 주문 상태 변경 updateOrder(order); // DB 업데이트 } catch (ApiException e) { // 재시도 로직을 직접 구현해야 함 for (int i = 0; i &lt; 3; i++) { try { ApiResult result = externalApi.call(...); order.doSomething(result); updateOrder(order); break; // 성공하면 탈출 } catch (Exception retryEx) { if (i == 2) { // 실패 기록을 직접 관리해야 함 log.error(\"Order {} 처리 실패\", order.getId()); saveToFailedOrderTable(order); } } } } } }} Spring Batch@Configuration@EnableBatchProcessingpublic class OrderBatchConfig { @Bean public Job orderJob(JobRepository jobRepository, Step orderStep) { return new JobBuilder(\"orderJob\", jobRepository) .start(orderStep) .build(); } @Bean public Step orderStep(JobRepository jobRepository, PlatformTransactionManager transactionManager) { return new StepBuilder(\"orderStep\", jobRepository) .&lt;Order, Order&gt;chunk(1000, transactionManager) .reader(orderReader()) .processor(orderProcessor()) .writer(orderWriter()) .faultTolerant() .skip(ApiException.class) // 특정 예외 건너뛰기 .skipLimit(100) // 최대 100건까지 Skip 허용 .retry(ApiException.class) // API 호출 실패 시 재시도 .retryLimit(3) // 최대 3번까지 재시도 .build(); } @Bean public JdbcPagingItemReader&lt;Order&gt; orderReader(DataSource dataSource) { return new JdbcPagingItemReaderBuilder&lt;Order&gt;() .name(\"orderReader\") .dataSource(dataSource) .selectClause(\"SELECT ...\") .fromClause(\"FROM orders\") .sortKeys(Collections.singletonMap(\"id\", Order.ASCENDING)) .pageSize(1000) .rowMapper(new BeanPropertyRowMapper&lt;&gt;(Order.class)) .build(); } @Bean public ItemProcessor&lt;Order, Order&gt; orderProcessor() { return order -&gt; { ApiResult result = externalApi.call(...); order.doSomething(result); return order; }; @Bean public JdbcBatchItemWriter&lt;Order&gt; orderWriter(DataSource dataSource) { return new JdbcBatchItemWriterBuilder&lt;Order&gt;() .dataSource(dataSource) .sql(\"UPDATE orders ...\") .beanMapped() .build(); }} 항목 Spring MVC Spring Batch 재시도 처리 try-catch + 반복문 직접 구현 필요 .retry(Exception.class).retryLimit(n) 으로 선언적 설정 스킵 처리 실패한 건을 따로 DB 저장 로직 작성 필요 .skip(Exception.class).skipLimit(n) 으로 선언적 설정 실패 내역 관리 별도 테이블(failed_orders) 직접 구현 JobRepository에 자동 저장 (실패 건수, 예외 타입 등) 재처리 실패 테이블 기반으로 재처리 Job 따로 작성 실패 이력 기반으로 Job/Step 재시작 가능 운영 편의성 로그 + 수작업 확인 필요 실패 내역이 메타DB에 기록 → 운영자가 재실행 가능 기타개발 배경 https://docs.spring.io/spring-batch/reference/spring-batch-intro.html#springBatchBackground 오픈소스 커뮤니티는 주로 웹/마이크로서비스 아키텍처에 집중했지만, Java 기반 배치 처리용 재사용 가능한 아키텍처 프레임워크는 부족했다. 기업 IT 환경에서는 여전히 배치 처리가 필요한데도 말이다. 표준 배치 프레임워크 부재로 인해 기업들은 제각각 자체 배치 솔루션을 개발해왔다. SpringSource(현재 VMware)와 Accenture가 이를 바꾸기 위해 협력했다. Accenture의 산업/배치 경험, SpringSource의 기술 전문성, Spring의 프로그래밍 모델이 합쳐져 엔터프라이즈 Java의 공백을 메우는 고품질 배치 프레임워크가 탄생했다. 두 회사는 고객들과 협력하며 Spring 기반 배치 아키텍처를 구축했고, 이 경험이 현실 제약 조건과 요구사항을 반영해 실제 문제에 적용 가능한 솔루션을 만드는 데 기여했다. Accenture는 과거 사내 독점 배치 프레임워크와 개발자 리소스를 Spring Batch에 기여해 기능 확장과 지원을 제공했다. Accenture는 메인프레임 COBOL, 유닉스 C++, 자바 기반 아키텍처 등 수십 년간의 배치 경험을 토대로 기여했다. Accenture와 SpringSource의 협력 목표는 엔터프라이즈 IT에서 배치 애플리케이션 개발 시 재사용 가능한 표준 프레임워크를 만드는 것이었다. 표준화되고 검증된 솔루션을 원하는 기업·정부 기관은 Spring Batch로부터 혜택을 얻을 수 있다.별도의 프로젝트 ? Spring Framework 프로젝트에서 하나의 모듈이 아닌 별도의 Spring Batch 프로젝트로 관리된다. (참고 : github repo) Spring Web MVC는 Spring Framework의 하나의 모듈로 관리된다. Spring Batch도 spring-aop, spring-beans 등의 핵심 모듈을 사용하는데, 왜 별도의 프로젝트로 분리되었을까? GPT에게 물어보았다. (맞는지는 모르겠다)1. 역할과 성격이 다른 기능- Spring Framework는 애플리케이션 전반의 기반 기능(IOC 컨테이너, AOP, 트랜잭션, 웹 MVC 등)을 제공하는 핵심 프레임워크입니다.- Spring Batch는 배치 처리에 특화된 기능(잡/스텝 정의, 잡 실행 관리, 리스타트/스킵/리트라이, 청크 기반 처리 등)을 제공하는 응용 계층 프레임워크입니다.- 즉, Batch는 **도메인 특화 기능(DSF, Domain-Specific Framework)**에 가깝기 때문에, 코어 프레임워크와 같은 레벨에서 관리하기보다는 별도의 독립 프로젝트로 분리하는 게 더 적절했습니다.2. 릴리스 주기 및 버전 독립성- Spring Framework는 전반적인 생태계에 영향을 주므로 안정적이고 보수적인 릴리스 주기가 필요합니다.- Spring Batch는 특정 기업 환경(금융, 대규모 데이터 처리 등) 요구사항에 맞추어 더 빠르게 발전하거나 독자적으로 패치될 필요가 있습니다.- 별도 프로젝트로 분리함으로써 Spring Framework 릴리스와 강하게 묶이지 않고, 독자적인 릴리스 주기를 가질 수 있습니다.3. 사용자 대상의 범위 차이- Spring Framework는 거의 모든 Spring 애플리케이션 개발자가 사용하는 범용 플랫폼입니다.- Spring Batch는 **배치 처리(ETL, 대규모 DB 마이그레이션, 정산 작업 등)**를 수행하는 특정 케이스에서만 필요합니다.- 따라서 일반적인 웹 애플리케이션 개발자가 꼭 가져가야 하는 모듈이 아니기 때문에, 선택적으로 추가할 수 있도록 별도 프로젝트로 분리한 겁니다.4. 팀/프로젝트 관리상의 이유- Spring Batch는 초기에 Accenture와 SpringSource가 협력해서 만든 프로젝트입니다.- 즉, 시작부터 **외부 파트너십과 특정 요구사항(기업용 배치 처리)**을 기반으로 설계되었기 때문에 Spring Framework 코어에 바로 들어가기보다는 독립된 프로젝트로 출발하는 게 자연스러웠습니다.- 지금도 Spring 팀은 Framework, Boot, Data, Security, Batch 등을 프로젝트 단위로 관리합니다.참고 자료 https://spring.io/projects/spring-batch#overview https://docs.spring.io/spring-batch/reference/spring-batch-intro.html" }, { "title": "Spring - Spring Boot 알아보기", "url": "/posts/what-is-spring-boot/", "categories": "지식 더하기, 이론", "tags": "Spring", "date": "2025-08-21 21:25:00 +0900", "snippet": "Spring Boot의 목표 모든 Spring 개발에서 극적으로 빠르고 누구나 쉽게 접근 가능한 시작 경험을 제공하는 것.→ 프로젝트를 시작할 때 복잡한 설정 없이 바로 개발을 시작할 수 있게 한다. 기본적으로는 권장되는(opinionated) 설정을 제공하지만, 개발 요구사항이 기본값에서 벗어나면 쉽게 커스터마이징할 수 있도록 ...", "content": "Spring Boot의 목표 모든 Spring 개발에서 극적으로 빠르고 누구나 쉽게 접근 가능한 시작 경험을 제공하는 것.→ 프로젝트를 시작할 때 복잡한 설정 없이 바로 개발을 시작할 수 있게 한다. 기본적으로는 권장되는(opinionated) 설정을 제공하지만, 개발 요구사항이 기본값에서 벗어나면 쉽게 커스터마이징할 수 있도록 하는 것. 대규모 프로젝트에서 공통적으로 필요한 비기능적 기능들을 제공하는 것.(예: 내장 서버, 보안, 메트릭 수집, 헬스 체크, 외부화된 설정 등) (네이티브 이미지 타겟이 아닌 경우) 절대 코드 생성에 의존하지 않고, XML 설정을 요구하지 않는 것.→ 즉, 순수 자바/애너테이션 기반 구성만으로 충분히 개발할 수 있게 한다. 핵심 개념1. Convention over Configuration (관습이 설정보다 우선) 개발자가 모든 걸 세세히 설정하지 않아도, 일반적으로 가장 많이 쓰는 방식대로 자동 설정해 줍니다. 예: spring-boot-starter-web만 추가하면 내장 톰캣, JSON 변환기, DispatcherServlet까지 자동 준비됨. 개발자는 원하는 경우에만 설정을 덮어씌우면 됨.2. Opinionated Defaults (권장되는 기본값) Spring Boot는 “실무에서 가장 합리적인 기본값”을 미리 선택해 둡니다. 예: HikariCP를 기본 커넥션 풀로 채택, UTF-8 기본 인코딩, actuator로 헬스 체크 제공. Spring에서라면 직접 골라야 할 선택지를 Spring Boot가 대신 정해줌.3. Auto Configuration (자동 설정) 클래스패스에 존재하는 라이브러리를 감지해서 관련 Bean을 자동 구성합니다. 예: spring-boot-starter-data-jpa를 추가하면 EntityManager, Hibernate, JPA Repository 자동 등록. 4. Standalone Application (독립 실행형 애플리케이션) 내장 톰캣/Jetty/Undertow를 포함한 실행 가능한 JAR로 배포 가능. 외부 WAS 설치 없이 java -jar만으로 실행.5. Production-ready Features Spring Boot Actuator: 헬스 체크, 메트릭, 로그레벨 조정, 환경 확인 등 운영 친화 기능 제공. DevTools: 코드 변경 시 자동 재시작, LiveReload 등 개발 편의 기능.주요 모듈 Spring Boot Repository README 참고spring-boot Spring Boot의 다른 부분들을 지원하는 기능들을 제공하는 메인 라이브러리1. SpringApplication 클래스 독립 실행형 Spring 애플리케이션을 작성할 때 사용할 수 있는 정적 편의 메서드(SpringApplication.run(Application.class, args 등) 를 제공. 이 클래스의 유일한 역할은 적절한 Spring ApplicationContext를 생성하고 갱신(refresh)하는 것.2. 내장 웹 애플리케이션 Tomcat, Jetty 같은 컨테이너 선택 가능.3. 외부 설정 지원 설정값을 코드에 하드코딩하지 않고, 외부에서 주입받을 수 있도록 디자인되어 있음 예: application.properties, 환경 변수 (OS level), 커맨드라인 인자 (--server.port=8081) 4. 편리한 ApplicationContext 초기화 도구 Spring Boot는 ApplicationContextInitializer를 미리 구현해 두어서, 로깅이나 환경설정 같은 것들이 애플리케이션 시작 시 자동으로 초기화되도록 해줍니다. 예: SpringApplication 실행 시 자동으로 LoggingApplicationListener 같은 리스너를 붙여줌. 5. 로깅 기본값 지원 포함 별도 설정을 하지 않아도 기본 로깅이 작동합니다. 기본 로깅 프레임워크는 SLF4J + Logback 로그 포맷, 로그 레벨(INFO), 출력 대상(Console) 등이 자동 지정 필요하면 application.properties에서 간단히 바꿀 수 있음. spring-boot-autoconfigure Spring Boot는 클래스패스에 포함된 내용에 따라 애플리케이션의 많은 부분을 자동으로 설정 예 : 클래스패스에 spring-webmvc가 있으면 자동으로 DispatcherServlet이 등록 (org.springframework.boot.webmvc.autoconfigure.DispatcherServletAutoConfiguration을 통해) @EnableAutoConfiguration 애너테이션이 Spring 컨텍스트의 자동 설정을 트리거 자동 설정은 사용자가 필요로 할 수 있는 Bean을 추론하려고 시도 예를 들어, HSQLDB가 클래스패스에 있고 사용자가 어떤 데이터베이스 연결도 설정하지 않았다면, 인메모리 데이터베이스를 정의하길 원한다고 추정 사용자가 직접 Bean을 정의하면 자동 설정은 덮어씌워진다. spring-boot-starters 스타터(Starters)는 애플리케이션에 포함시킬 수 있는 편리한 의존성(descriptor) 집합입니다. 이를 통해 필요한 Spring 및 관련 기술을 일일이 샘플 코드에서 찾아 붙이거나 의존성을 복사/붙여넣기 할 필요가 없습니다. 예를 들어, Spring과 JPA를 이용한 데이터베이스 접근을 시작하고 싶다면 spring-boot-starter-data-jpa 의존성을 프로젝트에 추가하면 됩니다.spring-boot-actuator Actuator 엔드포인트를 사용하면 애플리케이션을 모니터링하고 상호작용할 수 있습니다. Spring Boot Actuator는 이러한 엔드포인트를 위한 기반 인프라를 제공합니다. Actuator 엔드포인트용 애너테이션 지원을 포함하고 있으며, HealthEndpoint, EnvironmentEndpoint, BeansEndpoint 등 다양한 엔드포인트를 제공합니다. /actuator/health → 애플리케이션 헬스 체크 /actuator/env → 환경 변수 확인 /actuator/beans → 스프링 빈 목록 확인 spring-boot-actuator-autoconfigure 이 모듈은 클래스패스 내용과 속성 집합에 기반하여 actuator 엔드포인트의 자동 설정을 제공합니다. 예를 들어, Micrometer가 클래스패스에 있으면 MetricsEndpoint를 자동으로 설정합니다. HTTP나 JMX를 통한 엔드포인트 노출을 위한 설정을 포함합니다. Spring Boot AutoConfigure와 마찬가지로 사용자가 직접 Bean을 정의하기 시작하면 자동 설정은 물러납니다.spring-boot-test 이 모듈은 애플리케이션을 테스트할 때 도움이 될 수 있는 핵심 항목과 애너테이션을 포함합니다. 예 : @SpringBootTest, @MockBean spring-boot-test-autoconfigure 다른 Spring Boot 자동 설정 모듈과 마찬가지로, spring-boot-test-autoconfigure는 클래스패스를 기반으로 테스트를 위한 자동 설정을 제공합니다. 테스트하려는 애플리케이션의 일부만 자동으로 구성할 수 있는 많은 애너테이션을 포함합니다. 예 : @WebMvcTest, @DataJpaTest 등을 통해 전체 컨텍스트를 띄우지 않고 필요한 부분만 테스트 가능. spring-boot-loader Spring Boot Loader는 java -jar를 사용하여 실행할 수 있는 단일 jar 파일을 빌드할 수 있게 해주는 비밀 소스(secret sauce)를 제공합니다. Fat JAR (모든 의존성 포함 JAR)을 만들어서 독립 실행 가능. 일반적으로는 spring-boot-loader를 직접 사용할 필요는 없으며, 대신 Gradle이나 Maven 플러그인과 함께 사용합니다.spring-boot-devtools spring-boot-devtools 모듈은 자동 재시작과 같은 추가 개발 시 기능을 제공하여 원활한 애플리케이션 개발 경험을 제공합니다. 이 개발자 도구는 완전히 패키징된 애플리케이션을 실행할 때는 자동으로 비활성화됩니다.정리하면, Spring Boot는 위 모듈들이 조합되어: 실행(boot, loader) 자동 설정(autoconfigure) 의존성 관리(starters) 운영(actuator) 테스트(test) 개발 편의(devtools)까지 전 과정(개발–운영–테스트–배포)을 지원하는 풀 패키지 프레임워크이다.스프링 부트로 개발한다는 것 ?Spring Boot 없이 Spring MVC + Spring Data JPA 사용&lt;dependencies&gt; &lt;!-- Spring Web MVC --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;5.3.32&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Spring Data JPA --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-jpa&lt;/artifactId&gt; &lt;version&gt;2.7.18&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Hibernate (JPA 구현체) --&gt; &lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-core&lt;/artifactId&gt; &lt;version&gt;5.6.15.Final&lt;/version&gt; &lt;/dependency&gt; &lt;!-- MySQL Driver --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;8.0.33&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;@Configuration@EnableTransactionManagement@EnableJpaRepositories(basePackages = \"com.example.repo\")@ComponentScan(basePackages = \"com.example\")public class AppConfig { @Bean public DataSource dataSource() { DriverManagerDataSource ds = new DriverManagerDataSource(); ds.setDriverClassName(\"com.mysql.cj.jdbc.Driver\"); ds.setUrl(\"jdbc:mysql://localhost:3306/testdb\"); ds.setUsername(\"root\"); ds.setPassword(\"1234\"); return ds; } @Bean public LocalContainerEntityManagerFactoryBean entityManagerFactory() { LocalContainerEntityManagerFactoryBean emf = new LocalContainerEntityManagerFactoryBean(); emf.setDataSource(dataSource()); emf.setPackagesToScan(\"com.example.entity\"); emf.setJpaVendorAdapter(new HibernateJpaVendorAdapter()); return emf; } @Bean public PlatformTransactionManager transactionManager(EntityManagerFactory emf) { return new JpaTransactionManager(emf); }}@Configuration@EnableWebMvcpublic class WebConfig implements WebMvcConfigurer { // JSON MessageConverter 수동 등록 @Override public void configureMessageConverters(List&lt;HttpMessageConverter&lt;?&gt;&gt; converters) { converters.add(new MappingJackson2HttpMessageConverter()); } // 뷰 리졸버 (JSP 예시) @Bean public InternalResourceViewResolver viewResolver() { InternalResourceViewResolver resolver = new InternalResourceViewResolver(); resolver.setPrefix(\"/WEB-INF/views/\"); resolver.setSuffix(\".jsp\"); return resolver; }}public interface UserRepository extends JpaRepository&lt;User, Long&gt; {}@Entitypublic class User { @Id @GeneratedValue private Long id; private String name;}@RestControllerpublic class UserController { @Autowired private UserRepository repo; @PostMapping(\"/users\") public String addUser(@RequestBody User user) { repo.save(user); return \"saved\"; } @GetMapping(\"/users\") public List&lt;User&gt; getUsers() { return repo.findAll(); }} DB 연결, EntityManagerFactory, TransactionManager, JSON MessageConverter, ViewResolver 등 전부 직접 Bean으로 등록해야 함. 실행도 WAR 파일 만들어서 외부 Tomcat 같은 WAS에 배포해야 함.Spring Boot 사용&lt;dependencies&gt; &lt;!-- Spring Boot Starter Web (MVC + Jackson + 내장 톰캣) --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Spring Boot Starter Data JPA (Spring Data JPA + Hibernate) --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- MySQL Driver --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-j&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt;# application.propertiesspring.datasource.url=jdbc:mysql://localhost:3306/testdbspring.datasource.username=rootspring.datasource.password=1234spring.jpa.hibernate.ddl-auto=updatespring.jpa.show-sql=true@SpringBootApplicationpublic class Application { public static void main(String[] args) { SpringApplication.run(Application.class, args); }}// UserRepository, User Entity, UserController는 동일하게 작성 spring-boot-starter-data-jpa 덕분에 DataSource, EntityManager, TxManager 자동 설정 spring-boot-starter-web 덕분에 DispatcherServlet, JSON 변환기(Jackson), 내장 톰캣 자동 설정 실행은 mvn spring-boot:run 또는 java -jar app.jar Spring Boot 핵심 개념 Spring Spring Boot 차이 포인트 1. Convention over Configuration (관습 &gt; 설정) DispatcherServlet, ViewResolver, MessageConverter를 개발자가 전부 수동 등록 spring-boot-starter-web으로 자동 구성 (DispatcherServlet, Jackson, 정적 리소스 매핑 등) 흔히 쓰는 관례적 설정을 Boot가 자동 제공 → 개발자는 설정 대신 비즈니스 로직에 집중 2. Opinionated Defaults (권장 기본값) DataSource 풀, Hibernate 옵션, 로깅 설정 등 개발자가 선택 HikariCP(커넥션풀), Logback, UTF-8, Hibernate ddl-auto 등 합리적인 기본값 제공 실무에서 검증된 “좋은 선택지”를 기본값으로 지정해줌 3. Auto-Configuration (자동 설정) DataSource, EntityManagerFactory, TransactionManager 직접 Bean 등록 spring.datasource.*, spring.jpa.* 프로퍼티만 작성 → 자동 Bean 등록 클래스패스 기반 자동 설정 → Bean 수동 설정 필요 최소화 4. Standalone Application (독립 실행형) WAR 빌드 후 외부 Tomcat 같은 WAS에 배포해야 실행 가능 내장 Tomcat/Jetty 포함 → java -jar 한 줄로 실행 운영 환경에서 바로 실행 가능한 독립 실행형 애플리케이션 5. Production-ready Features (운영 친화 기능) 운영 모니터링/헬스체크 기능 없음 → 직접 구현해야 함 spring-boot-starter-actuator 추가만 하면 /actuator/health, /metrics 등 제공 운영 환경에서 필요한 기능(헬스체크, 메트릭, 로그관리)을 내장 참고 자료 https://github.com/spring-projects/spring-boot/wiki https://docs.spring.io/spring-boot/index.html https://github.com/spring-projects/spring-boot" }, { "title": "Spring - Spring Framework 알아보기", "url": "/posts/what-is-spring-framework/", "categories": "지식 더하기, 이론", "tags": "Spring", "date": "2025-08-19 22:25:00 +0900", "snippet": "스프링 ? “Spring”이라는 용어는 문맥에 따라 다른 의미를 가진다. 원래는 Spring Framework 프로젝트 자체를 가리키는 말이었으며, 거기서 모든 것이 시작됐다. 시간이 지나면서 Spring Framework 위에 다른 Spring 프로젝트들이 만들어졌다. 즉, 각 프로젝트들은 s...", "content": "스프링 ? “Spring”이라는 용어는 문맥에 따라 다른 의미를 가진다. 원래는 Spring Framework 프로젝트 자체를 가리키는 말이었으며, 거기서 모든 것이 시작됐다. 시간이 지나면서 Spring Framework 위에 다른 Spring 프로젝트들이 만들어졌다. 즉, 각 프로젝트들은 spring-core, spring-aop, spring-context 등의 핵심 모듈들을 사용하면서자신만의 기능을 추가하거나 확장한다. Spring Framework는 여러 모듈로 나누어져 있으며, 애플리케이션은 필요한 모듈만 선택해서 사용할 수 있다. 가장 중심에는 설정 모델과 의존성 주입(Dependency Injection) 메커니즘을 포함한 코어 컨테이너 모듈이 있다. 그 외에도 Spring Framework는 메시징, 트랜잭션 처리, 데이터 영속성, 웹 등 다양한 애플리케이션 아키텍처를 지원한다. 또한 서블릿 기반의 Spring MVC 웹 프레임워크와, 리액티브 방식의 Spring WebFlux 웹 프레임워크도 포함되어 있다. 모듈 예시: spring-core : 스프링의 핵심 기능(의존성 주입, IoC 컨테이너 등)을 담은 모듈 spring-beans : Bean 생성, 설정, 라이프사이클 관리 같은 기능을 담당 spring-context : ApplicationContext 같은 고수준 컨테이너 제공, 국제화(i18n), 이벤트 시스템 등 포함 spring-aop : 관점 지향 프로그래밍(AOP) 지원을 위한 모듈 spring-jdbc : JDBC를 더 쉽게 사용할 수 있도록 도와주는 모듈 spring-tx : 트랜잭션 관리 관련 기능 제공 spring-web / spring-webmvc / spring-webflux : 서블릿 기반 웹 MVC와 리액티브 웹(WebFlux) 지원 모듈 Spring Framework Repository 스프링 프레임워크의 역사 스프링은 초기 J2EE 명세의 복잡성에 대한 대응으로 2003년에 탄생했다. 참고 : J2EE 설계와 개발(expert one-on-one) 일부 사람들은 Java EE와 그 후속인 Jakarta EE가 스프링과 경쟁 관계라고 생각하지만, 실제로는 상호 보완적이다. 스프링 프로그래밍 모델은 Jakarta EE 플랫폼 전체 명세를 수용하지 않고, 선택적으로 몇 가지 명세를 통합했다: Servlet API (JSR 340) → 서블릿 API WebSocket API (JSR 356) → 웹소켓 API Concurrency Utilities (JSR 236) → 동시성 유틸리티 JSON Binding API (JSR 367) → JSON 바인딩 API Bean Validation (JSR 303) → 빈 검증 JPA (JSR 338) → 자바 영속성 API JMS (JSR 914) → 자바 메시징 서비스 예시 구분 EE (표준) Spring (프레임워크) 트랜잭션 관리 JTA (Java Transaction API) @Transactional (내부적으로 필요 시 JTA 호출, 단순화) 데이터 접근 JPA (스펙, API만 정의) Spring Data JPA (JPA 위에 얹어 더 편리한 API 제공) 웹 Servlet API, JSP Spring MVC (Servlet 기반, 더 편리한 MVC 모델 제공) ※ 참고 : EE 스펙 변화 구분 J2EE (Java 2 EE) Java EE Jakarta EE 시기 1999년 ~ 2006년 2006년 ~ 2017년 2018년 ~ 현재 명칭 변화 Java 2 EE J2EE → Java EE로 개명 Java EE → Jakarta EE로 개명 주요 기술 EJB, Servlet, JSP JPA, CDI, Bean Validation 추가 Servlet, JPA, JMS, Bean Validation 특징 복잡하고 무거움 J2EE보다 단순화, 그래도 복잡함 Eclipse Foundation 관리, 더 현대화 관리 주체 Sun Microsystems Oracle Eclipse Foundation 네임스페이스 javax.* javax.* jakarta.* (EE 9부터) 비고 너무 무거워서 Spring 같은 경량 프레임워크 등장 생산성 개선, 표준화 강화 현재 공식 자바 엔터프라이즈 표준 ※ 참고 : 서블릿, 톰캣, Spring MVC Servlet 스펙 = 실제 구현체가 필요 Jakarta EE 표준 API → jakarta.servlet.* “요청이 들어오면 어떻게 처리해야 하는가?”를 정의한 인터페이스 규칙 Tomcat = Servlet 컨테이너 (구현체) Servlet 스펙을 구현한 서버 (Servlet 스펙을 실제로 실행하는 엔진) 주요 역할: HTTP 처리: TCP 소켓 열기, HTTP 요청/응답 파싱 Servlet 라이프사이클 관리: init(), service(), destroy() 실행 스레드 풀 관리: 요청당 스레드 할당 Filter/Listener 실행: Servlet API 표준에 정의된 기능 수행 Spring MVC = Servlet 기반 프레임워크 Spring MVC는 Servlet 스펙을 직접 구현한 게 아니라, Servlet 위에서 동작하는 프레임워크 핵심 클래스: DispatcherServlet DispatcherServlet은 Servlet API를 상속받은 클래스 (extends HttpServlet) Tomcat 같은 컨테이너가 DispatcherServlet을 실행시켜 줌 그 안에서 @Controller, @RequestMapping 등을 해석하고 호출 즉, Spring MVC는 Servlet을 활용해 더 편리한 MVC 개발 모델을 제공하는 추상화 계층 스프링을 사용한다는 것 ?1. 스프링은 Java 엔터프라이즈 애플리케이션을 쉽게 만들 수 있게 해줌 엔터프라이즈 애플리케이션 = 기업용 대규모 시스템 (ex : 쇼핑몰, 은행, ERP 등) J2EE 시절: 서블릿, JDBC, 트랜잭션, 보안 등을 개발자가 일일이 처리해야 함. 예: DB 연결, SQL 실행, 커넥션 닫기, 예외 처리, 트랜잭션 롤백 등 전부 수동 관리. 스프링 사용 시: Spring JDBC / Spring Data JPA → DB 연결과 트랜잭션 관리 자동 처리 Spring Security → 로그인/권한 관리 쉽게 설정 Spring MVC → REST API나 웹 화면 쉽게 개발 가능 따라서 개발자는 비즈니스 로직에만 집중할 수 있음. 예시 : EJB 2.x 스타일 (회원 등록)public interface MemberServiceHome extends EJBHome { MemberService create() throws RemoteException, CreateException;}public interface MemberService extends EJBObject { void registerMember(String name) throws RemoteException;}public class MemberServiceBean implements SessionBean { public void registerMember(String name) { // DB 연결, SQL 실행... }} “회원 등록” 하나 하려는데도, Home Interface / Remote Interface / Bean Class를 다 만들어야 했습니다. 개발자가 원하는 간단한 비즈니스 로직을 작성하기 위해서, 인터페이스/설정/보일러플레이트 코드를 너무 많이 작성해야 했음 배포할 때도 애플리케이션 서버 종속적이라 포터블(portable)하지 않음 작은 프로젝트에서는 과잉 설계였음 EJB vs Spring EJB (초창기): 무겁고 복잡 → 스프링이 “경량 컨테이너(POJO 기반 DI)”를 들고 등장 EJB 3.x 이후: 스프링 아이디어를 많이 가져와서 단순화 → 스프링과 어느 정도 닮아짐 현재: 대부분의 기업 프로젝트는 EJB 대신 Spring을 선택 2. 스프링은 엔터프라이즈 환경에서 자바 언어를 활용하는 데 필요한 모든 것을 제공 “엔터프라이즈 환경에서 필요한 모든 것” = DB, 보안, 메시징, 배치, 분산 트랜잭션 등 예시: 은행 계좌 이체 시스템 필요한 요소: 트랜잭션(계좌에서 돈 빠지고 다른 계좌에 돈 들어가는 것, 반드시 원자성 보장 필요) 메시징(JMS로 다른 금융 기관에 이벤트 전달) 보안(로그인, 2FA, 권한 분리) 스케줄링(매일 자정에 이자 계산 배치 실행) 스프링 지원: Spring Transaction Management → 트랜잭션 자동 처리 Spring JMS → 메시지 큐 연동 Spring Security → 보안 기능 Spring Batch → 배치 작업 실행 및 관리 즉, 개발자가 복잡한 인프라(Infra) 코드를 직접 작성하지 않고, 스프링이 제공하는 모듈을 가져다 쓰면 됨. 스프링 핵심 개념 IoC(Inversion of Control) DI(Dependency Injection) AOP(Aspect-Oriented Programming) “핵심”인 이유는 이를 통해 개발자는 비즈니스 로직에 집중할 수 있으며, 애플리케이션의 결합도를 낮추고, 재사용성을 높이며, 유지보수를 용이하게 만들어 주기 때문IoC (Inversion of Control) 제어 : 객체의 생성, 주입, 생명주기 등을 관리하는 것제어의 역전 : 개발자가 객체를 직접 생성하고 관리하는 대신, 프레임워크가 대신 관리해주는 것 개발자가 직접 제어OrderService orderService = new OrderService(new OrderRepository()); 프레임워크가 제어 객체 생성, 라이프사이클 관리, 의존성 주입은 Spring이 처리하므로 개발자는 비즈니스 로직에 집중할 수 있음. 의존 관계를 외부 설정(ApplicationContext)에서 관리하기 때문에, 코드 변경 최소화. 설정만 바꾸면 다른 구현체를 주입할 수 있음. 이로 인해, 테스트 용이성도 높아짐 (ex : 쉽게 MockRepository로 교체 가능) @Serviceclass OrderService { private final OrderRepository orderRepository; @Autowired public OrderService(OrderRepository orderRepository) { this.orderRepository = orderRepository; // 컨테이너가 넣어줌 }}DI (Dependency Injection) 의존성 주입 : 객체 간의 의존 관계를 외부에서 주입해주는 것 DI는 제어의 역전을 하기 위한 방식 중 하나 DI가 아닌 다른 IoC 방식 : Service Locator 객체를 직접 생성하지 않고, 필요한 객체를 찾는 방식 하지만, Service Locator는 코드가 Service Locator에 의존하게 되어 테스트가 어려워짐. DI는 의존성을 외부에서 주입받기 때문에, 코드가 더 유연하고 테스트하기 쉬움. public class OrderController { private OrderService orderService; public OrderController() { // new는 없지만, 여전히 코드가 ServiceLocator에 묶여있음. this.orderService = ServiceLocator.getBean(OrderService.class); }}AOP (Aspect-Oriented Programming) 횡단 관심사(트랜잭션, 보안, 로깅 등)를 별도의 Aspect로 분리하여 비즈니스 로직에 집중할 수 있게해줌 예를 들어, 트랜잭션 처리시 AOP를 활용하지 않는다면 아래와 같은 코드가 곳곳에 중복되어 나타나게 될 것이다.public class OrderService { public void placeOrder() { System.out.println(\"트랜잭션 시작\"); // 주문 처리 로직 System.out.println(\"트랜잭션 커밋\"); }} AOP를 사용하면 비즈니스 로직에 집중할 수 있고, 코드의 중복을 줄일 수 있다.@Servicepublic class OrderService { @Transactional public void placeOrder() { // 주문 처리 로직만 집중 }}참고 자료 https://docs.spring.io/spring-framework/reference/overview.html" }, { "title": "WEB - HTTP Content-Disposition 헤더", "url": "/posts/content-disposition-header/", "categories": "지식 더하기, 이론", "tags": "WEB", "date": "2025-08-05 21:25:00 +0900", "snippet": "Content-Disposition 헤더 콘텐츠를 브라우저에서 웹페이지로 직접 표시할지, 아니면 첨부파일로 다운로드할지를 지정Content-Disposition: inlineContent-Disposition: attachmentContent-Disposition: attachment; filename=\"file name.jpg\"Content-Dis...", "content": "Content-Disposition 헤더 콘텐츠를 브라우저에서 웹페이지로 직접 표시할지, 아니면 첨부파일로 다운로드할지를 지정Content-Disposition: inlineContent-Disposition: attachmentContent-Disposition: attachment; filename=\"file name.jpg\"Content-Disposition: attachment; filename*=UTF-8''file%20name.jpg특징 inline이면 웹페이지로 보여지고 attachment이면 다운로드 창이 뜸 Content-Disposition 헤더는 이메일용 MIME 메시지에서 정의된 것이지만, HTTP에서는 일부 파라미터만 사용 (form-data, name, filename) 파일명 주변의 따옴표는 선택 사항이지만, 공백 등의 특수문자가 있을 경우 필수 filename*은 RFC 5987로 인코딩됨 (공백은 %20, 한글은 UTF-8로 percent-encode) 브라우저는 / 또는 \\ 같은 경로 구분자를 _로 바꾸는 등의 변환을 수행할 수 있음 Chrome 및 Firefox 82 이상에서는 &lt;a download&gt; 속성이 Content-Disposition: inline보다 우선시됨 (같은 출처의 URL에 한함)※ percent 인코딩 ? 일반적으로 HTTP 헤더나 URL에는 다음과 같은 제약이 존재: 공백이나 특수문자는 문제가 될 수 있음 (HTTP 파서 오류, 잘못된 URL 인식 등) ASCII 이외의 문자 (예: 한글, 일본어 등)는 깨질 수 있음 따라서, URL이나 HTTP 헤더에서 특수 문자, 공백, 비ASCII 문자(예: 한글) 등을 안전하게 전송하기 위해 사용하는 인코딩 방식으로 percent 인코딩(URL 인코딩)이 사용됨 예시 : 문자들을 % + ASCII 코드의 16진수 값으로 변경 원래 문자 Percent 인코딩 공백 (space) %20 \" (큰따옴표) %22 한글 가 %EA%B0%80 파일 이름.png 파일%20이름.png filename에서 percent 인코딩이 문제되는 이유 브라우저별로 처리방식이 다를 수 있음 (아래 예시 기준): %20을 공백으로 잘 디코딩함 디코딩 안 함 → 그대로 file%20name.png 저장됨 오류 발생하거나 파일명이 깨짐 Content-Disposition: attachment; filename=\"파일 이름.png\"Content-Disposition: attachment; filename=\"file%20name.png\" filename*은 RFC 5987에 따라 UTF-8 + percent 인코딩을 정식으로 허용 예시 : filename* → 인코딩된 진짜 이름, filename → fallback용 ASCII 이름 (구형 브라우저 대응) Content-Disposition: attachment; filename=\"fallback.png\"; filename*=UTF-8''파일%20이름.png multipart/form-data multipart/form-data 본문에서는 각 필드에 대한 정보를 제공하기 위해 Content-Disposition 헤더가 필수 첫 번째 지시자는 항상 form-data 추가 지시자는 대소문자를 구분하지 않으며, = 다음에 따옴표로 감싼 값을 가지게됨 각 서브 파트는 Content-Type 헤더에 정의된 boundary로 구분 예시 : field1은 일반 텍스트 field2는 파일 업로드# 헤더Content-Type: multipart/form-data; boundary=----WebKitFormBoundary7MA4YWxkTrZu0gW# 본문------WebKitFormBoundary7MA4YWxkTrZu0gWContent-Disposition: form-data; name=\"field1\"value1------WebKitFormBoundary7MA4YWxkTrZu0gWContent-Disposition: form-data; name=\"field2\"; filename=\"example.txt\"Content-Type: text/plainvalue2------WebKitFormBoundary7MA4YWxkTrZu0gW--실제 코드 예시 https://start.spring.io/ 에서 GENERATE 버튼 눌렀을 때 파일 다운로드 처리를 담당하는 코드 (코드 출처)// ProjectGenerationController.java@RequestMapping(path = \"/starter.zip\", method = { RequestMethod.GET, RequestMethod.POST })public ResponseEntity&lt;byte[]&gt; springZip(R request) throws IOException { ProjectGenerationResult result = this.projectGenerationInvoker.invokeProjectStructureGeneration(request); Path archive = createArchive(result, \"zip\", ZipArchiveOutputStream::new, ZipArchiveEntry::new, ZipArchiveEntry::setUnixMode); return upload(archive, result.getRootDirectory(), generateFileName(result.getProjectDescription().getArtifactId(), \"zip\"), \"application/zip\");}private ResponseEntity&lt;byte[]&gt; upload(Path archive, Path dir, String fileName, String contentType)\t\tthrows IOException {\tbyte[] bytes = Files.readAllBytes(archive);\tlogger.info(String.format(\"Uploading: %s (%s bytes)\", archive, bytes.length));\tResponseEntity&lt;byte[]&gt; result = createResponseEntity(bytes, contentType, fileName);\tthis.projectGenerationInvoker.cleanTempFiles(dir);\treturn result;}private ResponseEntity&lt;byte[]&gt; createResponseEntity(byte[] content, String contentType, String fileName) {\tString contentDispositionValue = \"attachment; filename=\\\"\" + fileName + \"\\\"\";\treturn ResponseEntity.ok()\t\t.header(\"Content-Type\", contentType)\t\t.header(\"Content-Disposition\", contentDispositionValue)\t\t.body(content);}참고 자료 https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Headers/Content-Disposition" }, { "title": "Spring - Spring Boot 버전 현황 (25.07.26)", "url": "/posts/spring-versions/", "categories": "지식 더하기, 이론", "tags": "Spring", "date": "2025-07-26 00:25:00 +0900", "snippet": "Spring Boot / Spring 릴리즈 및 지원 종료 일정Spring Boot Spring Boot는 6개월마다 새로운 메이저 또는 마이너 버전을 릴리스 (5월 / 11월) (보통) 세 번째주 목요일에 릴리스를 목표로 함 Branch Initial Release End of OSS Support E...", "content": "Spring Boot / Spring 릴리즈 및 지원 종료 일정Spring Boot Spring Boot는 6개월마다 새로운 메이저 또는 마이너 버전을 릴리스 (5월 / 11월) (보통) 세 번째주 목요일에 릴리스를 목표로 함 Branch Initial Release End of OSS Support End Enterprise Support 4.0.x 2025-11-20 2026-12-31 2027-12-31 3.5.x 2025-05-22 2026-06-30 2032-06-30 3.4.x 2024-11-21 2025-12-31 2026-12-31 3.3.x 2024-05-23 2025-06-30 2026-06-30 3.2.x 2023-11-23 2024-12-31 2025-12-31 3.1.x 2023-05-18 2024-06-30 2025-06-30 3.0.x 2022-11-24 2023-12-31 2024-12-31 2.7.x 2022-05-19 2023-06-30 2029-06-30 2.6.x 2021-11-17 2022-11-24 2024-02-24 2.5.x 2021-05-20 2022-05-19 2023-08-24 2.4.x 2020-11-12 2021-11-18 2023-02-23 2.3.x 2020-05-15 2021-05-20 2022-08-20 2.2.x 2019-10-16 2020-10-16 2022-01-16 2.1.x 2018-10-30 2019-10-30 2021-01-30 2.0.x 2018-03-01 2019-03-01 2020-06-01 1.5.x 2017-01-30 2019-08-06 2020-11-06 Spring Branch Initial Release End of OSS Support End Enterprise Support 7.0.x 2025-11-13 2027-06-30 2028-06-30 6.2.x 2024-11-14 2026-06-30 2032-06-30 6.1.x 2023-11-16 2025-06-30 2026-06-30 6.0.x 2022-11-16 2024-06-30 2025-06-30 5.3.x 2020-10-27 2023-06-30 2029-06-30 5.2.x 2019-09-30 2021-12-31 2023-12-31 5.1.x 2018-09-21 2020-12-31 2022-12-31 ※ 참고 항목 OSS (무료) Enterprise (유료 – VMware Tanzu) 비용 무료 유료 (라이선스 및 서비스 계약) 지원 Spring 커뮤니티에서 Q&amp;A (예: Stack Overflow, GitHub) VMware Spring 팀의 공식 기술 지원 보안 패치 커뮤니티가 유지 (EOL 이후엔 없음) EOL 이후에도 보안 패치 및 버그 수정 지원 업데이트 주기 공개 릴리스 일정에 따름 장기 지원(LTS) 보장 가능 대상 개인, 중소기업, 오픈소스 사용자 대기업, 금융권, 정부, 고가용성 시스템 운영 조직 버전 호환성Spring / JDK / Java &amp; Jakarta EE Branch JDK 지원 범위 Java/Jakarta EE 버전 지원 네임스페이스 7.0.x JDK 17 ~ 27 (예상) Jakarta EE 11 jakarta 6.2.x JDK 17 ~ 25 (예상) Jakarta EE 9 ~ 10 jakarta 6.1.x JDK 17 ~ 23 Jakarta EE 9 ~ 10 jakarta 6.0.x JDK 17 ~ 21 Jakarta EE 9 ~ 10 jakarta 5.3.x JDK 8 ~ 21 (5.3.26 기준) Java EE 7 ~ 8 javax Spring은 장기 지원(JDK LTS) 버전에 대해 완전한 테스트와 지원을 제공. 현재 기준 LTS 버전은 JDK 8, JDK 11, JDK 17, JDK 21 또한 JDK 18, 19, 20과 같은 중간 릴리즈 버전도 가능한 한 최선을 다해 지원 즉, 버그 리포트는 수용하며 기술적으로 가능한 범위 내에서 해결하려고 노력하지만, 서비스 수준 보장(SLA)은 제공하지 않음 Spring Framework 6.x 및 5.3.x를 운영 환경에서 사용할 경우, JDK 17 또는 21을 권장 Spring Framework 5.3.x가 지원하는 마지막 명세는 javax 기반의 Java EE 8 (이 명세에는 Servlet 4.0, JPA 2.2, Bean Validation 2.0이 포함) Spring Framework 6.0부터는 최소 Jakarta EE 9을 필요로 하며, Servlet 5.0, JPA 3.0, Bean Validation 3.0을 포함 최신인 Jakarta EE 10 (Servlet 6.0, JPA 3.1) 사용을 권장 Spring Boot / JDK / Spring Spring Boot 버전 최소 Java 버전 필요 Spring Framework 비고 4.0 Java 17 이상 Spring Framework 7.x Kotlin 2.2+ 지원, 최신 LTS Java 권장 3.0 Java 17 이상 Spring Framework 6.0 Java 8 미지원 2.0 Java 8 이상 Spring Framework 5.0 Java 6, 7 미지원 ※ 참고 : Java EE / Jakarta EE ? 웹, DB, 보안 등 기업용 기능을 위한 자바 API 명세 모음Java EE (Java Platform, Enterprise Edition) Oracle이 주도하던 기업용 자바 표준 플랫폼 웹, 트랜잭션, 보안, 메시징 등 엔터프라이즈 애플리케이션 개발에 필요한 API들을 정의 주요 구성 요소: Servlet API (웹 요청 처리) JPA (자바 객체 ↔ 관계형 DB 매핑) Bean Validation (입력 검증) JMS (자바 메시지 서비스) EJB, JAX-RS, 등 Jakarta EE Java EE의 후속 프로젝트로, 현재는 Eclipse Foundation이 주도 Java EE 8까지는 javax.* 네임스페이스를 사용했지만, Jakarta EE 9부터는 모든 API가 jakarta.*로 바뀜 Spring은 자체 프레임워크지만, 위와 같은 EE 스펙들을 내부적으로 사용하거나 호환되도록 설계되어 있음 기능 내부적으로 EE API 사용 WebMVC Servlet API 필요 (javax.servlet 또는 jakarta.servlet) JPA 연동 jakarta.persistence 또는 javax.persistence Bean Validation jakarta.validation or javax.validation JSON 바인딩 일부 구현체가 EE API에 의존 참고 자료 https://spring.io/projects/spring-boot#support https://spring.io/projects/spring-framework#support https://github.com/spring-projects/spring-framework/wiki/Spring-Framework-Versions https://github.com/spring-projects/spring-boot/wiki/Supported-Versions https://github.com/spring-projects/spring-boot/wiki/Spring-Boot-3.0-Migration-Guide https://github.com/spring-projects/spring-boot/wiki/Spring-Boot-4.0-Migration-Guide" }, { "title": "취미 코딩", "url": "/posts/fake-it-till-you-make-it/", "categories": "끄적끄적", "tags": "", "date": "2025-07-25 22:00:00 +0900", "snippet": "앞으로 두 달간 취미도 코딩인 것처럼 살아보고, 느껴지는게 있는지 지켜보자.", "content": "앞으로 두 달간 취미도 코딩인 것처럼 살아보고, 느껴지는게 있는지 지켜보자." }, { "title": "SXSSF를 사용하는데 OOM이 발생한다 ?", "url": "/posts/sxssf-memory-flush/", "categories": "경험하기, 이슈 노트", "tags": "Java", "date": "2025-07-12 22:25:00 +0900", "snippet": "상황 백오피스 시스템에서 조회 기간 길게해서 엑셀 파일 다운로드했더니 OOM 발생 해당 엑셀 다운로드 로직에서는 파일 생성을 위해 poi 라이브러리의 SXSSF 구현체 사용중 전에 XSSF 구현체와 비교했을때, SXSSF는 row를 메모리에서 디스크로 flush 하면서 생성한다고 했는데 왜 OOM이 발생한걸까 ? (SXSSFRow가 11만개나 ...", "content": "상황 백오피스 시스템에서 조회 기간 길게해서 엑셀 파일 다운로드했더니 OOM 발생 해당 엑셀 다운로드 로직에서는 파일 생성을 위해 poi 라이브러리의 SXSSF 구현체 사용중 전에 XSSF 구현체와 비교했을때, SXSSF는 row를 메모리에서 디스크로 flush 하면서 생성한다고 했는데 왜 OOM이 발생한걸까 ? (SXSSFRow가 11만개나 쌓여있는 것을 볼 수 있다)원인 파악1. SXSSF 구현체는 어떤 기준으로 row를 flush하는걸까 ? 먼저 org.apache.poi.xssf.streaming.SXSSFSheet 코드를 살펴보자 SXSSFSheet 기준으로 row생성시 SXSSFWorkbook을 생성할 때 넘긴 rowAccessWindowSize(default :100)라 불리는 윈도우 사이즈를 초과하면 메모리상의 row를 flush한다. 다음은 org.apache.poi.xssf.streaming.SXSSFWorkbook의 코드이다. SXSSFWorkbook.write 호출시 각 시트별로 메모리상에 남아있는 row를 flush하는 것을 알 수 있다.2. 엑셀 파일 생성 관련 코드public void 엑셀생성(...) { int excelTabNo = 1; int pagingSize = 10000; param.changePageSize(pagingSize); SXSSFWorkbook workbook = new SXSSFWorkbook(pagingSize); while (true) { List&lt;FooRow&gt; rows = 목록_조회(pagingSize); if (rows.isEmpty()) { break; } // sheet 생성 및 row 채우기 Sheet sheet = workbook.createSheet(\"foo_\" + excelTabNo); ... excelTabNo++; } // 엑셀 파일 생성 workbook.write(...); ...} pagingSize마다 엑셀 시트가 새롭게 만들어지기 때문에, 하나의 시트에 최대 row수는 pagingSize rowAccessWindowSize는 pagingSize와 같으므로, 시트별로 메모리상의 row수 &gt; rowAccessWindowSize가 될 수 없음. 따라서, workbook.write 호출전까지는 flush 되지않고 메모리에 계속 쌓이게됨조치 rowAccessWindowSize는 pagingSize가 아닌 default 값(100) 사용 그래도 workbook.write 전까지 시트별로 최대 100개의 row는 메모리 상에 쌓여있게됨 시트가 많지는 않으므로 메모리에 부담되진 않을 것으로 생각 100보다 더 작으면 Disk I/O가 너무 많이 발생하지 않을까 생각함 " }, { "title": "MySQL - LOCK 살펴보기", "url": "/posts/mysql-lock/", "categories": "지식 더하기, 이론", "tags": "MySQL", "date": "2025-07-01 21:00:00 +0900", "snippet": " 락을 잘 모르면 비효율적인 쿼리를 작성하거나, 데드락을 유발하거나, 불필요한 성능저하 등을 일으킬 수 있다고 생각한다. 또한 디버깅시 실마리를 못잡을 수도 있다.이런 일을 최대한 겪지 않도록 MySQL에서는 어떤 락을 제공하는지 살펴보자.Lock 유형 InnoDB는 행 수준(row-level) 잠금을 구현하며(테이블 수준 잠금도 지원), 공유(S...", "content": " 락을 잘 모르면 비효율적인 쿼리를 작성하거나, 데드락을 유발하거나, 불필요한 성능저하 등을 일으킬 수 있다고 생각한다. 또한 디버깅시 실마리를 못잡을 수도 있다.이런 일을 최대한 겪지 않도록 MySQL에서는 어떤 락을 제공하는지 살펴보자.Lock 유형 InnoDB는 행 수준(row-level) 잠금을 구현하며(테이블 수준 잠금도 지원), 공유(S, Shared Lock) 잠금과 배타(X, Exclusive Lock) 잠금 두 가지 유형이 있다.Shared Lock 공유(S) 잠금 해당 잠금을 보유한 트랜잭션이 행을 읽을 수 있도록 허용 트랜잭션 T1이 행 r에 대해 공유(S) 잠금을 보유하고 있는 경우, 별개의 트랜잭션 T2가 행 r에 대해 잠금을 요청하면 다음과 같이 처리된다: T2가 S 잠금을 요청한 경우, 즉시 허용한다. 그 결과, T1과 T2는 모두 r에 대해 S 잠금을 보유하게 된다. T2가 X 잠금을 요청한 경우는 허용하지 않는다.Exclusive Lock 배타(X) 잠금 해당 잠금을 보유한 트랜잭션이 행을 수정하거나 삭제할 수 있도록 허용 트랜잭션 T1이 행 r에 대해 배타(X) 잠금을 보유하고 있는 경우, 별개의 트랜잭션 T2가 r에 대해 어떤 종류의 잠금(S 또는 X)을 요청하더라도 허용하지 않는다. 즉, 트랜잭션 T2는 T1이 행 r에 대한 잠금을 해제할 때까지 기다려야 한다.Lock 종류Intention Lock 트랜잭션이 테이블의 특정 행에 대해 나중에 어떤 종류의 잠금(공유 또는 배타)을 설정할 것인지를 나타내는 테이블 수준의 잠금목적 다중 세분화 잠금(multiple granularity locking)을 지원하여 행 잠금(row locks) 과 테이블 잠금(table locks) 이 공존하며 효율적으로 작동할 수 있게 하기 위함예시를 통해 의도 잠금 필요성 이해하기A 트랜잭션이 IX 락을 테이블에 걸고 있는 상태B 트랜잭션이 LOCK TABLE my_table WRITE로 테이블 전체 락을 요청→ IX 락이 이미 있기 때문에, 바로 충돌 판단 후 B는 block됨→ 전체 행 잠금 목록을 다 뒤져볼 필요 없음※ 다중 세분화 잠금(Multiple Granularity Locking) ? 데이터베이스에서 여러 수준(granularity)의 객체에 대해 동시에 잠금을 설정할 수 있도록 하는 메커니즘 즉, 테이블(table), 페이지(page), 행(row)과 같이 데이터 구조의 다양한 수준에 대해 서로 다른 크기의 잠금을 동시에 적용할 수 있게 만드는 기법종류 의도 공유 잠금(IS 잠금) 트랜잭션이 테이블 내 개별 행들에 대해 공유 잠금을 설정하려고 의도하고 있음을 나타낸다. ex : SELECT ... FOR SHARE 는 IS 잠금을 설정 의도 배타 잠금(IX 잠금) 트랜잭션이 테이블 내 개별 행들에 대해 배타 잠금을 설정하려고 의도하고 있음을 나타낸다. ex : SELECT ... FOR UPDATE는 IX 잠금을 설정 동작 방식 트랜잭션이 테이블의 행에 대해 공유 잠금을 획득하려면, 먼저 해당 테이블에 대해 IS 잠금 또는 그보다 강한 잠금을 획득해야 한다. 트랜잭션이 테이블의 행에 대해 배타 잠금을 획득하려면, 먼저 해당 테이블에 대해 IX 잠금을 획득해야 한다. 트랜잭션은 충돌하는 잠금이 해제될 때까지 대기한다. 하지만, 잠금 요청이 기존 잠금과 충돌하고 이로 인해 데드락(deadlock) 이 발생할 수 있다면, 에러가 발생한다.잠금 호환성 의도 잠금은 전체 테이블 잠금 요청(예: LOCK TABLES … WRITE)을 제외하고는 아무것도 차단하지 않는다. 현재 잠금 ↓ / 요청 잠금 → X IX S IS X ❌ ❌ ❌ ❌ IX ❌ ✅ ❌ ✅ S ❌ ❌ ✅ ✅ IS ❌ ✅ ✅ ✅ -- 트랜잭션 ASTART TRANSACTION;SELECT * FROM my_table WHERE id = 1 FOR UPDATE; -- IX (on table) + X (on row id=1)-- 트랜잭션 BSTART TRANSACTION;SELECT * FROM my_table WHERE id = 2 FOR UPDATE; -- IX (on table) + X (on row id=2) 둘 다 같은 테이블에서 작업하지만 각각 다른 row에 row-level X 락을 요청 그래서 InnoDB는 테이블에 각각 IX 잠금을 설정함 IX와 IX는 호환되기 때문에 동시에 작업 가능Record Lock 인덱스 레코드에 설정되는 잠금 예를 들어, SELECT c1 FROM t WHERE c1 = 10 FOR UPDATE; 는 t.c1이 10인 행을 다른 트랜잭션이 삽입, 수정, 삭제하지 못하도록 막는다. 레코드 락은 항상 인덱스 레코드를 대상으로 작동하며, 테이블에 인덱스가 없어도 마찬가지이다. 인덱스가 없는 경우, InnoDB는 숨겨진 클러스터형 인덱스를 자동으로 생성하고, 이를 사용해 레코드 잠금을 수행한다.※ 인덱스 레코드 ? 인덱스에 저장된 각 항목(=행의 위치를 가리키는 엔트리)즉, 실제 행 데이터가 아닌 인덱스 내부의 키와 포인터로 구성된 데이터 구조 InnoDB는 모든 테이블을 클러스터형 인덱스(Primary Key 기반 B+트리) 구조로 저장 따라서, 어떤 행(row)을 찾고 수정할 때도 항상 인덱스를 통해 위치를 찾고 작업 잠금의 대상이 되는 행은 곧 인덱스 레코드 상에 존재하는 엔트리이기 때문에, 인덱스 레코드 단위 락을 거는 것이 정확하고 효율적 gap lock, next-key lock 같은 락 전략도 전부 인덱스 레코드 사이 또는 레코드 + 갭 을 기준으로 동작Gap Lock 인덱스 레코드들 사이의 간격이나, 첫 번째 이전 또는 마지막 이후의 간격에 설정되는 잠금이다. 예를 들어 SELECT c1 FROM t WHERE c1 BETWEEN 10 and 20 FOR UPDATE; 쿼리는, 해당 범위 내의 기존 값이 존재하든 아니든 관계없이, 다른 트랜잭션이 c1에 15를 삽입하지 못하게 막는다. 이는 해당 범위 내 모든 값들 사이의 간격에 갭 락이 걸리기 때문이다. 갭은 하나의 인덱스 값 사이일 수도 있고, 여러 값 사이일 수도 있으며, 심지어 아무 값이 없어도 갭이 존재할 수 있다. 갭 락은 성능과 동시성 간의 절충의 일부로, 일부 트랜잭션 격리 수준에서만 사용되고 다른 수준에서는 사용되지 않는다. 유니크 인덱스를 통해 특정 행을 찾는 쿼리는 갭 락이 필요 없다. 단, 다중 열 유니크 인덱스 중 일부 열만 조건에 포함된 경우에는 예외로, 갭 락이 발생한다. 예를 들어, id 컬럼에 유니크 인덱스가 있다면 다음 쿼리는 id가 100인 행에만 레코드 잠금만 걸리고, 다른 세션이 그 앞에 값을 삽입하더라도 문제되지 않는다.SELECT * FROM child WHERE id = 100; 만약 id가 인덱스가 없거나 유니크하지 않은 인덱스라면, 해당 쿼리는 앞쪽 갭에도 잠금을 건다. 여기서 주목할 점은, 서로 충돌하는 락이 같은 갭에 대해 서로 다른 트랜잭션에 의해 유지될 수 있다는 것이다. 즉, 갭 락(gap lock)은 공유/배타(S/X)라는 구분이 있지만, 실제로는 충돌하지 않는다. 예를 들어, 트랜잭션 A가 어떤 갭에 대해 공유 갭 락을 걸고 있는 동시에, 트랜잭션 B가 배타 갭 락을 같은 갭에 걸 수도 있다. 서로 충돌하는 갭 락이 허용되는 이유는, 인덱스에서 어떤 레코드가 제거될 경우, 해당 레코드에 걸린 여러 트랜잭션의 갭 락을 병합해야 하기 때문이다. InnoDB의 갭 락은 “순수하게 억제용” 으로, 그 목적은 다른 트랜잭션이 해당 갭에 값을 삽입하지 못하게 막는 것뿐이다. 공유 갭 락과 배타 갭 락은 실제로 차이가 없으며, 서로 충돌하지 않고, 같은 역할을 한다. 갭 락은 명시적으로 비활성화될 수 있으며, 이는 트랜잭션 격리 수준을 READ COMMITTED로 바꿀 때 발생한다. 이 경우, 갭 락은 검색이나 인덱스 스캔에서는 사용되지 않으며, 외래 키 제약 조건이나 중복 키 검사에만 사용된다. READ COMMITTED 격리 수준을 사용할 경우 기타 부수적인 효과들도 있다. WHERE 조건에 맞지 않는 레코드에 걸린 잠금은, MySQL이 조건을 평가한 후 즉시 해제된다. UPDATE 문에 대해서 InnoDB는 “세미 일관성(semi-consistent)” 읽기를 수행하는데, 이는 MySQL에게 가장 최근 커밋된 버전을 전달하여 해당 행이 WHERE 조건에 맞는지 판단하도록 하는 방식이다. ※ S/X 갭 락 차이가 없는데 구분하는 이유 ? S Gap Lock과 X Gap Lock은 MySQL 사용자의 관점에서는 구분되지만, 내부적으로는 “갭에 대한 삽입 차단”이라는 동일한 효과를 가짐. 따라서 실제로는 갭을 잠갔는지 여부만 중요. 즉, 삽입이 가능/불가능이 핵심이지, “공유냐 배타냐”는 큰 의미가 없음 다만, 사용자 또는 SQL 구문 단위로는 정확한 트랜잭션 격리 수준 표현을 위해 구분함. 즉, 쿼리의 의도(intent)를 명확히 하기 위해 SQL 레벨에서 구분하는 것 SELECT ... FOR UPDATE: \"나는 값을 수정할 수도 있어\" → X Gap LockSELECT ... LOCK IN SHARE MODE: \"나는 읽기만 할 거야\" → S Gap Lock※ 필터링 조건과 갭 락 유니크 인덱스를 통해 특정 행을 찾는 쿼리는 갭 락이 필요 없다. 왜냐하면 이 경우 InnoDB는 정확히 하나의 행만 잠그면 되므로, 범위 전체를 잠글 필요가 없음 단, 다중 열 유니크 인덱스 중 일부 열만 조건에 포함된 경우에는 예외로, 갭 락이 발생한다. 왜냐하면 조건이 애매하거나 불완전하면 MySQL이 정확히 하나의 행을 찾았는지 확신할 수 없기 때문 CREATE TABLE product ( category_id INT, code VARCHAR(50), price INT, UNIQUE(category_id, code));-- 쿼리SELECT * FROM product WHERE category_id = 1 FOR UPDATE; (category_id, code)는 복합 유니크 인덱스 그런데 검색 조건은 category_id = 1만 포함됨 (불완전 조건) InnoDB는 “정확히 하나의 행만” 찾았는지 보장할 수 없음 따라서, 검색된 범위 전체에 next-key lock (record + gap) 걸게됨 SELECT * FROM product WHERE id = 123 FOR UPDATE LIMIT 1; 정확한 유니크 인덱스를 이용한 조건 + LIMIT 이 경우는 정확히 한 row만 찾아서 그 row만 record lock (X) 걸림Next-Key Lock 인덱스 레코드에 대한 레코드 락과, 그 앞에 있는 갭에 대한 갭 락의 조합 InnoDB는 테이블 인덱스를 검색하거나 스캔할 때, 발견한 인덱스 레코드에 공유 또는 배타 잠금을 설정하는 방식으로 행 수준 잠금을 수행한다. 따라서, 행 수준 잠금은 실제로는 인덱스 레코드 잠금이다. 인덱스 유형 next-key lock 기준 컬럼 유니크 인덱스 (단일/복합) 모든 인덱스 컬럼이 완전히 조건에 포함되어야 정확히 1건만 잠금 (→ gap lock 없음) 비유니크 복합 인덱스 첫 번째 컬럼 기준으로 범위 스캔, 스캔된 범위의 모든 레코드에 next-key lock 발생 인덱스 레코드에 설정된 넥스트키 락은 해당 인덱스 레코드 바로 앞의 갭에도 영향을 미친다. 즉, 넥스트키 락은 인덱스 레코드 잠금 + 그 앞 갭에 대한 갭 락이다. 따라서, 한 세션이 인덱스에서 레코드 R에 대해 공유 또는 배타 잠금을 가지고 있다면, 다른 세션은 R 바로 앞의 갭에 새로운 인덱스 레코드를 삽입할 수 없다.예시-- 인덱스에 값이 10, 11, 13, 20이 있다고 가정-- 이 인덱스에 대해 가능한 넥스트키 락은 다음 구간들을 포함-- (소괄호는 끝점 제외, 대괄호는 끝점 포함)(-무한대, 10](10, 11](11, 13](13, 20](20, +무한대) 마지막 구간에서는, 넥스트키 락이 인덱스에서 가장 큰 값보다 위에 있는 갭과, 실제 인덱스 값보다 큰 값을 가지는 “supremum” 가상 레코드를 잠근다. supremum은 실제 인덱스 레코드가 아니므로, 결국 이 넥스트키 락은 가장 큰 인덱스 값 이후의 갭만 잠그는 것과 같다. 기본적으로 InnoDB는 REPEATABLE READ 트랜잭션 격리 수준에서 동작한다. 이 경우 InnoDB는 검색과 인덱스 스캔에 넥스트키 락을 사용하여, 팬텀 레코드(phantom row)의 발생을 방지한다.Insert Intention Locks INSERT 작업이 행을 실제로 삽입하기 전에 설정하는 일종의 갭 락(gap lock) 이 잠금은 같은 인덱스 갭에 삽입하려는 의도를 표시하며, 서로 다른 위치에 삽입하는 경우에는 트랜잭션끼리 서로 대기하지 않도록 해준다.예시1-- 데이터 세팅mysql&gt; CREATE TABLE child (id int(11) NOT NULL, PRIMARY KEY(id)) ENGINE=InnoDB;mysql&gt; INSERT INTO child (id) values (90),(102);-- 트랜잭션 ASTART TRANSACTION;INSERT INTO child (id) VALUES (101); -- (90, 102) 갭에 Insert Intention Lock이 걸림-- 트랜잭션 BSTART TRANSACTION;INSERT INTO child (id) VALUES (95); 트랜잭션 B에서 동일하게 (90, 102) 갭에 Insert Intention Lock을 잡지만 Insert Intention Lock끼리는 호환되므로 대기없이 바로 Insert 가능예시2-- 데이터 세팅mysql&gt; CREATE TABLE child (id int(11) NOT NULL, PRIMARY KEY(id)) ENGINE=InnoDB;mysql&gt; INSERT INTO child (id) values (90),(102);-- 트랜잭션 Amysql&gt; CREATE TABLE child (id int(11) NOT NULL, PRIMARY KEY(id)) ENGINE=InnoDB;mysql&gt; INSERT INTO child (id) values (90),(102);mysql&gt; START TRANSACTION;mysql&gt; SELECT * FROM child WHERE id &gt; 100 FOR UPDATE; -- 102 레코드에 배타 잠금 + 그 앞 갭에 갭 락 발생+-----+| id |+-----+| 102 |+-----+-- 트랜잭션 Bmysql&gt; START TRANSACTION;mysql&gt; INSERT INTO child (id) VALUES (101); 트랜잭션 B에서 101을 삽입하려고 하지만, A의 FOR UPDATE가 id &gt; 100 범위의 갭을 이미 잠가버렸기 때문에, B는 insert intention lock을 걸고 기다리게 됨. (즉, 갭 락과는 호환되지 않음)확인해보기 데이터 세팅CREATE TABLE students ( id INT AUTO_INCREMENT PRIMARY KEY, class_no INT NOT NULL, name VARCHAR(100) NOT NULL, age INT, INDEX idx_class_no_name (class_no, name));INSERT INTO students (class_no, name, age) VALUES(1, 'Alice', 14),(1, 'Bob', 15),(2, 'Charlie', 13),(2, 'Alice', 14),(3, 'David', 16),(1, 'Eve', 14),(2, 'Frank', 15),(3, 'Grace', 13),(1, 'Bob', 15),(3, 'Heidi', 14);SELECT * FROM studentsORDER BY class_no; id class_no name age 1 1 Alice 14 2 1 Bob 15 6 1 Eve 14 9 1 Bob 15 3 2 Charlie 13 4 2 Alice 14 7 2 Frank 15 5 3 David 16 8 3 Grace 13 10 3 Heidi 14 ※ 참고 쿼리 수행후 락 관련 정보는 다음 쿼리로 확인SELECT object_name, index_name, lock_type, lock_mode, lock_dataFROM performance_schema.data_locks;performance_schema.data_locks에서의 lock_mode 해석 X 보통 Next-Key Lock을 의미 하지만, 이 정보만으로는 정확하게 Record-only Lock인지 Next-Key Lock인지 구분이 안 될 수도 있음 X_REC_NOT_GAP 명확하게 Record-only Lock을 의미 즉, 해당 레코드만 잠그고 GAP은 잠그지 않음 대표적으로 Unique Index로 정확하게 일치하는 값을 조회할 때 발생 X,GAP Gap Lock만 발생한 경우로, 보통 범위 조건이나 범위 외 레코드가 없을 때 발생 TEST1SELECT * FROM studentsWHERE class_no = 1FOR UPDATE; object_name index_name lock_type lock_mode lock_data students (NULL) TABLE IX (NULL) students idx_class_no_name RECORD X,GAP 2, ‘Alice’, 4 students idx_class_no_name RECORD X 1, ‘Alice’, 1 students idx_class_no_name RECORD X 1, ‘Bob’, 2 students idx_class_no_name RECORD X 1, ‘Eve’, 6 students idx_class_no_name RECORD X 1, ‘Bob’, 9 students PRIMARY RECORD X_REC_NOT_GAP 1 students PRIMARY RECORD X_REC_NOT_GAP 2 students PRIMARY RECORD X_REC_NOT_GAP 6 students PRIMARY RECORD X_REC_NOT_GAP 9 TEST2SELECT * FROM studentsWHERE class_no = 1AND age &lt; 15FOR UPDATE; object_name index_name lock_type lock_mode lock_data students (NULL) TABLE IX (NULL) students idx_class_no_name RECORD X,GAP 2, ‘Alice’, 4 students idx_class_no_name RECORD X 1, ‘Alice’, 1 students idx_class_no_name RECORD X 1, ‘Bob’, 2 students idx_class_no_name RECORD X 1, ‘Eve’, 6 students idx_class_no_name RECORD X 1, ‘Bob’, 9 students PRIMARY RECORD X_REC_NOT_GAP 1 students PRIMARY RECORD X_REC_NOT_GAP 2 students PRIMARY RECORD X_REC_NOT_GAP 6 students PRIMARY RECORD X_REC_NOT_GAP 9 age &lt; 15인 조건에 포함되는 레코드는 2개지만, age = 15인 레코드도 스캔 대상에 들어가서 불필요하게 락이 잡히게된다.TEST3SELECT * FROM studentsWHERE class_no = 1LIMIT 2FOR UPDATE; object_name index_name lock_type lock_mode lock_data students (NULL) TABLE IX (NULL) students idx_class_no_name RECORD X 1, ‘Alice’, 1 students idx_class_no_name RECORD X 1, ‘Bob’, 2 students PRIMARY RECORD X_REC_NOT_GAP 1 students PRIMARY RECORD X_REC_NOT_GAP 2 처음부터 2개만 스캔하기 때문에, 정렬된 순서대로 2개만 락이 걸림SELECT * FROM studentsWHERE class_no = 1LIMIT 2, 2FOR UPDATE; object_name index_name lock_type lock_mode lock_data students (NULL) TABLE IX (NULL) students idx_class_no_name RECORD X 1, ‘Alice’, 1 students idx_class_no_name RECORD X 1, ‘Bob’, 2 students idx_class_no_name RECORD X 1, ‘Eve’, 6 students idx_class_no_name RECORD X 1, ‘Bob’, 9 students PRIMARY RECORD X_REC_NOT_GAP 1 students PRIMARY RECORD X_REC_NOT_GAP 2 students PRIMARY RECORD X_REC_NOT_GAP 6 students PRIMARY RECORD X_REC_NOT_GAP 9 LIMIT으로 2개만 리턴하지만 2부터이므로, 결국 4개 스캔하는거라 4개 모두 락 걸림TEST4SELECT * FROM studentsWHERE id &lt; 5FOR UPDATE; object_name index_name lock_type lock_mode lock_data students (NULL) TABLE IX (NULL) students PRIMARY RECORD X,GAP 5 students PRIMARY RECORD X 1 students PRIMARY RECORD X 2 students PRIMARY RECORD X 3 students PRIMARY RECORD X 4 4,5 사이에 아무것도 들어갈 수 없는데 왜 갭락이 걸릴까 ? 사용자가 어떤 제약 조건을 갖고 있든 간에, InnoDB는 B+Tree 인덱스 구조만 보고 범위를 판단하고 잠금을 건다. 즉, id가 AUTO_INCREMENT든, PRIMARY KEY든, 중간 삽입이 불가능하든 그건 InnoDB 입장에서 알 수 없는 정보 범위 걸리는 잠금 종류 (-∞, 1) GAP only [1] X (Next-Key Lock) (1, 2) GAP 포함 → [2]도 마찬가지 [2] X (Next-Key Lock) … … [4] X (Next-Key Lock) (4, 5) → id=5에 X,GAP TEST5 유니크 인덱스 email 추가 Tx1 INSERT STUDENTS (class_no, email, name, age, flag) VALUES (5, 'email@test.com', 'lee', 10, 'N'); object_name index_name lock_type lock_mode lock_data students (NULL) TABLE IX (NULL) Tx2 INSERT STUDENTS (class_no, email, name, age, flag) VALUES (5, 'email@test.com', 'kim', 10, 'N'); object_name index_name lock_type lock_mode lock_data students (NULL) TABLE IX (NULL) students (NULL) TABLE IX (NULL) students idx_email RECORD X,REC_NOT_GAP ‘email@test.com’, 17 students idx_email RECORD S ‘email@test.com’, 17 Tx2도 동일한 이메일로 INSERT를 시도함. MySQL은 UNIQUE 제약 위반 가능성이 있으므로 다음을 수행: UNIQUE 인덱스를 조회하여 email@test.com이 존재하는지 확인 이 과정에서 인덱스 키 email@test.com'에대해 S Lock (공유 잠금)을 걸어 존재 유무를 확인 하지만, Tx1이 이미 인덱스 레코드 락 (X, REC_NOT_GAP)을 잡고 있어서, Tx2는 대기(blocking) 상태 참고 자료 https://dev.mysql.com/doc/refman/8.4/en/innodb-locking.html" }, { "title": "MySQL - ReadView 들여다보기", "url": "/posts/mysql-readview/", "categories": "지식 더하기, 들여다보기", "tags": "MySQL", "date": "2025-06-30 22:00:00 +0900", "snippet": " MySQL InnoDB에서 트랜잭션 격리수준이 READ COMMITTED이면 다른 트랜잭션에서 커밋 완료한걸 읽을 수 있고, REPEATABLE READ이면 다른 트랜잭션에서 커밋을 완료했더라도 해당 데이터를 무조건 읽을 수 있는 것은 아니다. 이게 어떻게 가능한걸까 ? 결론부터 말하면, MVCC 매커니즘은 현재 트랜잭션 내에서 어떤 레코드(...", "content": " MySQL InnoDB에서 트랜잭션 격리수준이 READ COMMITTED이면 다른 트랜잭션에서 커밋 완료한걸 읽을 수 있고, REPEATABLE READ이면 다른 트랜잭션에서 커밋을 완료했더라도 해당 데이터를 무조건 읽을 수 있는 것은 아니다. 이게 어떻게 가능한걸까 ? 결론부터 말하면, MVCC 매커니즘은 현재 트랜잭션 내에서 어떤 레코드(row)를 볼 수 있는지(가시성)를 판단하기 위해 ReadView 객체를 활용한다. READ COMMITTED : 매 SELECT 질의마다 ReadView 생성 REPEATABLE READ : 최초 SELECT 질의에서만 ReadView 생성 REPEATABLE READ 예시-- 트랜잭션 ABEGIN;UPDATE users SET ... WHERE id = 1; -- 아직 SELECT 없음 → ReadView 생성 안 됨-- 이 사이에 트랜잭션 B가 COMMITSELECT * FROM users WHERE age &gt; 30; -- 이제 ReadView 생성 → B의 결과 보일 수 있음SELECT * FROM users WHERE age &gt; 30; -- 위와 동일한 ReadView 사용COMMIT;들어가기전 InnoDB의 각 레코드는 db_trx_id와 db_roll_pointer라는 두 개의 숨겨진 컬럼을 가지고 있다. 그리고 테이블에 프라이머리 키나 NULL을 허용하지 않는 유니크 키가 없다면, 자동 증가하는 숨겨진 컬럼인 db_row_id를 생성한다.db_trx_id 이 레코드를 최종적으로 수정한 트랜잭션의 IDdb_roll_pointer 이 레코드의 이전 버전(undo 로그) 위치를 가리키는 포인터db_row_id 기본 키(PK)나 NOT NULL 유니크 키가 없을 경우, 내부적으로 레코드를 유일하게 식별하기 위한 ID 출처 : https://xialeistudio.medium.com/understanding-mvcc-in-mysql-innodb-116100a27b65 ReadView 살펴보기 ReadView 클래스 문서 참고한 소스코드 mysql-server/storage/innobase/include/read0types.h mysql-server/storage/innobase/read/read0read.cc 멤버 변수 및 초기화// mysql-server/storage/innobase/include/read0types.h private: trx_id_t m_low_limit_id; trx_id_t m_up_limit_id; trx_id_t m_creator_trx_id; ids_t m_ids; trx_id_t m_low_limit_no;// mysql-server/storage/innobase/read/read0read.ccvoid ReadView::prepare(trx_id_t id) { ut_ad(trx_sys_mutex_own()); m_creator_trx_id = id; m_low_limit_no = trx_get_serialisation_min_trx_no(); m_low_limit_id = trx_sys_get_next_trx_id_or_no(); ut_a(m_low_limit_no &lt;= m_low_limit_id); if (!trx_sys-&gt;rw_trx_ids.empty()) { copy_trx_ids(trx_sys-&gt;rw_trx_ids); } else { m_ids.clear(); } /* The first active transaction has the smallest id. */ m_up_limit_id = !m_ids.empty() ? m_ids.front() : m_low_limit_id; ut_a(m_up_limit_id &lt;= m_low_limit_id); ut_d(m_view_low_limit_no = m_low_limit_no); m_closed = false;} 멤버 변수 설명 ids_t m_ids 스냅샷(ReadView)이 만들어질때 활성화 되어있던(커밋되지 않은) Read-Write 트랜잭션의 id 목록. 즉, 이 ReadView가 볼 수 없어야 하는 트랜잭션 ID 목록. trx_id_t m_low_limit_id Read View 생성 시점에 아직 시작되지 않은 트랜잭션의 ID (즉, 다음으로 부여될 trx_id). 따라서, 이 값 이상의 trx_id를 가진 레코드는 보이지 않음 trx_id_t m_up_limit_id Read View 생성 시점에 활성 상태였던 트랜잭션들 중 가장 작은 trx_id. 따라서, 이 값 미만의 trx_id를 가진 레코드는 보임. ※ m_ids가 비어있으면, m_low_limit_id 값과 같음 trx_id_t m_creator_trx_id 이 ReadView를 만든 트랜잭션의 ID. trx_id_t m_low_limit_no 이 값보다 작은 트랜잭션 id는 Undo 로그를 볼 필요가 없음. 따라서, Undo 로그에서 삭제해도 되는 최소 트랜잭션 번호 기준 (purge 판단 시 사용). 즉, 현재 열려 있는 모든 ReadView 중 가장 오래된 것의 m_low_limit_no 를 기준으로, 그보다 작은 트랜잭션의 undo 로그는 모두 정리 가능 가시성 여부 판단// mysql-server/storage/innobase/include/read0types.h/** Check whether the changes by id are visible.@param[in] id transaction id to check against the view@param[in] name table name@return whether the view sees the modifications of id. */[[nodiscard]] bool changes_visible(trx_id_t id, const table_name_t &amp;name) const { ut_ad(id &gt; 0); if (id &lt; m_up_limit_id || id == m_creator_trx_id) { return (true); } check_trx_id_sanity(id, name); if (id &gt;= m_low_limit_id) { return (false); } else if (m_ids.empty()) { return (true); } const ids_t::value_type *p = m_ids.data(); return (!std::binary_search(p, p + m_ids.size(), id));}/**@param id transaction to check@return true if view sees transaction id */bool sees(trx_id_t id) const { return (id &lt; m_up_limit_id); }레코드의 trx_id가 m_ids에 포함된(즉, 아직 활성화된) 트랜잭션이면 가시성은 false여야 할텐데, 이 부분은 어딨을까 ? return (!std::binary_search(...)) 이 라인이 m_ids에 포함되면 false, 포함되지 않으면 true를 리턴하는 로직 std::binary_search(...)는 true를 리턴 (trx_id가 m_ids에 포함되어 있음) 즉, !std::binary_search(...)이면 포함되어 있지 않다는 뜻 = 가시성 있음 참고 자료 https://dev.mysql.com/doc/dev/mysql-server/8.4.4/classReadView.html#ac5d400b93acde22a935a34a672641e3c https://xialeistudio.medium.com/understanding-mvcc-in-mysql-innodb-116100a27b65" }, { "title": "MySQL - CHAR vs VARCHAR", "url": "/posts/mysql-char-vs-varchar/", "categories": "지식 더하기, 이론", "tags": "MySQL", "date": "2025-05-29 23:00:00 +0900", "snippet": "CHAR vs VARCHAR 데이터를 저장하고 조회하는 방식에서 차이가 있음 또한, 최대 길이와 후행 공백(trailing spaces) 유지 여부에서도 차이가 있음 엄격한(Strict) SQL 모드가 활성화되어 있지 않은 경우, CHAR 또는 VARCHAR 열에 해당 열의 최대 길이를 초과하는 값을 할당하면, 그 값은 열의 크기에 맞게 잘리고(...", "content": "CHAR vs VARCHAR 데이터를 저장하고 조회하는 방식에서 차이가 있음 또한, 최대 길이와 후행 공백(trailing spaces) 유지 여부에서도 차이가 있음 엄격한(Strict) SQL 모드가 활성화되어 있지 않은 경우, CHAR 또는 VARCHAR 열에 해당 열의 최대 길이를 초과하는 값을 할당하면, 그 값은 열의 크기에 맞게 잘리고(truncated) 경고가 발생 공백이 아닌 문자가 잘릴 경우에는, Strict SQL 모드를 사용하면 경고 대신 오류를 발생시키고 해당 값을 삽입하지 않도록 할 수 있음CHAR CHAR(n) 실제 데이터 길이에 상관없이 저장되는 데이터 길이가 n으로 고정 n은 0 ~ 255까지 가능 문자셋 문자당 바이트 수 CHAR(255)의 최대 바이트 latin1 1 byte 255 bytes utf8 (3바이트) 최대 3 bytes 최대 765 bytes utf8mb4 (4바이트) 최대 4 bytes 최대 1020 bytes 저장될때 n보다 작으면 나머지 부분은 공백으로 right-padded됨 값이 조회될때 PAD_CHAR_TO_FULL_LENGTH가 활성화되어 있지 않는이상 (기본은 비활성화), right-padded된 공백은 제거돼서 조회된다. PAD_CHAR_TO_FULL_LENGTH 속성은 deprecated됨 초과된 후행 공백은 SQL 모드와 관계없이 조용히(silently) 잘린다.VARCHAR 가변길이 데이터 길이는 0 ~ 65,535까지 가능 VARCHAR의 실질적인 최대 길이는 전체 행(Row)의 최대 크기인 65,535바이트와, 사용하는 문자셋에 따라 달라진다. 문자셋 문자당 바이트 실질 최대 VARCHAR 길이 (단일 컬럼 기준) latin1 1 byte 약 65,532자 utf8 최대 3 bytes 약 21,844자 utf8mb4 최대 4 bytes 약 16,383자 CHAR와 다르게 저장되는 값의 바이트를 알려주는 1바이트 또는 2바이트의 prefix 데이터가 따라 붙는다. 저장되는 형태 : [길이 prefix] + [실제 문자열] 길이 prefix 1바이트 : 저장되는 값이 255바이트 이하 2바이트 : 저장되는 값이 255바이트 초과 저장되는 데이터의 길이가 char(n)의 n보다 작아도 공백이 right-padded되지 않는다. 열 길이를 초과하는 후행 공백은 삽입 전에 잘리며, 이때 SQL 모드와 상관없이 경고가 발생 후행 공백은 그대로 저장 및 조회된다. 따라서, abc뒤에 공백이 붙은 상태로 VARCHAR 컬럼에 저장되면 SELECT * FROM test WHERE col_varchar = 'abc'; 쿼리는 col_varchar에 저장된 값이 ‘abc ‘이므로 매칭되지 않음. CHAR는 조회시 후행 공백이 제거되므로 조회됨※ 참고 : CHAR(n) 또는 VARCHAR(n)에서 n이 0인 경우 ? 비어 있는 문자열만 허용되는 컬럼 GPT에게 언제 필요할 것 같은지 물어보았다.-- 지금은 type이 의미 없지만,-- 나중에 type이 'coupon', 'event', 'system' 등의 의미를 가질 수 있음CREATE TABLE event_flags ( user_id INT, type VARCHAR(16), -- 지금은 VARCHAR(0)이지만 확장 가능 UNIQUE (user_id, type)); 처음에는 type이 항상 공백이지만, 미래에는 다양한 type을 넣고, 한 user_id에 여러 row를 허용할 수 있게 확장 가능 이럴 경우 기존 인덱스/제약 조건 구조를 변경하지 않고도 확장 가능기타 InnoDB는 768바이트 이상인 고정 길이 필드를 내부적으로 가변 길이 필드로 인코딩하여 데이터 페이지 외부(off-page)에 저장 예를 들어 CHAR(255) 컬럼은 utf8mb4 같이 한 글자가 최대 4바이트인 문자셋을 사용하면, 전체 길이가 1020바이트까지 될 수 있어 768바이트를 초과 CHAR, VARCHAR, TEXT 컬럼 값은 해당 컬럼에 지정된 문자셋의 collation 규칙에 따라 정렬 및 비교 MySQL의 대부분의 Collation은 PAD SPACE 속성을 가짐 단, UCA 9.0.0 이상을 기반으로 한 Unicode Collation은 NO PAD 속성을 가짐 NO PAD 속성에서는 문자열 뒤의 공백도 의미 있는 문자로 취급되어 비교 시 영향을 미침 PAD SPACE 속성에서는 문자열 뒤의 공백은 무시되며, 비교 시 없는 것처럼 취급됨 예시 : NO PAD vs PAD SPACE-- 두 테이블은 같은 구조지만 다른 collation 사용CREATE TABLE pad_space_test (val VARCHAR(10) COLLATE utf8mb4_general_ci -- PAD SPACE (기본값));CREATE TABLE no_pad_test (val VARCHAR(10) COLLATE utf8mb4_0900_as_cs -- NO PAD);-- 동일한 문자지만 후행 공백이 다른 값을 삽입INSERT INTO pad_space_test VALUES ('abc'), ('abc ');INSERT INTO no_pad_test VALUES ('abc'), ('abc ');-- 비교 결과SELECT 'PAD SPACE' AS collation, val FROM pad_space_test WHERE val = 'abc';SELECT 'NO PAD' AS collation, val FROM no_pad_test WHERE val = 'abc'; PAD SPACE: ‘abc’와 ‘abc ‘는 동일하다고 판단, 따라서 둘 다 조회됨 collation val PAD SPACE abc PAD SPACE abc NO PAD: ‘abc ‘는 다르다고 판단됨, 따라서 ‘abc’만 조회됨 collation val NO PAD abc 참고 자료 https://dev.mysql.com/doc/refman/8.4/en/char.html" }, { "title": "Spring - @Transactional 살펴보기 (1) - AOP Proxy", "url": "/posts/spring-transactional-part1/", "categories": "지식 더하기, 이론", "tags": "Spring", "date": "2025-05-25 00:25:00 +0900", "snippet": "AOP Proxy 스프링 : https://docs.spring.io/spring-framework/reference/core/aop/introduction-proxies.html 스프링 부트 : https://docs.spring.vmware.com/spring-boot/docs/3.0.14/reference/htmlsingle/#features....", "content": "AOP Proxy 스프링 : https://docs.spring.io/spring-framework/reference/core/aop/introduction-proxies.html 스프링 부트 : https://docs.spring.vmware.com/spring-boot/docs/3.0.14/reference/htmlsingle/#features.aop 프레임워크 AOP 프록시 기본값 조건 설명 Spring Framework JDK 동적 프록시 기본 인터페이스 기반이 권장되므로 JDK Proxy 선호 Spring Boot CGLIB 기본 (spring.aop.proxy-target-class=true) 실용성과 호환성 고려해서 CGLIB이 기본값 Spring Boot 3.4.4 additional-spring-configuration-metadata.json 파일 확인해보기 additional-spring-configuration-metadata.json : Spring Boot의 application.properties / application.yml 자동완성 기능을 지원하기 위한 메타데이터 파일 { \"name\": \"spring.aop.proxy-target-class\", \"type\": \"java.lang.Boolean\", \"description\": \"Whether subclass-based (CGLIB) proxies are to be created (true), as opposed to standard Java interface-based proxies (false).\", \"defaultValue\": true}Spring Framework Spring의 철학은 “인터페이스 기반 프로그래밍”을 권장 따라서 프록시도 JDK 동적 프록시 (인터페이스 기반)을 기본으로 선택 클래스에 대해 프록시를 만들려면 proxyTargetClass = true 명시적으로 설정해야 함Spring Boot “편의성과 일관성”을 중요시함 많은 개발자들이 인터페이스 없이도 AOP 사용을 원함 특히 @Transactional, @Async, @Scheduled 같은 어노테이션 기반 기능은 클래스에 붙는 경우가 많음 그래서 Spring Boot는 자동 설정으로 CGLIB을 기본으로 사용하도록 설정함Cglib (Byte Code Generation Library) github repo에 적힌 설명을 보면 다음과 같다. Byte Code Generation Library is high level API to generate and transform Java byte code.It is used by AOP, testing, data access frameworks to generate dynamic proxy objects and intercept field access. 바이트코드 생성 라이브러리(Byte Code Generation Library)는 Java 바이트코드를 생성하고 변형하기 위한 고수준 API입니다.이 라이브러리는 AOP(관점 지향 프로그래밍), 테스트, 데이터 접근 프레임워크 등에서 동적 프록시 객체를 생성하거나 필드 접근을 가로채기 위해 사용됩니다.   CGLIB 라이브러리 Spring 내장 CGLIB 클래스 경로 net.sf.cglib.proxy.Enhancer org.springframework.cglib.proxy.Enhancer 소스 cglib GitHub Spring이 자체적으로 재포장 왜 Spring은 CGLIB을 내장시켰나 ? 라이브러리 충돌 방지 예전엔 다른 라이브러리도 cglib을 사용했는데, 버전 충돌이 자주 발생 Spring은 이를 피하기 위해 패키지를 변경해서 자체 포함 (shading) 경량 의존성 구성 Spring Boot 사용 시 spring-core 하나로 충분히 동작 개발자가 cglib를 별도로 추가할 필요 없음 일관성 보장 Spring AOP, ProxyFactory, @Transactional 등 내부 동작이 cglib에 크게 의존 버전 변화나 API 변경의 영향을 줄이기 위해 org.springframework.cglib.*로 고정 Cglib AOP Proxy 살펴보기 Controller에서 보면 구현체가 UserService$$SpringCGLIB$$인 것을 확인할 수 있다.@Service@RequiredArgsConstructorpublic class UserService { private final UserRepository userRepository; private final UserHistoryRepository userHistoryRepository; public List&lt;User&gt; getAllUsers() { return userRepository.findAll(); } @Transactional public User create(User user) { userRepository.save(user); userHistoryRepository.save(UserHistory.from(user)); return user; }}어디서 생성하는걸까 ?// org.springframework.aop.framework.CglibAopProxy#buildProxyprivate Object buildProxy(@Nullable ClassLoader classLoader, boolean classOnly) { if (logger.isTraceEnabled()) { logger.trace(\"Creating CGLIB proxy: \" + String.valueOf(this.advised.getTargetSource())); } try { ... this.validateClassIfNecessary(proxySuperClass, classLoader); Enhancer enhancer = this.createEnhancer(); ... enhancer.setSuperclass(proxySuperClass); enhancer.setInterfaces(AopProxyUtils.completeProxiedInterfaces(this.advised)); enhancer.setNamingPolicy(SpringNamingPolicy.INSTANCE); enhancer.setAttemptLoad(true); enhancer.setStrategy(KotlinDetector.isKotlinType(proxySuperClass) ? new ClassLoaderAwareGeneratorStrategy(classLoader) : new ClassLoaderAwareGeneratorStrategy(classLoader, undeclaredThrowableStrategy)); Callback[] callbacks = this.getCallbacks(rootClass); Class&lt;?&gt;[] types = new Class[callbacks.length]; for(x = 0; x &lt; types.length; ++x) { types[x] = callbacks[x].getClass(); } ProxyCallbackFilter filter = new ProxyCallbackFilter(this.advised.getConfigurationOnlyCopy(), this.fixedInterceptorMap, this.fixedInterceptorOffset); enhancer.setCallbackFilter(filter); enhancer.setCallbackTypes(types); Object var22; try { var22 = classOnly ? this.createProxyClass(enhancer) : this.createProxyClassAndInstance(enhancer, callbacks); } finally { filter.advised.reduceToAdvisorKey(); } return var22; } ...}Enhancer 위 코드에서 보면, Enhancer가 프록시 생성에서 중요한 역할을 역할을 하는 것처럼 보인다.org.springframework.cglib.proxy.Enhancer (CGLIB의 핵심 클래스) CGLIB은 인터페이스가 아닌 클래스 자체를 상속하여 프록시를 만들 때 사용됨 이때 프록시 클래스를 만드는 역할을 담당하는 것이 Enhancer. 즉, CGLIB 프록시는 항상 Enhancer를 통해 생성됨 Enhancer는 내부적으로 ASM을 사용해서 프록시 클래스를 위한 바이트코드를 생성 이 바이트코드는 JVM의 defineClass()를 통해 메모리에 로드됨 기능 설명 setSuperclass(Class) 어떤 클래스를 상속할지 지정 (프록시 대상 클래스) setCallbacks(Callback[]) 메서드 호출 시 실행할 인터셉터 설정 setCallbackFilter(CallbackFilter) 어떤 메서드에 어떤 Callback을 적용할지 결정 create() 실제로 프록시 클래스를 생성하고 인스턴스화 // org.springframework.cglib.proxy.Enhancerpublic void setCallback(final Callback callback) { this.setCallbacks(new Callback[]{callback});}public void setCallbacks(Callback[] callbacks) { if (callbacks != null &amp;&amp; callbacks.length == 0) { throw new IllegalArgumentException(\"Array cannot be empty\"); } else { this.callbacks = callbacks; }}// org.springframework.cglib.proxy.MethodInterceptorpublic interface MethodInterceptor extends Callback { Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable;}예제public class TempInterceptor implements MethodInterceptor { @Override public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable { System.out.println(\"Before method: \" + method.getName()); Object result = proxy.invokeSuper(obj, args); // 원본 메서드 호출 System.out.println(\"After method: \" + method.getName()); return result; }}import org.springframework.cglib.proxy.Enhancer;public class Main { public static void main(String[] args) { Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(TempService.class); // 원본 클래스 지정 enhancer.setCallback(new TempInterceptor()); // 인터셉터 지정 // 프록시 인스턴스 생성 TempService proxy = (TempService) enhancer.create(); // TempService$$EnhancerByCGLIB$$ // 프록시 메서드 호출 (intercept → invokeSuper → 원본 호출) proxy.hello(\"CGLIB\"); }} 결과 Before method: helloHello, CGLIBAfter method: hello final 클래스는 프록시 생성 안됨 (상속을 못하기 때문에) Caused by: org.springframework.aop.framework.AopConfigException: Could not generate CGLIB subclass of class ... : Common causes of this problem include using a final class or a non-visible class @Transactional로 인해 생성된 프록시 객체에는 어떤 Interceptor가 세팅될까 ?DynamicAdvisedInterceptor 실제 프록시 메서드 실행 시 AOP advice를 연결하는 핵심 인터셉터 DynamicAdvisedInterceptor#intercept()는 프록시된 메서드가 호출될 때, 실제 메서드 호출 전 이 메서드가 먼저 실행됨 실제 호출흐름※ 참고 : AOP 관련 용어 Advice : 실제로 실행되는 부가 기능 로직 (예: 로깅, 트랜잭션 시작/커밋, 보안 체크 등) Pointcut : 어디에 Advice를 적용할지 지정. (예: com.example.service..(..)에 있는 모든 메서드) Advisor : Pointcut + Advice의 묶음. 즉, 실행 위치 + 실행할 로직을 함께 보유하는 AOP 구성 단위 JoinPoint: 애플리케이션 실행 중에 AOP로 가로챌 수 있는 모든 지점 (Spring AOP에서는 오직 메서드 실행만 해당)TransactionInterceptor, TransactionAspectSupport// org.springframework.transaction.interceptor.TransactionInterceptorpublic class TransactionInterceptor extends TransactionAspectSupport implements MethodInterceptor, Serializable { public TransactionInterceptor() { } public TransactionInterceptor(TransactionManager ptm, TransactionAttributeSource tas) { this.setTransactionManager(ptm); this.setTransactionAttributeSource(tas); } ... @Nullable public Object invoke(MethodInvocation invocation) throws Throwable { Class&lt;?&gt; targetClass = invocation.getThis() != null ? AopUtils.getTargetClass(invocation.getThis()) : null; Method var10001 = invocation.getMethod(); Objects.requireNonNull(invocation); return this.invokeWithinTransaction(var10001, targetClass, invocation::proceed); } ...}// org.springframework.transaction.interceptor.TransactionAspectSupport#invokeWithinTransaction@Nullableprotected Object invokeWithinTransaction(Method method, @Nullable Class&lt;?&gt; targetClass, final InvocationCallback invocation) throws Throwable { TransactionAttributeSource tas = this.getTransactionAttributeSource(); TransactionAttribute txAttr = tas != null ? tas.getTransactionAttribute(method, targetClass) : null; TransactionManager tm = this.determineTransactionManager(txAttr, targetClass); if (this.reactiveAdapterRegistry != null &amp;&amp; tm instanceof ReactiveTransactionManager rtm) { ... } else { PlatformTransactionManager ptm = this.asPlatformTransactionManager(tm); String joinpointIdentification = this.methodIdentification(method, targetClass, txAttr); if (txAttr != null &amp;&amp; ptm instanceof CallbackPreferringPlatformTransactionManager cpptm) { ... } else { TransactionInfo txInfo = this.createTransactionIfNecessary(ptm, txAttr, joinpointIdentification); Object retVal; try { retVal = invocation.proceedWithInvocation(); } catch (Throwable var23) { this.completeTransactionAfterThrowing(txInfo, var23); throw var23; } finally { this.cleanupTransactionInfo(txInfo); } if (retVal != null &amp;&amp; txAttr != null) { TransactionStatus status = txInfo.getTransactionStatus(); if (status != null) { ... } } this.commitTransactionAfterReturning(txInfo); return retVal; } }}" }, { "title": "WEB - nginx 처리율 제한 살펴보기", "url": "/posts/nginx-rate-limiting/", "categories": "지식 더하기, 이론", "tags": "WEB", "date": "2025-05-20 21:25:00 +0900", "snippet": " nginx는 처리율 제한을 위해 ‘leaky bucket algorithm’ 사용 leaky bucket algorithm 물 : 사용자 요청 버킷 : 요청이 대기하는 큐 넘치는 물 : 큐가 다 차서 거절되는 요청 새는 물 : 서버에 의해 처리되는 요청 limit_req_zone $binary_...", "content": " nginx는 처리율 제한을 위해 ‘leaky bucket algorithm’ 사용 leaky bucket algorithm 물 : 사용자 요청 버킷 : 요청이 대기하는 큐 넘치는 물 : 큐가 다 차서 거절되는 요청 새는 물 : 서버에 의해 처리되는 요청 limit_req_zone $binary_remote_addr zone=mylimit:10m rate=10r/s;server { location /login/ { limit_req zone=mylimit; proxy_pass http://my_upstream; }}limit_req_zone 문법 : limit_req_zone key zone=name:size rate=rate [sync]; Context : httpkey 어떤 기준으로 요청을 제한할지를 지정 예제에서는 $binary_remote_addr를 사용하고 있는데, 이 변수는 클라이언트의 IP 주소를 이진 형태로 나타낸다. 특정 api 호출 전체를 제한하려면 ? $server_name 같은 공통키 사용 zone 제한 상태(예: 각 IP가 얼마나 자주 요청했는지 등)를 저장하는 공유 메모리 공간을 지정 공유 메모리를 사용하기 때문에, 이 정보는 여러 NGINX 워커 프로세스 간에 공유 IP 주소 하나당 상태 정보 저장에 약 64바이트(32bit), 128바이트(64bit) 가 필요 따라서 32bit 기준 1MB로 약 16,000개 64비트 기준 약 8,000개 저장 가능 새로운 항목을 만들 때마다 최근 60초 동안 사용되지 않은 항목 최대 2개를 제거해 메모리 고갈을 방지 만약 저장 공간이 가득 찼는데 새로운 IP 주소의 상태 정보를 저장하려고 하면: 가장 오래된 항목을 제거하고, 그래도 공간이 부족하면 503(Service Temporarily Unavailable) 상태 코드를 반환 rate 최대 요청 속도를 지정 예를 들어 10r/s라고 하면, 초당 10건의 요청만 허용 nginx는 밀리초 단위로 요청을 추적하므로, 이 설정은 100ms마다 1개의 요청을 허용한다는 의미 지원 단위\t: 초(r/s), 분(r/m)sync ? 워커 프로세스가 여러 개일 때 공유 메모리(zone)를 이용해 rate limit 상태를 공유하지만, 워크로드이 많거나 고속 요청 환경에서는 다음 문제가 생길 수 있다: 워커 프로세스들이 동시에 zone을 업데이트하면 동기화 지연이나 race condition처럼 보이는 현상이 발생할 수 있음 즉, 잠깐 동안 초과 요청이 제한 없이 들어가는 것처럼 보일 수 있음 sync 옵션은 이런 상황에서: 정확도 향상 : 요청 제한 속도를 보다 정확하게 적용 레이스 컨디션 방지 : 워커 간 동시 업데이트 시 충돌 완화 Burst 처리의 일관성 개선 : 여러 워커가 동시에 burst 큐를 채우는 상황을 제어 limit_req 문법 : limit_req zone=name [burst=number] [nodelay | delay=number]; Context : http, server, locationburst 만약 위 같은 예시에서 동일한 IP에서 100ms 이내에 2개의 요청이 들어오면, NGINX는 두 번째 요청에 대해 503 (Service Temporarily Unavailable) 응답을 보냄 다른 응답 코드를 보내고 싶으면 limit_req_status 활용 하지만 대부분의 애플리케이션은 순간적으로 요청이 몰리는 현상(burst)이 발생location /login/ { limit_req zone=mylimit burst=20; proxy_pass http://my_upstream;} burst=20 : 지정된 속도(예: 초당 10회 요청)를 초과하는 최대 20개의 요청까지 큐에 저장 가능 예를 들어, 동시에 21개의 요청이 들어오면: 첫 번째 요청은 즉시 처리나머지 20개 요청은 큐에 저장 이후 100ms마다 하나씩 처리됨 (설정된 속도: 10r/s에 맞춰) 만약 큐가 꽉 찬 상태에서 또 요청이 들어오면, 그 요청은 503 에러로 거부됨 nodelay burst 설정은 요청을 일정 간격으로 처리하므로 트래픽이 부드럽게 흐르지만, 사용자 입장에서는 사이트가 느려진 것처럼 느껴질 수 있다. 예를 들어 큐의 마지막 요청(20번째)은 최대 2초까지 기다려야 하며, 이 응답은 이미 쓸모 없어졌을 수도 있다.location /login/ { limit_req zone=mylimit burst=20 nodelay; proxy_pass http://my_upstream;} nodelay는 큐 슬롯은 유지하지만, 큐에 넣은 요청을 즉시 처리 단, 속도 제한은 여전히 적용되며, 슬롯을 차지한 후 일정 시간(예: 100ms)이 지나야 슬롯이 다시 사용 가능 즉, 매100ms마다 슬롯 한 개를 비움 예시 상황 큐가 비어 있는 상태에서 IP 하나가 동시에 21개의 요청을 보냄 21개 모두 즉시 처리 이 중 20개는 큐 슬롯을 ‘점유 중’으로 표시 슬롯은 이후 100ms마다 하나씩 해제됨 501ms 후 20개 요청이 들어옴 슬롯 5개가 해제되어 있음 5개 요청 처리, 15개 요청 거부 (503) delaylimit_req_zone $binary_remote_addr zone=ip:10m rate=5r/s;server { listen 80; location / { limit_req zone=ip burst=12 delay=8; proxy_pass http://website; }} delay를 지정하면 일정 개수까지는 바로 처리하고, 나머지는 초당 요청 제한(rate)을 지키며 처리 출처 : https://blog.nginx.org/blog/rate-limiting-nginx 특정 IP 별도 처리geo $limit { default 1; 10.0.0.0/8 0; 192.168.0.0/24 0;}map $limit $limit_key { 0 \"\"; 1 $binary_remote_addr;}limit_req_zone $limit_key zone=req_zone:10m rate=5r/s;server { location / { limit_req zone=req_zone burst=10 nodelay; # ... }} geo 클라이언트 IP를 기반으로 $limit 변수 값을 설정 10.0.0.0/8 및 192.168.0.0/24 대역에 속한 IP는 $limit = 0 그 외 IP는 $limit = 1 (기본값) map $limit 값을 기준으로 $limit_key를 설정 $limit == 0 → $limit_key = \"\" (빈 문자열) $limit == 1 → $limit_key = $binary_remote_addr limit_req_zone $limit_key를 기준으로 요청 속도를 제한 키가 빈 문자열(““)이면 속도 제한이 적용되지 않음 즉, 허용된 IP(allowlist)는 제한 없이 통과되고, 나머지는 초당 5건으로 제한됨 여러 개의 limit_req 디렉티브를 동시에 사용 각 제한이 서로 충돌하면: 가장 강한 제한(최소 속도 or 가장 긴 지연)이 적용됨http { limit_req_zone $limit_key zone=req_zone:10m rate=5r/s; limit_req_zone $binary_remote_addr zone=req_zone_wl:10m rate=15r/s; server { location / { limit_req zone=req_zone burst=10 nodelay; limit_req zone=req_zone_wl burst=20 nodelay; # ... } }} 대상 IP req_zone (5r/s) 적용 req_zone_wl (15r/s) 적용 실제 제한 허용된 IP X (key가 “”) O (binary IP로 적용됨) 15r/s 제한 허용되지 않은 IP O O 5r/s 제한 (더 엄격한 제한 적용됨) 드는 생각1. 정확한 요청 수로 제한해야하는 경우에는 적합하지 않은 것 같다. 예를 들어, 1초에 최대 1000개의 요청을 허용하는 것을 기대하고 1000r/s로 세팅한 경우, busrt, nodelay를 사용하면 한번에 동시에 1000개 요청이 업스트림 서버로 전달될 수 있지만, 0.99초에 1000개가 한번에 들어오고 1.01초에 1000개가 또 들어오면 아직 점유된 슬롯들이 대부분 해제되지 않은 상태라 대부분의 요청이 거절될 것 또한, limit_req_zone의 sync 설명에서처럼 요청량이 많은 상황에서 공유 메모리가 어느정도로 잘 동기화 되는지는 미지수인 것 같음 가장 길게 줄 수 있는 지연이 1r/m 이기 때문에 그 이상으로 제한하기는 어렵다. rate=1r/60m 이런식의 설정이 안되는 이유 (아래 코드 참고, r/s, r/m만 가능)※ ngx_http_limit_req_module.c// ngx_http_limit_req_module.cstatic char *ngx_http_limit_req_zone(ngx_conf_t *cf, ngx_command_t *cmd, void *conf){ ... if (ngx_strncmp(value[i].data, \"rate=\", 5) == 0) { len = value[i].len; p = value[i].data + len - 3; if (ngx_strncmp(p, \"r/s\", 3) == 0) { scale = 1; len -= 3; } else if (ngx_strncmp(p, \"r/m\", 3) == 0) { scale = 60; len -= 3; } rate = ngx_atoi(value[i].data + 5, len - 5); if (rate &lt;= 0) { ngx_conf_log_error(NGX_LOG_EMERG, cf, 0, \"invalid rate \\\"%V\\\"\", &amp;value[i]); return NGX_CONF_ERROR; } continue; } ...}2. 1차적인 어뷰징 방지 수단으로는 적절할 수 있을 것 같다. 예를 들어 웹사이트의 메인 페이지에서 로그인하지 않아도 이용할 수 있는 기능들이 있을 때, (사용자에 의해서든, 봇에 의해서든) 불필요하게 많이 호출되는 것을 방지하기 위해 IP 기반으로 처리율을 제한하면 불필요하게 업스트림 서버까지 요청이 가는 것을 어느정도는 막을 수 있을 것으로 생각참고 자료 https://blog.nginx.org/blog/rate-limiting-nginx https://nginx.org/en/docs/http/ngx_http_limit_req_module.html#limit_req_zone https://nginx.org/en/docs/http/ngx_http_limit_req_module.html#limit_req" }, { "title": "자바에서 null 처리에 대해 생각해보기", "url": "/posts/how-to-deal-with-null/", "categories": "생각해보기, 코드 작성", "tags": "", "date": "2025-05-04 10:25:00 +0900", "snippet": "나는 언제 주로 null을 사용했을까 ? 참조값이 ‘없음’을 표현하는 경우 따라서, 보통 아래와 같이 코드를 작성하게 되는 것 같다.if (obj != null) { // do something}if (obj == null) { // null인게 비정상적인 상황이라면 적절한 예외 발생시키기}// do something그렇다면, null의...", "content": "나는 언제 주로 null을 사용했을까 ? 참조값이 ‘없음’을 표현하는 경우 따라서, 보통 아래와 같이 코드를 작성하게 되는 것 같다.if (obj != null) { // do something}if (obj == null) { // null인게 비정상적인 상황이라면 적절한 예외 발생시키기}// do something그렇다면, null의 단점은 뭐가 있을까 ? 해당 객체를 다룰 때 주의하지 않으면 NPE가 발생한다. 따라서, null인지를 확인하는 코드가 반복적으로 나타나야하고 이로 인해 코드가 지저분해질 수 있다. 또한, 이런 null 객체들이 코드 이곳 저곳에 돌아다니면 NPE가 발생할 확률은 높아질 것이고 이로 인해 서비스의 신뢰성이 낮아질 수 있다. null을 리턴받는 입장에서는 이게 진짜 값이 없는 것인지, 어떤 에러가 발생한 것인지 등 해당 값만으로는 정확한 맥락을 파악할 수가 없거나, 상세 구현을 확인해봐야 한다. 정확한 맥락을 파악할 수 없다는 것은, 그에 따른 적절한 처리를 하기가 어렵다는 것 그럼 어떤식으로 처리하면 좋을까 ?null 리턴 private 메서드와 같이 사용 범위가 제한적인 경우예외 발생 참조값이 없는게 예외로 간주되어야하는 경우Optional 이펙티브 자바를 참고해보면, null인게 예외인지 아닌지 등을 호출한 쪽에서 맥락에 따라 판단해야하는 경우 Optional이 괜찮은 선택일 수 있음 (CheckedException처럼 호출자에게 이 메서드를 호출할시 ‘리턴값이 없을 수 있음’을 명확하게 알릴 수 있기 때문에)빈 컬렉션 new ArrayList&lt;&gt;();, Collections.emptyList(); 등 NPE가 발생하지 않는다. 이에 따라, 코드가 조금 더 깔끔해질 수 있다. List&lt;Item&gt; items = getItems();// if (items != null &amp;&amp; !items.isEmpty()) -&gt; 이런 체크가 불필요for (Item item : items) { ...}null 오브젝트 패턴public interface Result { void doSomething();}public class RealResult implements Result { @Override public void doSomething() { System.out.println(\"Doing real work\"); }}public class NullResult implements Result { @Override public void doSomething() { // do nothing }}public Result findById(String id) { if (/* not found */) { return new NullResult(); } else { return new RealResult(); }}Result result = findById(\"someId\");result.doSomething(); // null 체크 없이 안전하게 호출" }, { "title": "Controller는 어떤 부분을 테스트 해야할까 ?", "url": "/posts/controller-test/", "categories": "생각해보기, 코드 작성", "tags": "", "date": "2025-04-29 22:20:00 +0900", "snippet": " 웹 서비스에서 보편적으로 사용되는 레이어 구조(Controller - Service - Repository)에서 Controller는 어떤 부분을 테스트하면 좋을지 생각해보자.테스트 코드의 필요성 ? 내가 생각하기에 테스트 코드가 필요한 이유는 크게 두 가지가 있을 것 같다. 코드가 변경됐을 때, 변경된 부분은 의도한대로 잘 동작하는지, 관련이...", "content": " 웹 서비스에서 보편적으로 사용되는 레이어 구조(Controller - Service - Repository)에서 Controller는 어떤 부분을 테스트하면 좋을지 생각해보자.테스트 코드의 필요성 ? 내가 생각하기에 테스트 코드가 필요한 이유는 크게 두 가지가 있을 것 같다. 코드가 변경됐을 때, 변경된 부분은 의도한대로 잘 동작하는지, 관련이 없는 부분은 원래대로 잘 동작하는지에 대한 보장을 받을 수 있다. 이를 통해 불필요하게 발생하는 운영 이슈를 줄이고 시스템 장애를 예방함으로 좀 견고하고 안정적인 서비스가 될 수 있다. 테스트하고자 하는 대상(기능, 클래스 등)이 어떻게 동작해야하는지에 대한 최신화된 명세서가 될 수 있다. 이를 통해 코드 내부를 일일이 다 살펴보지 않아도 어떤 역할을 하는 코드인지에 대한 파악이 가능하다.Controller 그렇다면, Controller의 어떤 부분에 대한 테스트를 작성해야 좀 더 안정적인 서비스가 될 수 있을까 내가 생각하는 Controller의 주된 역할은 클라이언트의 요청을 받고, 요청에 대한 처리 응답을 내주는 것이다. 클라이언트와 맞닿아 있기 때문에, Controller가 의도와 다르게 동작하면 호출하는 클라이언트에서도 예기치 못한 이슈가 발생할 수 있다.정상 케이스 정상적인 요청이 왔을때 정상적인 응답이 반환되는지 확인. 즉, 호출하는 입장에서 기대한 응답값이 있는 것을 보장. 이를 통해, 의도치않게 응답 필드가 변경되거나 빠지는 것 또는 추가되는 것 방지 반드시 존재해야하는 응답값 검증 가능 @WebMvcTest(FooController.class)class FooControllerTest { @Autowired private MockMvc mockMvc; @MockBean private FooService fooService; @Test void 정상_응답에_a_b만_포함되고_c는_없어야_한다() throws Exception { // Given ResultObject resultObject = new ResultObject(\"valueA\", \"valueB\", \"valueC\", \"valueD\"); given(fooService.doSomething(any(), any(), any())).willReturn(resultObject); RequestDTO requestDTO = new RequestDTO(\"param1\", \"param2\", \"param3\"); // When &amp; Then mockMvc.perform(post(\"/api/foo\") .contentType(MediaType.APPLICATION_JSON) .content(new ObjectMapper().writeValueAsString(requestDTO))) .andExpect(status().isOk()) .andExpect(jsonPath(\"$.data.a\").value(\"valueA\")) .andExpect(jsonPath(\"$.data.b\").value(\"valueB\")) .andExpect(jsonPath(\"$.data.c\").doesNotExist()); }}비정상 케이스 비정상적인 요청이 왔을때 1차적으로 Controller에서 막을 부분과 그에 대한 응답 처리가 적절하게 되는지 확인. 이를 통해 비정상적인 요청으로 인한 시스템에 이슈가 생길 수 있는 부분(데이터가 꼬이거나, 불필요한 데이터가 쌓이거나, 의도치않게 데이터가 삭제되거나, 페이징 사이즈 너무 커서 DB에 부하가 발생하는 등) 방지 호출하는 쪽에서 실수로 특정 값을 누락하는 등의 상황에서 응답에 포함된 적절한 메세지로 어떤 문제인지 인지할 수 있음 적절한 status code가 응답되는지 @WebMvcTest(SearchController.class)class SearchControllerTest { @Autowired private MockMvc mockMvc; @MockBean private SearchService searchService; @Test void 필수값_누락시_400과_에러메세지를_반환한다() throws Exception { // Given: 필수 필드 빠진 요청 String invalidRequestJson = \"\"\" { \"page\": 1, \"size\": 100 // \"keyword\" 빠짐 } \"\"\"; // When &amp; Then mockMvc.perform(post(\"/api/search\") .contentType(MediaType.APPLICATION_JSON) .content(invalidRequestJson)) .andExpect(status().isBadRequest()) .andExpect(jsonPath(\"$.success\").value(false)) .andExpect(jsonPath(\"$.errors\").isArray()) .andExpect(jsonPath(\"$.errors\", hasItem(\"검색어는 필수입니다.\"))); } @Test void 사이즈가_500을_초과하면_400과_에러메세지를_반환한다() throws Exception { // Given: size가 501로 너무 큰 요청 PagingRequestDTO request = new PagingRequestDTO(1, 501, \"검색어\"); // When &amp; Then mockMvc.perform(post(\"/api/search\") .contentType(MediaType.APPLICATION_JSON) .content(new ObjectMapper().writeValueAsString(request))) .andExpect(status().isBadRequest()) .andExpect(jsonPath(\"$.success\").value(false)) .andExpect(jsonPath(\"$.errors\", hasItem(\"size는 최대 500까지만 허용합니다.\"))); }} 이런 테스트들을 기반으로 Spring Rest Docs 등을 활용하여 API 명세까지 제공한다면, 호출하는 입장에서도 우리 서비스가 좀 더 예측 가능하지 않을까하는 생각이든다.실제 사례 참고 https://techblog.woowahan.com/14874/ 컨트롤러 유닛 테스트는 mockMvc를 사용하고, Service는 테스트 대역으로 대체합니다. =&gt; 지금 선물하기 시스템에는 테스트의 피드백 속도가 더 중요했기에 이런 트레이드오프의 결과로 ‘유닛 테스트 작성 시 테스트 대상 유닛과 다른 유닛의 협동, 위임 관계가 존재하는 테스트는 단독 테스트와 테스트 대역을 적극 사용한다.’ 라는 원칙이 합의되고 @SpringBootTest 애너테이션을 제거하게 됩니다.=&gt; 기본적으로 격리된 방식으로 쉽게 테스트할 수 있는 유닛에 대한 테스트는 협동 테스트를 적극 사용한다.테스트 대상 유닛과 다른 유닛의 협동, 위임 관계가 존재하는 테스트는 단독 테스트와 테스트 대역을 사용할 수 있다. https://rieckpil.de/how-to-test-spring-boot-web-controller-the-right-way/ https://mosyarch.com/guide-spring-boot-controller-tests/=&gt; 이해해보자" }, { "title": "Spring - Spring Rest Docs란 ?", "url": "/posts/spring-restdocs/", "categories": "지식 더하기, 이론", "tags": "Spring", "date": "2025-04-24 20:25:00 +0900", "snippet": "Spring REST Docs RESTful 서비스를 문서화하는 데 도움을 주는 도구 Asciidoctor로 작성한 수동 문서와 Spring MVC Test를 통해 자동 생성된 스니펫을 결합하여 사용 Spring WebFlux의 WebTestClient나 REST Assured 5 사용도 가능 이 프로젝트의 핵심 철학 ...", "content": "Spring REST Docs RESTful 서비스를 문서화하는 데 도움을 주는 도구 Asciidoctor로 작성한 수동 문서와 Spring MVC Test를 통해 자동 생성된 스니펫을 결합하여 사용 Spring WebFlux의 WebTestClient나 REST Assured 5 사용도 가능 이 프로젝트의 핵심 철학 중 하나는 테스트를 통해 문서를 생성한다는 점 이는 API의 실제 동작과 항상 일치하는 정확한 문서가 생성되도록 보장 테스트를 실행하면 요청과 그에 대한 응답에 대한 문서 스니펫이 자동으로 생성 Asciidoctor ? Asciidoc이라는 마크업 언어를 HTML, PDF 등의 문서로 변환해주는 도구 (공식 사이트) Spring REST Docs는 API 설명서의 본문을 Asciidoc 형식으로 작성하게 하고, 이를 Asciidoctor로 변환해서 최종 문서를 생성Asciidoctor로 작성한 수동 문서와 Spring MVC Test를 통해 자동 생성된 스니펫 ? Spring REST Docs는 테스트 코드를 실행할 때, API 요청과 응답을 자동으로 캡처해서 “스니펫(snippet)”이라고 불리는 작은 문서 조각들을 생성 Spring MVC 테스트 @Testvoid helloRestDocs() throws Exception { mockMvc.perform(get(\"/api/hello\")) .andExpect(status().isOk()) .andDo(document(\"hello\", responseFields( fieldWithPath(\"message\").description(\"The hello message\") ) ));} 스니펫 예시 curl-request.adoc: cURL 명령어 예시http-request.adoc: 요청 헤더/본문http-response.adoc: 응답 헤더/본문response-fields.adoc: 응답 JSON의 필드 설명 Asciidoctor로 작성한 수동 문서= API Documentation:toc: left= API 문서입니다.== Hello API=== `/api/hello`include::{snippets}/hello/http-request.adoc[] 생성된 HTML 문서참고 자료 https://spring.io/projects/spring-restdocs#overview https://www.baeldung.com/spring-rest-docs https://docs.spring.io/spring-restdocs/docs/current/reference/htmlsingle/" }, { "title": "OS - 리눅스 Load Average 살펴보기", "url": "/posts/load-average-part1/", "categories": "지식 더하기, 이론", "tags": "OS", "date": "2025-04-03 20:25:00 +0900", "snippet": "Load Average ? 리눅스의 load average는 시스템 전체의 부하 평균을 나타내며, 실행 중이거나 대기 중인 스레드(태스크)의 수를 평균으로 나타낸 것. 즉, 실행 가능한(runnable) 태스크뿐만 아니라, uninterruptible sleep 상태(중단 불가능한 대기 상태)에 있는 태스크들도 추적역사 처음엔 실행 가능한 태스크에...", "content": "Load Average ? 리눅스의 load average는 시스템 전체의 부하 평균을 나타내며, 실행 중이거나 대기 중인 스레드(태스크)의 수를 평균으로 나타낸 것. 즉, 실행 가능한(runnable) 태스크뿐만 아니라, uninterruptible sleep 상태(중단 불가능한 대기 상태)에 있는 태스크들도 추적역사 처음엔 실행 가능한 태스크에 대해 CPU가 처리해야 할 작업량을 나타내는 지표였다. 하지만, 이렇게 했을때 생길수 있는 문제점은: 시스템은 전체적으로 느려지지만 load average는 낮아지는 경우가 있음. 예 : 빠른 swap 디스크에서 느린 swap 디스크로 바꾼 경우 빠른 디스크 : 프로세스가 swap을 빨리 끝내고 다시 CPU 실행 대기열에 들어감. runnable 상태가 많아짐 → load average가 높게 나옴 느린 디스크 : 프로세스가 swap 중 I/O wait 상태로 오래 머무름 → runnable 상태에서 빠짐 (uninterruptible sleep 상태로 바뀜) → load average에서 제외됨 → load average가 낮게 나옴 따라서, 실행 가능한 태스크만 포함하게되면 부하로 인해 시스템 성능이 저하되는데 load average는 낮아질 수 있기 때문에 해당 지표가 직관적이지 않게됨 왜 uninterruptible sleep 상태의 작업이 많아지면 시스템이 느려질까 ? 예시 : swap-in/out, 파일 시스템 락, 디바이스 응답 대기, I/O wait 중인 프로세스 (보통 디스크 I/O 등)uninterruptible sleep (D 상태)의 특징 커널이 깰 수 없다. sleep 상태(S)는 인터럽트나 시그널로 깰 수 있는데, D 상태는 시스템 콜이 끝날 때까지 무조건 기다려야 한다. 리소스를 점유한다. CPU를 직접 점유하진 않지만, 다음과 같은 시스템 자원을 계속 붙잡고 있게됨: 메모리 페이지 I/O 버퍼 락 (파일 시스템 락, inode 락 등) 스레드/프로세스 slot 자체 스케줄러 입장에서 “일이 밀리는 중” CPU가 직접 처리하진 않더라도, 해당 프로세스가 리소스를 기다리며 큐에 쌓이고 있는 상태 평균이란 ? 1분, 5분, 15분 동안의 평균은 어떻게 계산되는걸까 ? load average 관련 커널 소스 코드를 참고해보았다. loadavg.c loadavg.h 평균 계산과 관련하여 참고한 주석과 코드는 아래와 같다.* The global load average is an exponentially decaying average of nr_running +* nr_uninterruptible.** Once every LOAD_FREQ:** nr_active = 0;* for_each_possible_cpu(cpu)*\tnr_active += cpu_of(cpu)-&gt;nr_running + cpu_of(cpu)-&gt;nr_uninterruptible;** avenrun[n] = avenrun[0] * exp_n + nr_active * (1 - exp_n)...avenrun[0] = calc_load_n(avenrun[0], EXP_1, active, n);avenrun[1] = calc_load_n(avenrun[1], EXP_5, active, n);avenrun[2] = calc_load_n(avenrun[2], EXP_15, active, n);...calc_load_n(unsigned long load, unsigned long exp, unsigned long active, unsigned int n){\treturn calc_load(load, fixed_power_int(exp, FSHIFT, n), active);}...#define LOAD_FREQ\t(5*HZ+1)\t/* 5 sec intervals */#define EXP_1\t\t1884\t\t/* 1/exp(5sec/1min) as fixed-point */#define EXP_5\t\t2014\t\t/* 1/exp(5sec/5min) */#define EXP_15\t\t2037\t\t/* 1/exp(5sec/15min) */calc_load(unsigned long load, unsigned long exp, unsigned long active){\tunsigned long newload;\tnewload = load * exp + active * (FIXED_1 - exp);\tif (active &gt;= load)\t\tnewload += FIXED_1-1;\treturn newload / FIXED_1;}예시 : 1분 Load Average 계산 위 코드를 온전히 이해하기는 어려워서 주석에 있는 avenrun[n] = avenrun[0] * exp_n + nr_active * (1 - exp_n) 식을 기반으로 GPT에게 물어보았다. (근데 avenrun[0] 이 아니라 avenrun[n]이 되어야하지 않나 ?) - 가정: 매 샘플 주기(LOAD_FREQ)마다 nr_active = 10 (즉, 실행 또는 대기 중인 프로세스 수가 10으로 일정함)- 초기 값: t=0일 때 load average 값=0- exp_1: 0.9200 (1분에 해당하는 지수 상수, 5초 간격으로 1분 평균을 맞추기 위한 감쇠율) 샘플 주기 (Tick) 이전 load (old_load) 계산식 새로운 load (new_load) 1 0 0 × 0.9200 + 10 × (1 - 0.9200) = 0 + 10 × 0.0800 0.8 2 0.8 0.8 × 0.9200 + 10 × 0.0800 = 0.736 + 0.8 1.536 3 1.536 1.536 × 0.9200 + 10 × 0.0800 = 1.414 + 0.8 ≈ 2.214 4 2.214 2.214 × 0.9200 + 10 × 0.0800 = 2.0369 + 0.8 ≈ 2.837 5 2.837 2.837 × 0.9200 + 10 × 0.0800 = 2.613 + 0.8 ≈ 3.413 … … … … 지수 평균의 의미 1분 load average는 단순히 60번 반복해서 평균을 낸 값이 아니라, 지수적으로 과거를 감쇠하며 현재 값을 더해가는 방식 단순 평균: “1분 동안 60번 재서 더한 뒤 나눈 값” 지수 평균 : “1분 동안 부하가 계속 유지되었다면 어떤 값으로 수렴하겠는가?” 샘플 주기 : LOAD_FREQ #define LOAD_FREQ\t(5*HZ+1)\t/* 5 sec intervals */ 위 코드는 load average를 5초마다 갱신하려는 의도 HZ : “1초당 몇 번 시스템 타이머 인터럽트가 발생하는가”를 나타내는 값 (커널의 시간 단위, 또는 1초를 몇 개의 틱(jiffies)으로 나눌 것인가를 결정하는 상수) 100 :\t1초에 100번 인터럽트 발생 → 1 tick = 10ms 250\t: 1초에 250번 인터럽트 → 1 tick = 4ms 1000 : 1초에 1000번 인터럽트 → 1 tick = 1ms 즉, HZ가 높을수록 load average 갱신 주기가 더 세밀해지고 정확해짐 HZ가 100인 시스템 기준으로는 LOAD_FREQ ≈ 501 jiffies ≈ 5.01초 참고 https://www.brendangregg.com/blog/2017-08-08/linux-load-averages.html" }, { "title": "iframe이 열리지 않는 이슈 (ancestor violates the following Content Security Policy directive)", "url": "/posts/csp-iframe-issue/", "categories": "경험하기, 이슈 노트", "tags": "HTTP", "date": "2025-03-15 22:25:00 +0900", "snippet": "상황 기존에는 http://alpha.domain.com 부모창에서 http://alpha-foo.domain.com iframe을 사용하는 시스템에서, 특정 작업으로 인해 http://alpha.domain.com 부모창에서 https://test.alpha-foo.domain.com iframe을 사용하게 되었는데, 콘솔에 다음과 같은 에러 메...", "content": "상황 기존에는 http://alpha.domain.com 부모창에서 http://alpha-foo.domain.com iframe을 사용하는 시스템에서, 특정 작업으로 인해 http://alpha.domain.com 부모창에서 https://test.alpha-foo.domain.com iframe을 사용하게 되었는데, 콘솔에 다음과 같은 에러 메세지가 찍히면서 화면이 뜨지 않았다. http://alpha.domain.com은 타시스템이고 iframe으로 제공하는 화면이 내가 관리하는 시스템이다.Refused to frame 'https://test.alpha-foo.domain.com' because an ancestor violates the following Content Security Policy directive: \"frame-ancestors 'self' *.domain.com\".내가 관리하는 시스템 Nginx 설정 add_header Content-Security-Policy \"frame-ancestors 'self' *.domain.com\";원인 파악 CSP 관련해서 정리하면서 알게되었고, 원인은 굉장히 간단했다. 앞서 세팅된 설정에 보면 *.domain.com으로 되어있는데, 스키마가 생략되면 CSP가 적용된 출처(origin)의 스키마를 따라가게된다. 즉, https://test.alpha-foo.domain.com iframe(origin)을 띄우려면 부모창(frame-ancestors)도 반드시 https://*.domain.com의 형식이어야 했던것이다. 해결 방안1. 설정 변경 add_header Content-Security-Policy \"frame-ancestors 'self' http://*.domain.com\"; http로 명시했더라도, “보안 단계가 더 높은 스키마로 업그레이드된 요청은 자동으로 허용”되기 때문에 https도 허용됨 2. 부모창에서 https 사용" }, { "title": "WEB - Content-Security-Policy(CSP) 살펴보기", "url": "/posts/content-security-policy/", "categories": "지식 더하기, 이론", "tags": "WEB", "date": "2025-03-15 22:25:00 +0900", "snippet": "Content-Security-Policy ? 콘텐츠 보안 정책(CSP)은 특정 유형의 보안 위협의 위험을 방지하거나 최소화하는 데 도움이 되는 기능으로 웹사이트에서 브라우저로 보내는 일련의 지침으로 구성되어 있으며, 브라우저에서 코드가 수행할 수 있는 작업에 제한을 두도록 지시CSP 전달 방식 HTTP 응답 헤더 메인 문서뿐만 아...", "content": "Content-Security-Policy ? 콘텐츠 보안 정책(CSP)은 특정 유형의 보안 위협의 위험을 방지하거나 최소화하는 데 도움이 되는 기능으로 웹사이트에서 브라우저로 보내는 일련의 지침으로 구성되어 있으며, 브라우저에서 코드가 수행할 수 있는 작업에 제한을 두도록 지시CSP 전달 방식 HTTP 응답 헤더 메인 문서뿐만 아니라 모든 요청에 대한 응답에 적용되어야 한다. &lt;meta&gt; 태그의 http-equiv 속성으로 정의정책 정책은 세미콜론으로 구분된 일련의 지시문(Fetch directives)으로 구성 각 지시문은 보안 정책의 다른 측면을 제어예시 Content-Security-Policy: default-src 'self'; img-src 'self' example.com 지시문1 : default-src 'self' 다른 리소스 유형에 대해 다른 정책을 설정하는 구체적인 지시어가 없는 한, 문서와 동일한 출처의 리소스만 로드하라고 브라우저에 지시 지시문2 : img-src 'self' example.com 브라우저에 동일한 출처의 이미지나 example.com에서 제공된 이미지를 로드하라고 지시 Fetch directives 리소스(JavaScript, CSS, 이미지 등)에 대한 허용된 범주(도메인, URL, 프로토콜, 또는 패턴 등)를 지정 예를 들어, script-src는 JavaScript 로드를 허용하는 범주를 설정 전체 지시어 목록은 여기에서 확인할 수 있다. default-src : 지시문이 명시적으로 나열되지 않은 모든 리소스에 대한 대체 정책을 설정 각 Fetch directives는 단일 키워드 'none' 또는 공백으로 구분된 하나 이상의 출처 표현식(source expressions)으로 지정 none : 해당 리소스 로드 자체를 차단 self : 문서 자체와 동일한 출처(origin)의 리소스만 허용 두 개 이상의 출처 표현식이 나열된 경우 하나라도 리소스를 허용하면 리소스가 허용됨 하지만, 예외적으로 none이 같이 있으면 다른 출처 표현식들은 무시됨 예시 Content-Security-Policy: default-src 'self'; img-src 'self' example.com default-src는 단일 출처 표현식 self로 구성. img-src는 두 개의 출처 표현식 self와 example.com으로 구성※ Fetch ? fetch는 일반적으로 영어에서 “가져오다” 또는 “불러오다”라는 의미 fetch는 브라우저가 외부 리소스를 요청하거나 가져오는 행위 즉, Fetch directives란 웹 문서가 불러올 수 있는 리소스의 종류나 출처를 제한하는 지시어출처 표현식 형식 살펴보기 &lt;host-source&gt; 및 &lt;scheme-source&gt; 형식은 따옴표로 묶어서는 안되며, 다른 모든 형식은 작은 따옴표로 묶어야 한다.&lt;host-source&gt; 해당 리소스를 허용할 출처의 URL 또는 호스트 IP 주소 스키마(예: http, https), 포트 번호, 경로(path)는 생략할 수 있다. origin = &lt;scheme&gt; \"://\" &lt;host&gt; [ \":\" &lt;port&gt; ] 스키마를 생략하면, 문서의 출처(origin)에서 사용된 스키마가 자동으로 적용된다. 예를 들어, iframe에서 Content-Security-Policy: frame-ancestors *.example.com으로 설정했고 iframe의 url 스키마가 https라면 부모창도 반드시 https 여야한다. CSP(Content-Security-Policy)에서 스키마를 지정하면 원칙적으로는 정확한 일치가 요구되지만, 보안 단계가 더 높은 스키마로 업그레이드된 요청은 자동으로 허용된다. http://example.com은 https://example.com의 리소스도 허용 ws://example.com은 wss://example.org의 리소스도 허용 와일드카드(*)는 서브도메인, 호스트 주소 및 포트 번호에서 사용할 수 있으며, 각 항목에서 가능한 모든 유효한 값이 허용됨을 의미한다. http://*.example.com은 example.com의 모든 서브도메인으로부터 오는 리소스를 HTTP 또는 HTTPS로 허용 경로가 /로 끝나는 경우, 그 경로로 시작하는 모든 하위 경로를 매칭한다. example.com/api/는 다음과 같은 경로 모두 허용: example.com/api/users example.com/api/users/list example.com/api/orders/123 만약 경로가 /로 끝나지 않는다면, 정확히 그 경로 하나만 매칭한다. example.com/api : example.com/api만 허용 &lt;scheme-source&gt; https:와 같은 scheme을 사용할 수 있으며, 콜론(:)이 필수적이다 보안 업그레이드가 허용됨 http를 허용하면 https로 로드된 리소스도 허용 ws를 허용하면 wss로 로드된 리소스도 허용 upgrade-insecure-requests 지시어 사이트는 때때로 주요 문서를 HTTPS를 통해 제공하지만 리소스는 HTTP를 통해 제공한다.&lt;script src=\"http://example.org/my-cat.js\"&gt;&lt;/script&gt; 이를 mixed content라고 하며, 안전하지 않은 리소스가 있으면 HTTPS에서 제공하는 보호가 크게 약화된다. 브라우저가 구현하는 mixed content 알고리즘에 따라 문서가 HTTPS를 통해 제공되는 경우 안전하지 않은 리소스는 “업그레이드 가능한 콘텐츠”와 “차단 가능한 콘텐츠”로 분류된다. mixed content에 대한 궁극적인 해결책은 개발자가 모든 리소스를 HTTPS를 통해 로드하는 것입니다. 그러나 사이트가 모든 콘텐츠를 HTTPS를 통해 제공할 수 있더라도 개발자가 리소스를 로드하는 데 사용하는 모든 URL을 다시 작성하는 것은 여전히 매우 어려움 (아카이브된 콘텐츠와 관련된 경우 사실상 불가능할 수도 있음). upgrade-insecure-requests 지시문은 이 문제를 해결하기 위해 사용된다. Content-Security-Policy: upgrade-insecure-requests 이 지시문이 문서에 설정된 경우 브라우저는 다음과 같은 경우 모든 HTTP URL을 HTTPS로 자동 업그레이드한다. 리소스(예: 이미지, 스크립트 등)를 로드하는 요청 문서와 동일한 출처인 탐색 요청(예: 링크 대상) 중첩된 브라우징 컨텍스트의 탐색 요청(예: iframe) form 제출 하지만, 대상이 다른 출처인 최상위 탐색(top-level navigation) 요청은 업그레이드되지 않습니다. Top-level navigation은 브라우저가 현재 페이지에서 다른 페이지로 직접 이동하는 요청을 의미 &lt;a href=\"...\"&gt; 링크를 클릭해서 다른 페이지로 이동하거나, 주소창에 URL을 직접 입력해서 이동하거나, 브라우저에서 페이지를 새로고침하거나 다른 웹사이트로 직접 이동하는 등 예를 들어, https://example.org의 문서가 upgrade-insecure-requests 지시어를 포함하는 CSP와 함께 제공되고 해당 문서에 다음과 같은 마크업이 포함되어 있다고 가정했을 때, 브라우저는 두 요청을 모두 자동으로 HTTPS로 업그레이드한다. &lt;script src=\"http://example.org/my-cat.js\"&gt;&lt;/script&gt;&lt;script src=\"http://not-example.org/another-cat.js\"&gt;&lt;/script&gt; 하지만, 다음의 경우 브라우저는 첫 번째 링크는 HTTPS로 업그레이드하지만, 두 번째 링크는 다른 출처로 이동하기 때문에 업그레이드하지 않는다. &lt;a href=\"http://example.org/more-cats\"&gt;See some more cats!&lt;/a&gt;&lt;a href=\"http://not-example.org/even-more-cats\"&gt;More cats, on another site!&lt;/a&gt; CSP로 악의적인 공격 막는 예시외부 스크립트 주입 예시 : &lt;script src=\"https://evil.example.com/hacker.js\"&gt;&lt;/script&gt; 방어 : Content-Security-Policy: script-src 'self'; 현재 도메인의 스크립트만 허용 인라인 JavaScript 차단 예시 &lt;script&gt; console.log(\"You've been hacked!\");&lt;/script&gt; 방어 Content-Security-Policy: script-src 'self'; &lt;script&gt; 내부의 인라인 JavaScript 실행 차단 ('unsafe-inline’ 키워드가 없으면 인라인 JavaScript 실행 차단됨) 이벤트 핸들러 차단 예시 : &lt;img onmouseover=\"console.log(You've been hacked!)\" /&gt; 방어 : Content-Security-Policy: script-src 'self'; 위와 마찬가지로 'unsafe-inline'을 제거하면 onmouseover, onclick 같은 인라인 이벤트 핸들러 차단 참고 자료 https://developer.mozilla.org/en-US/docs/Web/HTTP/CSP https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Security-Policy/frame-ancestors" }, { "title": "Hello 2025", "url": "/posts/hello2025/", "categories": "끄적끄적", "tags": "", "date": "2025-02-28 22:00:00 +0900", "snippet": "지난 시간들을 되돌아보며 다짐해본다.감사하자.겸손하자.꾸준하자.넓게보자.진짜가 되자.", "content": "지난 시간들을 되돌아보며 다짐해본다.감사하자.겸손하자.꾸준하자.넓게보자.진짜가 되자." }, { "title": "Java - try-with-resources 알아보기", "url": "/posts/try-with-resources/", "categories": "지식 더하기, 이론", "tags": "Java", "date": "2025-02-23 18:29:00 +0900", "snippet": "try-with-resources란 ? 자바7부터 등장한 구문으로 try 블록이 종료될 때 AutoClosable 또는 Closable 인터페이스를 구현한 리소스에 대해 (JVM이) 자동으로 close()를 호출한다.기존(try-finally) 방식static String readFirstLineFromFileWithFinallyBlock(Strin...", "content": "try-with-resources란 ? 자바7부터 등장한 구문으로 try 블록이 종료될 때 AutoClosable 또는 Closable 인터페이스를 구현한 리소스에 대해 (JVM이) 자동으로 close()를 호출한다.기존(try-finally) 방식static String readFirstLineFromFileWithFinallyBlock(String path) throws IOException { FileReader fr = new FileReader(path); BufferedReader br = new BufferedReader(fr); try { return br.readLine(); } finally { br.close(); fr.close(); }} 리소스 해제를 위해서는 직접 close()를 호출해야한다. 만약 br.close()에서 예외가 발생하면, fr.close()가 실행되지 않을 수 있음. (자원 누수 발생 가능)try-with-resources 방식static String readFirstLineFromFileWithFinallyBlock(String path) throws IOException { try (FileReader fr = new FileReader(path); BufferedReader br = new BufferedReader(fr) ) { return br.readLine(); }}※ 자바 9부터는 이미 선언된 리소스도 try-with-resources에서 사용할 수 있다.static String readFirstLineFromFileWithFinallyBlock(String path) throws IOException { FileReader fr = new FileReader(path); BufferedReader br = new BufferedReader(fr); try (fr; br) { return br.readLine(); }}예외 발생 시try (MyResource resource = new MyResource()) { throw new Exception(\"Exception in try block\");} catch (Exception e) { System.out.println(\"Main Exception: \" + e.getMessage()); for (Throwable suppressed : e.getSuppressed()) { System.out.println(\"Suppressed Exception: \" + suppressed.getMessage()); }}Main Exception: Exception in try blockSuppressed Exception: Exception in close() try 블록에서 예외 발생 예외가 발생하더라도 close()는 자동으로 호출됨 try 블록과 close()에서 모두 예외 발생 try 블록에서 발생한 예외가 주된 예외(primary exception)가 되고 close()에서 발생한 예외는 억제된 예외(suppressed exception)가 됨 ※ Suppressed Exception ? 위에서 살펴보았듯이 try-with-resources에서는 두 개 이상의 예외가 발생할 수 있음 이런 경우 예외가 덮어씌워져 실제 원인(주 예외)이 사라질 수 있고, 이로 인해 디버깅이 어려워질 수 있음 따라서, try 블록에서 발생한 예외를 유지하고, close() 중 발생한 예외는 억제(Suppressed)함 자바 7에서 Throwable에 추가된 getSuppressed 메서드를 이용해서 억제된 예외를 가져올 수 있음자원 해제 순서 닫히는 순서는 생성 순서의 반대 (LIFO, 후입선출) 즉, writer.close() → zf.close() 순서로 실행됨 try ( java.util.zip.ZipFile zf = new java.util.zip.ZipFile(zipFileName); java.io.BufferedWriter writer = java.nio.file.Files.newBufferedWriter(outputFilePath, charset)) { for (java.util.Enumeration entries = zf.entries(); entries.hasMoreElements();) { String zipEntryName = ((java.util.zip.ZipEntry) entries.nextElement()).getName(); writer.write(zipEntryName); }}Closable vs AutoClosable Java 7에서 try-with-resources를 추가하면서 기존 Closeable(Java 5에서 등장)을 수정하면 하위 호환성이 깨질 수 있기 때문에 AutoCloseable을 새로 만들고, Closeable이 이를 확장하는 방식으로 해결.public interface AutoCloseable { void close() throws Exception;}public interface Closeable extends AutoCloseable { public void close() throws IOException;}예외 처리 관점 Closeable은 IO 스트림 전용으로 설계되었으며, IOException을 던진다. AutoCloseable은 일반 Exception을 던질 수 있음 즉, AutoCloseable은 일반적인 자원 관리에 더 넓게 적용될 수 있음. 하지만, 구체적인 예외(IOException, SQLException 등)를 던지는 것이 권장됨. 만약 close()에서 예외가 발생할 가능성이 없다면, 예외를 던지지 않는 것이 더 바람직하다. AutoCloseable 구현시 close()에서 예외가 발생할 가능성이 있는 경우, 반드시 리소스를 먼저 해제해야 한다. 만약 close()에서 예외가 발생할 수 있다면, 리소스를 먼저 해제하고 예외를 던지는 것이 중요하다. 예외가 발생해도 리소스가 해제되지 않으면, 자원 누수(resource leak)가 발생할 수 있음. 멱등성 관점 Closeable은 idempotent(멱등성) 을 보장 예를 들어, FileInputStream.close()를 여러 번 호출해도 추가적인 예외 없이 안전하게 종료됨. AutoCloseable은 이러한 보장을 제공하지는 않지만, 리소스를 여러 번 닫더라도 문제가 발생하지 않도록 설계하는 것이 중요하다. 즉, close()는 일반적으로 한 번만 호출되지만, 여러 번 호출될 가능성도 고려해야 한다. 참고 자료 https://stackoverflow.com/questions/13141302/implements-closeable-or-implements-autocloseable https://docs.oracle.com/javase/tutorial/essential/exceptions/tryResourceClose.html https://docs.oracle.com/javase/8/docs/api/java/lang/AutoCloseable.html" }, { "title": "HashMap의 put 메서드 들여다보기", "url": "/posts/java-hashmap-put/", "categories": "지식 더하기, 들여다보기", "tags": "Java", "date": "2025-02-21 22:25:00 +0900", "snippet": " 자바 17 기준, GPT 선생님과 함께 공부해보았다.static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { final int hash; final K key; V value; Node&lt;K,V&gt; next; Node(int hash, K key, V valu...", "content": " 자바 17 기준, GPT 선생님과 함께 공부해보았다.static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { final int hash; final K key; V value; Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) { this.hash = hash; this.key = key; this.value = value; this.next = next; } public final K getKey() { return key; } public final V getValue() { return value; } public final String toString() { return key + \"=\" + value; } public final int hashCode() { return Objects.hashCode(key) ^ Objects.hashCode(value); } public final V setValue(V newValue) { V oldValue = value; value = newValue; return oldValue; } public final boolean equals(Object o) { if (o == this) return true; return o instanceof Map.Entry&lt;?, ?&gt; e &amp;&amp; Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue()); }}transient Node&lt;K,V&gt;[] table; // (0)public V put(K key, V value) { return putVal(hash(key), key, value, false, true);}final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // (1) if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // (2) if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); // (3) else { Node&lt;K,V&gt; e; K k; // (3-1) if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // (3-2) else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // (3-3) else { for (int binCount = 0; ; ++binCount) { if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; } if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; } } // (4) if (e != null) { V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; } } // (5) ++modCount; if (++size &gt; threshold) resize(); // (6) afterNodeInsertion(evict); return null;}(0) Node&lt;K,V&gt;[] tabletransient Node&lt;K,V&gt;[] table; HashMap은 Node객체를 담고있는 배열로 구현된다.(1) 테이블 크기 확인 및 초기화if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; table이 아직 생성되지 않았거나, 길이가 0이면 resize()를 호출하여 초기화한다. 해시 테이블(table)이 존재하지 않으면 기본 크기(DEFAULT_INITIAL_CAPACITY = 16)로 초기화된다.(2) 해시 충돌 없이 바로 저장 가능한 경우if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); (n - 1) &amp; hash 연산을 수행하여 해시 값을 배열 인덱스로 변환한다. (n - 1) &amp; hash는 hash % n과 동일한 역할을 하면서 성능을 최적화한다. 해당 위치(tab[i])가 null이면 새로운 노드를 생성하여 저장한다.(3) 해시 충돌한 경우 즉, tab[i = (n - 1) &amp; hash]가 null이 아닌 경우(3-1) 이미 존재하는 키인 경우if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; p.hash == hash이고 p.key.equals(key)라는건 이미 존재하는 키 (4)에서 기존 값을 현재 값으로 덮어쓴다. (e.value = value)(3-2) 트리로 저장해야 하는 경우else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); 기존 노드 p가 트리 구조(TreeNode)로 저장된 경우 트리에 값을 추가한다. HashMap은 특정 조건(TREEIFY_THRESHOLD = 8)에서 연결 리스트 대신 레드-블랙 트리를 사용한다. (3-3에서 살펴봄) (3-3) 연결 리스트로 저장해야 하는 경우for (int binCount = 0; ; ++binCount) { if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; } if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e;} 다음 노드를 가리키는 p.next를 통해 연결 리스트를 순회 동일한 키를 찾았다면 (4)에서 기존 값을 현재 값으로 덮어쓴다. 동일한 키를 찾지 못하면 연결 리스트의 마지막 노드의 다음(p.next)에 새로운 노드 추가 노드 추가시 연결 리스트의 크기가 8이 된다면 TreeNode 구조로 변경 (4) 기존 키가 존재하는 경우 값 덮어쓰기if (e != null) { // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue;} 기존 키가 존재하면(e가 null이 아님) 값을 덮어씌운다. onlyIfAbsent가 true이면 기존 값이 없을 때만 변경. afterNodeAccess(e)는 LRU 캐시 같은 구조에서 사용될 수 있다.(5) HashMap 크기 증가 및 리사이징++modCount;if (++size &gt; threshold) resize(); modCount는 구조적 변경(삽입/삭제 등) 횟수를 의미하며, 반복문에서 변경 감지를 위해 사용된다. size(현재 요소 개수)가 threshold(용량 * loadFactor)보다 크면 해시 테이블을 두 배로 확장한다.(6) 후처리 및 반환afterNodeInsertion(evict);return null; afterNodeInsertion(evict)는 LRU 캐시 같은 경우 특정 조건에서 노드를 제거할 때 사용된다. 새 값을 추가한 경우 null을 반환한다.궁금한 부분1. HashMap의 연결 리스트와 LinkedList 비교 비교 항목 HashMap의 연결 리스트 LinkedList (Java java.util.LinkedList) 목적 해시 충돌 해결 (체이닝) 일반적인 자료구조 활용 사용 방식 Node&lt;K,V&gt;의 next 필드를 사용하여 직접 연결 리스트 구현 Node&lt;E&gt; 클래스를 내부적으로 사용 이중 연결 리스트 여부 X O 2. binCount, treeifyBin에서 쓰는 bin이란 ? 3-3 코드for (int binCount = 0; ; ++binCount) { if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; } ...slot, bin, bucket 용어 정리slot slot은 일반적인 해시 테이블 개념에서 해시 테이블의 인덱스 하나를 의미 예를 들어, table[3]에 값이 저장되었다면 3번 슬롯(slot)에 저장된것bin(=bucket) HashMap을 설명할 때 보통 bucket이라는 단어를 많이 사용 table[index]에 여러 개의 값이 저장될 경우, 이 공간을 bucket(버킷)이라고 부른다. 즉, bin(bucket)은 같은 해시값을 가진 노드들이 저장되는 공간 HashMap 내부 (배열 + 연결 리스트)Index (Slot) Bucket (bin) 내용-----------------------------------table[0] → nulltable[1] → (5, \"X\") -&gt; (15, \"Y\") -&gt; nulltable[2] → (10, \"A\") -&gt; (20, \"B\") -&gt; nulltable[3] → (30, \"C\") -&gt; nulltable[4] → null버킷 충돌 ? 엄밀히 말하면 “슬롯이 충돌한다”는 표현이 더 정확하지만, “버킷에 충돌이 발생한다”는 표현이 일반적으로 많이 사용됨" }, { "title": "API 호출시 타임아웃이 발생하면 ? (caller 관점에서)", "url": "/posts/response-timeout(caller)/", "categories": "생각해보기, what-if", "tags": "", "date": "2025-01-25 00:29:00 +0900", "snippet": "상황 가정 사용자 요청을 처리할 때 외부 API 호출이 필요하고, 해당 API(api/foo) 호출에 대해 타임아웃이 발생 (client ---&gt; 내 서버 ---X 외부 서버) 생각해볼 수 있는 다양한 타임아웃 상황 외부 서버에서 api/foo에 대한 처리가 늦어짐 api/foo 요청이 외부 서버에 늦게 도착 ...", "content": "상황 가정 사용자 요청을 처리할 때 외부 API 호출이 필요하고, 해당 API(api/foo) 호출에 대해 타임아웃이 발생 (client ---&gt; 내 서버 ---X 외부 서버) 생각해볼 수 있는 다양한 타임아웃 상황 외부 서버에서 api/foo에 대한 처리가 늦어짐 api/foo 요청이 외부 서버에 늦게 도착 api/foo에 대한 응답이 내 서버에 도착하지 않거나 늦게 도착함 기타 등등… 가장 심플한 케이스 api/foo에 대한 응답 없이 사용자 요청 처리가 가능한 경우 특정 데이터는 빈 상태로 조회 응답 내려줄 수 있는 상황 또는, 정책적으로 협의가 되어서 api/foo에 대한 재시도가 필요하지 않은 상황조금 더 생각이 필요한 케이스 요청을 처리하려면 api/foo에 대한 응답이 반드시 필요한 경우 (즉, 재시도 필요)api/foo가 멱등성이 보장되는 API라면 ? 사용자 요청이 처리돼야하는 전체 시간을 고려하여 바로 재시도하거나 또는 사용자에게 “잠시 후 재시도해주세요” 등의 응답을 내려준다.api/foo가 멱등성이 보장되지 않는 API라면 ? 외부 서버와 협의한 키 값을 통해 api/foo의 처리 상태를 조회할 수 있는 API(api/bar) 호출 즉, 사용자 요청을 처리하는 로직에서 api/foo 호출 전 api/bar를 호출하는 로직이 있어야됨 api/bar의 처리 상태 확인 후 실패이면 재시도, 성공이면 사용자 요청에 대한 나머지 로직 처리 ‘실패’인 경우 재시도 가능한 경우, 재시도 가능하지 않은 경우까지 구분이 가능하다면 이 부분도 협의하면 좋을 것으로 생각 만약 api/bar로 조회한 상태가 ‘처리중’이면 ? 사용자에게 “처리가 지연되고 있습니다. 잠시만 기다려주세요.” 등의 응답을 내려준다. 만약 계속 ‘처리중’인 상황이 지속되는 경우, 외부 서버와 협의된 시간이 지나면 재시도를 한다던지 외부사에 문의한다던지 등의 정책이 필요할 것으로 생각 만약 api/bar로 조회했는데 값이 없다면 ? api/foo 요청이 유실됐거나, api/bar로 조회한 시점보다 늦게 외부 서버에 도착한 경우 위에서 살펴본 모든 상황에서 협의가 필요하겠지만, 이 케이스는 반드시 필요하다고 생각 협의를 한다면 어떤 부분에 대해 할 수 있을까 ? 재시도했을때 멱등성이 보장되도록 결과 없음에 대한 요청을 따로 보관 (특정 시간이 경과하면) 운영팀에서 해당 업체와 연락해서 확인 기타 등등.. 관련 글 찾아보기1. zalando 기술 블로그타임아웃 값 설정 관련 라이브러리들 중 비효율적으로 긴 타임아웃이 기본값으로 설정된 경우들도 있다 타임아웃을 최대값(p99.9)으로 설정 타임아웃을 최대한 길게 설정하여 거의 모든 요청이 응답을 받을 수 있도록 함. 결과적으로 False Timeout Rate를 0.1%로 유지. 타임아웃을 짧게 설정하고 재시도를 활성화 타임아웃을 p99.9보다 낮게 설정해 빠르게 실패를 감지하고, 필요한 경우 요청을 재시도. 단, 짧은 타임아웃은 실패를 더 자주 발생시킬 수 있으므로 재시도 정책을 신중히 설계해야 함. 재시도 관련 멱등성이 보장되는 API는 안전하게 재시도 가능 그렇지 않은 API는 재시도하는 경우 데이터 일관성이 깨지는 등의 이슈가 있을 수 있기 때문에 Idempotency-Key 헤더 등을 이용해서 멱등성을 보장하는 방식으로 작업하는 것을 추천 서킷 브레이커를 사용해서 재시도가 너무 많은 부하를 유발하는 것을 방지 Exponential backoff (지수적 재시도) 방식을 통해 재시도 간격을 늘린다. 재시도하기 적합한 상황 p50과 p99 사이의 차이가 큼 즉, 대부분의 요청은 정상적인 속도로 응답하지만, 일부 요청이 비정상적으로 지연되는 경우. 네트워크 일시적 지연 등일 확률이 높기 때문에 다음 요청은 정상 속도로 처리될 가능성이 높음 재시도하기 적합하지 않은 상황 p50과 p99가 비슷한 경우 즉, 특정 요청만 느린 게 아니라, 전체적으로 느린 구조적인 문제가 존재할 확률이 높음 재시도보다는 타임아웃 값을 최적화하거나, 시스템 자체를 튜닝하는 것이 더 효과적일 수 있음 2. AWS글 실패 확률을 줄이도록 시스템을 설계하지만 결코 실패하지 않는 시스템을 구축하는 것은 불가능 Amazon에서는 실패 확률을 허용하고 줄이고, 작은 비율의 실패가 완전한 중단으로 확대되는 것을 방지하도록 시스템을 설계 타임아웃을 정하는 좋은 기준은 p99.9와 같은 지표들이 있다. 대부분의 경우에는 잘 작동하지만 몇 가지 함정이 있다. p99.9와 p50의 지연 값이 비슷한 서비스에서는 적합하지 않음어떤 서비스는 지연 값의 분포가 매우 빡빡해서 p99.9 값이 p50(평균적인 지연) 값에 근접한 경우가 있습니다. 이런 경우, 타임아웃 값에 약간의 여유(padding)를 더해서, 작은 지연 증가로 인해 대량의 타임아웃이 발생하는 것을 방지합니다. 타임아웃에 대한 재시도시 생각해볼 것들 일시적인 에러라면 재시도해서 성공할 수도 있겠지만 그게 아니라 부하가 많이 몰린 상황이라면 재시도 요청들이 부하를 가중시켜 상황을 더 악화시킬 수 있다. 여러 마이크로 서비스를 거치는 요청이라면, DB에 부하가 더 많이 갈 수 있다. 재시도 할만한 가치가 있는지 (4xx 에러, 5xx 에러 등) 실패한 모든 요청이 같은 시간에 백오프 후 재시도를 하면, 다시 한 번 과부하나 경쟁이 발생할 수 있음 지터는 백오프에 일정한 랜덤성(randomness)을 추가하여 재시도가 시간적으로 분산되도록 만드는 방법입니다. 이를 통해 요청이 특정 시간에 몰리지 않게 되어 과부하 문제를 완화할 수 있습니다. 더 나아가, 지터는 재시도뿐 아니라 모든 주기적인 작업(timers, periodic jobs 등)에도 적용할 수 있습니다. 3. AWS글재시도시 중복 처리를 방지하는 방법 Amazon은 API 계약에 고유한 클라이언트 요청 식별자를 포함시키는 방식을 선호 이미 생성된 리소스에 대해 재시도 하는 경우 서비스는 “리소스가 이미 존재한다(ResourceAlreadyExists)”는 응답을 반환할 수 있다. 이런 응답은 기본적으로 멱등성을 만족하지만, 클라이언트에게 혼란을 줄 수 있다. 즉, 이번 요청으로 인해 리소스가 생성된 것인지, 이전 요청의 결과로 생성된 것인지 알 수 없음 따라서 클라이언트는 “리소스가 이미 존재하는” 상황을 별도로 처리해야 하며, 이는 클라이언트 코드의 복잡성을 증가시킴 멱등성 설계 방식 요청의 파라미터를 해시(hash)로 변환하여 중복 여부를 확인하는 방법이 있음. 하지만, 이 방법이 항상 유효하지 않음. 예: EC2 인스턴스 생성 요청이라면? → 동일한 요청이더라도, 사용자는 실제로 두 개의 EC2 인스턴스를 원할 수도 있음. 따라서, 각 API 요청에 고유한 요청 식별자(Client Token)를 추가하도록 설계. 즉, 동일한 Client Token을 가진 요청은 동일한 작업으로 간주. 요청을 처음 받을 때 Client Token을 기록 작업이 원자적으로 실행되도록 보장 동일한 Client Token을 가진 요청이 다시 들어오면, 같은 응답을 반환 분산 시스템에서 요청이 지연되어 도착하는 경우, 멱등성 처리 과정에서 복잡한 상황이 발생할 수 있음 예를 들어 : 첫 번째 요청이 리소스를 생성함. 두 번째 요청이 재시도되기 전에 누군가 리소스를 삭제함. 재시도된 요청이 도착했을 때, 서비스는 어떻게 반응해야 할까? AWS의 해결책 Amazon EC2 RunInstances의 경우, 이러한 상황에서도 최초의 멱등성 계약(idempotent contract)을 존중 즉, 삭제된 리소스의 상태를 반영하되, 의미적으로 동일한 응답을 반환함으로써 고객이 혼란스러워하지 않도록 합니다. Client Token 저장 이슈 너무 오래 보관하면 이전 요청과의 식별자 충돌 문제가 발생할 가능성이 있음. 서비스 및 리소스 유형에 따라 멱등성 정보 유지 기간이 다를 수 있음. EC2 인스턴스의 경우, 정보 유지 기간을 해당 리소스의 생애 주기(lifetime) + 추가적인 일정 기간으로 제한. 이렇게 하면 늦게 도착하는 요청(재시도 요청 포함)이 도착할 가능성이 있는 기간까지만 정보를 유지하고, 이후에는 제거하여 불필요한 충돌을 방지함. " }, { "title": "DB 롤체인지 후 애플리케이션 헬스체크가 정상적으로 되지 않은 이슈 (feat. DB 커넥션 풀)", "url": "/posts/after-db-role-change-issue/", "categories": "경험하기, 이슈 노트", "tags": "DBCP", "date": "2024-10-14 22:25:00 +0900", "snippet": "상황 L4/L7 스위치 --- WEB --- WAS(애플리케이션)로 구성된 환경에서 2~3초 간격으로 L4/L7 스위치에서 헬스체크 신호를 보냄 애플리케이션은 헬스체크를 위해 DB에 SELECT 1 질의하게 되어있음 인프라팀 작업 이슈로 마스터 DB 서버 메모리 점유율이 너무 높아져 DB가 롤체인지 됨(약 12초 정도 소요) 롤체인지 되었는데도...", "content": "상황 L4/L7 스위치 --- WEB --- WAS(애플리케이션)로 구성된 환경에서 2~3초 간격으로 L4/L7 스위치에서 헬스체크 신호를 보냄 애플리케이션은 헬스체크를 위해 DB에 SELECT 1 질의하게 되어있음 인프라팀 작업 이슈로 마스터 DB 서버 메모리 점유율이 너무 높아져 DB가 롤체인지 됨(약 12초 정도 소요) 롤체인지 되었는데도 애플리케이션 헬스체크는 down 상태 (DB 연결이 계속 안됨) 애플리케이션 재시작하니까 다시 정상 동작 이후에 DBA가 롤체인지 했을때 약 2초 정도 소요되었고, 헬스체크 잠깐 down 되었다가 다시 잘 올라옴“왜 롤체인지에 12초가 걸렸을 때는 DB가 정상화되었는데도 연결이 안됐고, 2초가 걸렸을 때는 다시 연결이 잘 됐을까 ?”에러 로그 살펴보기롤체인지 12초 1, 2, 3 순서대로 나타난 후, 3번이 계속 반복됨1. com.mysql.cj.exceptions.CJCommunicationsException: Communications link failure2. java.sql.SQLNonTransientConnectionException: Could not create connection to database server. Attempted reconnect 3 times. Giving up.3. java.sql.SQLNonTransientConnectionException: No operations allowed after connection closed.롤체인지 2초 com.mysql.cj.jdbc.exceptions.CommunicationsException: Communications link failuredb 관련 설정 살펴보기 db 접속 정보 autoReconnect가 사용되고 있음 jdbc:mysql://{ip}:{port}/{db}?autoReconnect=true&amp;useLocalSessionState=true 커넥션 풀(톰캣 DBCP) eviction 스레드 관련 설정dataSource.setTestOnBorrow(false);dataSource.setTestOnReturn(false);dataSource.setTestWhileIdle(false);원인 파악하기 디버깅한 것을 바탕으로 간단하게 나타낸 그림 (틀릴 수 있음)롤체인지 12초 때의 애플리케이션 실행 흐름 하나의 DB 커넥션 관점에서의 흐름 L4에서 헬스체크 요청 들어옴 -&gt; DB에 질의 요청 질의 실패 ( autoReconnect true이면, 다음 질의 때 질의 날리기 전 먼저 ping 검사하라는 변수(needsPing)를 true로 만듦) Communications link failure 예외 발생 (health check down) L4에서 헬스체크 요청 들어옴 -&gt; DB에 질의 요청전 먼저 ping 검사 수행 ping 실패시 DB 세션에 다시 연결(reconnect)하는 과정 수행 (먼저 NativeSession close 후) reconnect 최대 재시도 횟수(default : 3)까지 실패 Could not create connection to database server. Attempted reconnect 3 times. Giving up. 예외 발생 L4에서 헬스체크 요청 들어옴 -&gt; DB 질의를 위해 PreparedStatement 만드는 과정에서 커넥션에 매핑된 NativeSession이 닫혀있는지 먼저 확인 5번에서 NativeSession close하고 재연결에도 실패했기 때문에 NativeSession은 닫혀있는 상태이므로, No operations allowed after connection closed. 예외 발생 즉, NativeSession만 체크하고 예외 던지므로 DB가 롤체인지 되어 살아난 것과는 관계없이 질의를 할 수 없게됨 ※ 롤체인지 2초때는 3또는 4에서 성공한 것으로 보임실행 흐름 코드 참고 1~4 : com.mysql.cj.NativeSession#execSQL 5 : com.mysql.cj.jdbc.ConnectionImpl#connectWithRetries com.mysql.cj.LocalizedErrorMessage.properties 6 : com.mysql.cj.jdbc.ConnectionImpl#prepareStatement,com.mysql.cj.NativeSession#checkClosed com.mysql.cj.LocalizedErrorMessage.properties조치 더 이상 유효하지 않은 DB 세션을 물고있는 커넥션을 커넥션 풀에서 정리커넥션 풀 옵션 변경 AS-ISdataSource.setTestOnBorrow(false);dataSource.setTestOnReturn(false);dataSource.setTestWhileIdle(false); TO-BE PoolCleaner(org.apache.tomcat.jdbc.pool.ConnectionPool.PoolCleaner)가 5초(timeBetweenEvictionRunsMillis 기본값)에 한번씩 idle 큐에 있는 커넥션들의 DB 세션 유효성 검사dataSource.setTestOnBorrow(false);dataSource.setTestOnReturn(false);dataSource.setTestWhileIdle(true);dataSource.setValidationQuery(\"SELECT 1\");dataSource.setMinEvictableIdleTimeMillis(-1);헬스체크 방식 변경 DB에 질의하지 않고 애플리케이션이 실행되고 있는지 여부로 판단하도록 (요청 자체가 못들어오는 것보다는 서버 에러라고 응답 주는게 낫다고 생각)" }, { "title": "MySQL에서 4bytes 이모지가 물음표로 저장되는 이슈", "url": "/posts/4bytes-emoji-to-question-marks/", "categories": "경험하기, 이슈 노트", "tags": "MySQL", "date": "2024-06-04 22:25:00 +0900", "snippet": "상황 MySQL DB에 🎃이모지 저장시 ‘????’로 저장되었고, connection properties(jdbc url에 쿼리 스트링으로 붙는 값들)를 변경해가며 테스트 해 본 결과는 다음과 같았다. connection properties 현상 mysql-connector-j 버전 mysql 버전 ...", "content": "상황 MySQL DB에 🎃이모지 저장시 ‘????’로 저장되었고, connection properties(jdbc url에 쿼리 스트링으로 붙는 값들)를 변경해가며 테스트 해 본 결과는 다음과 같았다. connection properties 현상 mysql-connector-j 버전 mysql 버전 characterEncoding=UTF-8 이모지 저장 안됨 (에러 발생) 5.1.12 8.0.35 characterEncoding=UTF-8&amp;connectionCollation=utf8mb4_bin 이모지가 ????로 저장됨 5.1.12 8.0.35 characterEncoding=UTF-8&amp;connectionCollation=utf8mb4_bin 이모지 저장 제대로 됨 8.0.28 8.0.35 ※ 들어가기 전 참고 : 유니코드는 문자를 매핑된 코드 포인트로 관리 출처 : https://www.iemoji.com/view/emoji/256/smileys-people/jack-o-lanternMySQL에 문자가 저장되기까지의 과정 공식 문서 및 chat gpt 답변 참고하여 추측 다음 시스템 변수가 중요 : character_set_client, character_set_connection 애플리케이션(클라이언트)에서 쿼리문 보냄 서버에서는 수신한 쿼리문을 character_set_client에 정의된 방식으로 디코딩하여 유니코드 코드 포인트로 만듦 만들어진 코드 포인트를 character_set_connection에 정의된 방식으로 인코딩하여 최종 값 저장 내용 추가 : 포스팅1과 포스팅2를 보니 character_set_client와 character_set_connection이 동일한 경우에는 변환이 일어나지 않는다고 한다. 즉, 그림에서 아래 흐름은 틀림이모지 저장 실패 흐름 MySQL은 질의문이 최대 3바이트 UTF-8로 인코딩 됐다고 생각(character_set_client=utf8 (utf8mb3)) utf8mb3은 코드 포인트 U+0000 ~ U+FFFF(BMP 문자)의 유니코드 문자만 지원 (최대 3바이트) - 공식문서 참고 하지만, 4바이트 문자는 일반적으로 유니코드 코드 포인트 U+10000 ~ U+10FFFF에 해당 (🎃 –&gt; U+1F383) 따라서, character_set_client=utf8 (utf8mb3)인 경우 최대 3바이트 문자만 처리할 수 있으므로 4바이트 문자를 유니코드 코드 포인트로 디코딩하지 못함 이 실패로 인해 오류가 발생하거나 지원되지 않는 문자가 종종 ????로 대체됨※ 참고 utf8mb3은 deprecated됨 위에서 언급한 공식문서에도 UTF-8 데이터를 사용하지만 supplementary character(이모지 같은 BMP이외의 문자)에 대한 지원이 필요한 애플리케이션은 utf8mb3 대신 utf8mb4를 사용해야 합니다.라고 되어있음상황별 character_set_client, character_set_connection 살펴보기 MySQL 버전은 8.0.35로 동일 connection properties mysql-connector-j 버전 mysql 버전 character_set_client character_set_connection characterEncoding=UTF-8 5.1.12 8.0.35 utf8 utf8 characterEncoding=UTF-8&amp;connectionCollation=utf8mb4_bin 5.1.12 8.0.35 utf8 utf8mb4 별도 세팅 없음 8.0.28 8.0.35 utf8mb4 utf8mb4 characterEncoding=UTF-8 8.0.28 8.0.35 utf8mb4 utf8mb4 characterEncoding=UTF-8&amp;connectionCollation=utf8mb4_bin 8.0.28 8.0.35 utf8mb4 utf8mb4 mysql-connector-j 5.1.12 버전에서는 character_set_client가 utf8 즉, utf8mb3이라 최대 3바이트까지 밖에 지원이 안돼서위 실패 흐름에서 살펴본 것처럼 4바이트 이모지가 제대로 저장되지 않았던 것※ character_set_client, character_set_connection 확인 쿼리SELECT * FROM performance_schema.session_variablesWHERE VARIABLE_NAME IN ('character_set_client', 'character_set_connection');character_set_client과 character_set_connection 값에 영향을 미치는 요소 https://dev.mysql.com/doc/connector-j/en/connector-j-connp-props-session.htmlcharacterEncoding character_set_client 및 character_set_connection을 지정된 Java 문자 인코딩에 대해 MySQL이 지원하는 기본 문자 집합으로 설정 characterEncoding=UTF-8 (java) =&gt; utf8 or utf8mb4 (mysql) characterEncoding=ISO-8859-1 (java) =&gt; latin1 (mysql) collation_connection을 이 문자 집합의 기본 collation으로 설정※ 참고 (문자 집합별 기본 collation 확인) SHOW CHARACTER SETconnectionCollation 세션 시스템 변수 collation_connection을 지정된 collation으로 설정하고 character_set_client 및 character_set_connection을 상응하는 문자 집합으로 설정 이 속성은 characterEncoding이 구성되지 않았거나 collation과 호환되지 않는 문자 집합으로 구성된 경우에만 이 collation이 속한 기본 문자 집합으로 characterEncoding 값을 재정의※ 이 케이스 다시 생각해보기 connection properties mysql-connector-j 버전 mysql 버전 character_set_client character_set_connection characterEncoding=UTF-8&amp;connectionCollation=utf8mb4_bin 5.1.12 8.0.35 utf8 utf8mb4 connectionCollation=utf8mb4_bin에 상응하는 문자 집합이면 character_set_client도 utf8mb4이 되어야하지 않나? connector/j 5.1.13 부터 변경된 부분 (5.1.12 버전까지는) Connector/J는 mysql 서버 5.5.2 이상에 대해 utf8mb4를 지원하지 않았음 5.1.13 버전부터 Connector/J는 character_set_server=utf8mb4로 구성된 서버를 자동 감지 또는, characterEncoding=UTF-8을 사용하여 전달된 Java 인코딩 utf-8을 utf8mb4로 처리 characterEncoding, connectionCollation 둘 다 설정 안되어있는 경우 Connector/J 8.0.25 이하 : 서버의 기본 문자 집합을 사용(character_set_server 값) Connector/J 8.0.26 이상 : utf8mb4 사용" }, { "title": "프록시를 통해 API 호출할 때 지연되는 이슈", "url": "/posts/api-call-through-proxy-issue/", "categories": "경험하기, 이슈 노트", "tags": "", "date": "2024-01-11 22:25:00 +0900", "snippet": "상황 프록시 서버를 통해 API를 호출할 때 응답이 지연되는 케이스가 있었고, tcpdump를 통해 확인해본 결과 프록시 서버에 요청을 했다가 응답이 없으니 직접 요청 서버로 접근하는 것을 확인할 수 있었다. 프록시 서버에서 응답을 못 받은 이유는 네트워크 ACL 신청이 누락되어서였으며 ACL 등록 이후 해당 현상은 나타나지 않았다.의문점 아래 ...", "content": "상황 프록시 서버를 통해 API를 호출할 때 응답이 지연되는 케이스가 있었고, tcpdump를 통해 확인해본 결과 프록시 서버에 요청을 했다가 응답이 없으니 직접 요청 서버로 접근하는 것을 확인할 수 있었다. 프록시 서버에서 응답을 못 받은 이유는 네트워크 ACL 신청이 누락되어서였으며 ACL 등록 이후 해당 현상은 나타나지 않았다.의문점 아래 코드로 로컬에서 재현해봤을때 자바 버전에 따라 결과가 다른 것을 알게되었다. java.net.ConnectException: Connection timed out: connect 발생 (jdk 버전 : jdk8u362-b09) 이슈 상황처럼 지연 응답옴 (jdk 버전 : openjdk 1.8.0-242) public class ProxyTest { public static void main(String[] args) { Scanner sc = new Scanner(System.in); String url = sc.nextLine(); String ipAddr = \"접근되지 않는 서버 ip\"; String port = \"임의의 포트\"; try { Properties systemSettings = System.getProperties(); systemSettings.put(\"http.proxyHost\", ipAddr); systemSettings.put(\"http.proxyPort\", port); systemSettings.put(\"https.proxyHost\", ipAddr); systemSettings.put(\"https.proxyPort\", port); HttpURLConnection con = (HttpURLConnection) new URL(url).openConnection(); System.out.println(con.getResponseCode() + \" : \" + con.getResponseMessage()); } catch (Exception e) { e.printStackTrace(); } }}원인 파악하기 실행 흐름을 확인해보기 위해 지연 응답이 오는 openjdk 1.8.0-242 버전에서 프록시 서버뿐 아니라, 요청 url 서버도 접근이 되지 않도록 해보았다. 좌 : jdk8u362-b09, 우 : openjdk 1.8.0-242 표시한 부분부터 실행흐름이 달라지는걸 볼 수 있었고, 이전 호출을 따라가본 결과 sun.net.www.protocol.http.HttpURLConnection.plainConnect0에서 프록시 서버로 연결 후 예외가 발생했을 때, 구현이 다르게 되어있는 것을 확인할 수 있었다. 좌 : jdk8u362-b09 (프록시 서버로 재요청), 우 : openjdk 1.8.0-242 (프록시 서버 null로 세팅하고 재요청) RestTemplate 사용하면 ? 내부적으로 HttpURLConnection 사용하기 때문에 동일하다.참고 https://bugs.openjdk.org/browse/JDK-8268881 해당 이슈는 버그 리포팅 되었었고 8u311 버전에서 해결된 것으로 보인다." }, { "title": "MyBatis-Spring 트랜잭션 관리", "url": "/posts/how-mybatis-spring-transaction-works/", "categories": "지식 더하기, 들여다보기", "tags": "MyBatis", "date": "2024-01-02 22:25:00 +0900", "snippet": " 해당 포스팅에서 mybatis, mybatis-spring 사용법을 간단히 살펴보았다. 이번에는 개인적으로 가장 궁금했던 mybatis-spring에서 트랜잭션을 다루는 부분이 내부적으로 어떻게 동작하는지 살펴보려고 한다. (예제 코드는 훑어보기 편에서 사용된 코드)들어가기 전 : 예제 코드 다시 살펴보기 SqlSession의 구현체로 SqlSe...", "content": " 해당 포스팅에서 mybatis, mybatis-spring 사용법을 간단히 살펴보았다. 이번에는 개인적으로 가장 궁금했던 mybatis-spring에서 트랜잭션을 다루는 부분이 내부적으로 어떻게 동작하는지 살펴보려고 한다. (예제 코드는 훑어보기 편에서 사용된 코드)들어가기 전 : 예제 코드 다시 살펴보기 SqlSession의 구현체로 SqlSessionTemplate 주입해서 사용하는 경우, sqlSession.insert 호출하면 auto-commit됨public class MybatisSpringMemberQueryServiceImpl implements MemberQueryService { private final SqlSession sqlSession; public MybatisSpringMemberQueryServiceImpl(SqlSession sqlSession) { this.sqlSession = sqlSession; } @Override public void createMember(String id, String name) { Member newMember = new Member(id, name); sqlSession.insert(\"member.save\", newMember); sqlSessionTemplate.insert(\"memberHistory.save\", MemberHistory.forJoin(newMember)); }} 반면 위와 동일하게 SqlSessionTemplate 주입해서 사용하더라도, 아래 코드는 sqlSession.insert 호출하면 auto-commit 되지 않고 transactionManager.commit 시점에서 커밋됨public class MybatisSpringMemberQueryServiceImpl implements MemberQueryService { private final SqlSession sqlSession; private final PlatformTransactionManager transactionManager; public MybatisSpringMemberQueryServiceImpl(SqlSession sqlSession, PlatformTransactionManager transactionManager) { this.sqlSession = sqlSession; this.transactionManager = transactionManager; } @Override public void createMember(String id, String name) { TransactionStatus txStatus = transactionManager.getTransaction(new DefaultTransactionDefinition()); Member newMember = new Member(id, name); sqlSession.insert(\"member.save\", newMember); sqlSession.insert(\"memberHistory.save\", MemberHistory.forJoin(newMember)); transactionManager.commit(txStatus); }} 이게 어떻게 가능한 것인지 파악하기 위해 아래 순서로 내부 흐름을 살펴볼 예정이다. transactionManager.getTransaction sqlSession.insert // 첫번째 insert 1. transactionManager.getTransaction 내부적으로는 다양한 조건들에 의해 더 복잡하게 분기처리 되지만, 위 예제 코드가 어떻게 동작하는지의 흐름을 파악하는 정도로만 시퀀스 다이어그램을 그려봤다. (처음이라 제대로 그린것인지는 잘 모르겠지만 …) transactionManager.getTransaction 호출시 흐름을 내가 이해한대로 정리하면, 추상 클래스인 AbstractPlatformTransactionManager는 실제 구현체인 DataSourceTransactionManager에게 트랜잭션을 다루기 위한 객체를 만들어달라고 요청한다. 동일한 트랜잭션 내부에서는 하나의 커넥션이 사용되어야 하기 때문에, 그러한 커넥션을 관리할 ConnectionHolder가 필요하다(트랜잭션 동기화). 따라서, 트랜잭션을 다루기 위한 객체는 ConnectionHolder를 필요로하고 이를 얻기 위해 TransactionSynchronizationManager에게 요청한다. AbstractPlatformTransactionManager는 얻은 트랜잭션 객체(DataSourceTransactionObject)가 이미 처리하고 있는 트랜잭션이 존재하는지 확인한다. 없는 경우, 트랜잭션 처리를 시작하기 위한 준비를 한다. (트랜잭션 객체에 ConnectionHolder 세팅, ConnectionHolder에 트랜잭션 동기화 여부 세팅, ConnectionHolder에서 관리하는 Connection에 auto-commit false 세팅, TransactionSynchronizationManager에 ConnectionHolder 세팅되도록 등) 트랜잭션 및 트랜잭션 동기화와 관련된 속성들을 TransactionSynchronizationManager쪽에 세팅한다. 즉, transactionManager.getTransaction는 “기존에 처리되고 있는 트랜잭션이 있는지 확인(1)하고, 없으면 트랜잭션 처리를 위해 준비(2)하는 과정” 이라고 생각하면 될 것 같다. 참고 : org.springframework.jdbc.datasource.DataSourceTransactionManager#doBeginprotected void doBegin(Object transaction, TransactionDefinition definition) { DataSourceTransactionObject txObject = (DataSourceTransactionObject)transaction; Connection con = null; try { if (!txObject.hasConnectionHolder() || txObject.getConnectionHolder().isSynchronizedWithTransaction()) { Connection newCon = this.obtainDataSource().getConnection(); if (this.logger.isDebugEnabled()) { this.logger.debug(\"Acquired Connection [\" + newCon + \"] for JDBC transaction\"); } txObject.setConnectionHolder(new ConnectionHolder(newCon), true); } txObject.getConnectionHolder().setSynchronizedWithTransaction(true); con = txObject.getConnectionHolder().getConnection(); Integer previousIsolationLevel = DataSourceUtils.prepareConnectionForTransaction(con, definition); txObject.setPreviousIsolationLevel(previousIsolationLevel); txObject.setReadOnly(definition.isReadOnly()); if (con.getAutoCommit()) { txObject.setMustRestoreAutoCommit(true); if (this.logger.isDebugEnabled()) { this.logger.debug(\"Switching JDBC Connection [\" + con + \"] to manual commit\"); } con.setAutoCommit(false); } this.prepareTransactionalConnection(con, definition); txObject.getConnectionHolder().setTransactionActive(true); int timeout = this.determineTimeout(definition); if (timeout != -1) { txObject.getConnectionHolder().setTimeoutInSeconds(timeout); } if (txObject.isNewConnectionHolder()) { TransactionSynchronizationManager.bindResource(this.obtainDataSource(), txObject.getConnectionHolder()); } } catch (Throwable var7) { if (txObject.isNewConnectionHolder()) { DataSourceUtils.releaseConnection(con, this.obtainDataSource()); txObject.setConnectionHolder((ConnectionHolder)null, false); } throw new CannotCreateTransactionException(\"Could not open JDBC Connection for transaction\", var7); }} 참고 : org.springframework.transaction.support.AbstractPlatformTransactionManager#prepareSynchronizationprotected void prepareSynchronization(DefaultTransactionStatus status, TransactionDefinition definition) { if (status.isNewSynchronization()) { TransactionSynchronizationManager.setActualTransactionActive(status.hasTransaction()); TransactionSynchronizationManager.setCurrentTransactionIsolationLevel(definition.getIsolationLevel() != -1 ? definition.getIsolationLevel() : null); TransactionSynchronizationManager.setCurrentTransactionReadOnly(definition.isReadOnly()); TransactionSynchronizationManager.setCurrentTransactionName(definition.getName()); TransactionSynchronizationManager.initSynchronization(); }}2. 첫번째 sqlSession.insert TransactionSynchronizationManager에게 트랜잭션 동기화가 활성화 되어있는지 확인. 활성화되어 있으면 동기화에 필요한 SqlSessionHolder가 있는지 확인 SqlSessionHolder가 없으면 생성하여 TransactionSynchronizationManager에 바인딩 DB에 질의하기 위해 얻어온 SqlSession의 구현체 DefaultSqlSession를 통해 DB에 질의 질의시 1번 단계(transactionManager.getTransaction)에서 세팅한 ConnectionHolder의 Connection이 사용된다. 트랜잭션 동기화중인지 확인뒤, 동기화 중이지 않으면 질의한 것 바로 commit 후 SqlSession.close() 동기화 중이면, commit 하지않고 동기화에 사용되는 SqlSessionHolder의 SqlSession을 release 요약하면, (1) 트랜잭션 동기화중인지 확인후 동기화를 위한 SqlSessionHolder 등 준비 (2) 동기화 중이면, DB에 질의 후 SqlSession을 commit &amp; close하지 않고, SqlSession을 release SqlSession의 구현체로 사용되는 SqlSessionTemplate의 내부에서this.sqlSessionProxy = (SqlSession)Proxy.newProxyInstance(SqlSessionFactory.class.getClassLoader(), new Class[]{SqlSession.class}, new SqlSessionInterceptor());" }, { "title": "JDBC / MyBatis / MyBatis-Spring 훑어보기", "url": "/posts/hello-mybatis/", "categories": "지식 더하기, Hello-World 구현", "tags": "Mybatis", "date": "2023-12-21 22:25:00 +0900", "snippet": " JDBC부터 mybatis-spring까지 간단하게 사용해보면서 드는 개인적인 생각을 적어보자. 시나리오 : 회원 테이블에 회원 정보 저장 후, 회원 이력 테이블에 이력 저장하기 전 예외 발생 상황1 : 회원 정보는 저장됨 상황2 : 회원, 이력 테이블에 모두 저장되지 않음 JDBC 상황1public class...", "content": " JDBC부터 mybatis-spring까지 간단하게 사용해보면서 드는 개인적인 생각을 적어보자. 시나리오 : 회원 테이블에 회원 정보 저장 후, 회원 이력 테이블에 이력 저장하기 전 예외 발생 상황1 : 회원 정보는 저장됨 상황2 : 회원, 이력 테이블에 모두 저장되지 않음 JDBC 상황1public class JdbcMemberQueryServiceImpl implements MemberQueryService { @Override public void createMember(String id, String name) { try(Connection con = getConnection()) { PreparedStatement createMemberStmt = con.prepareStatement(\"INSERT INTO member (member_id, member_name) VALUES (?, ?)\"); createMemberStmt.setString(1, id); createMemberStmt.setString(2, name); createMemberStmt.executeUpdate(); PreparedStatement createMemberHistStmt = con.prepareStatement(\"INSERT INTO member_history (member_id, action_type, registered_at) VALUES (?, ?, ?)\"); createMemberHistStmt.setString(1, id); createMemberHistStmt.setString(2, \"JOIN\"); long epochSec = LocalDateTime.now().atZone(ZoneId.systemDefault()).toInstant().getEpochSecond(); createMemberHistStmt.setDate(3, new Date(epochSec)); causeException(); createMemberHistStmt.executeUpdate(); } catch (SQLException e) { e.printStackTrace(); } } private Connection getConnection() throws SQLException { String url = \"jdbc:mysql://localhost:3306/STUDY\"; String user = \"root\"; String password = \"root\"; return DriverManager.getConnection(url, user, password); }} 상황2public class JdbcMemberQueryServiceImpl implements MemberQueryService { @Override public void createMember(String id, String name) { try(Connection con = getConnection()) { con.setAutoCommit(false); // 위와 동일 ... causeException(); createMemberHistStmt.executeUpdate(); con.commit(); } catch (SQLException e) { e.printStackTrace(); } }}생각하는 문제점 비즈니스 로직(멤버 저장)에 DB관련 로직(커넥션 가져오기, SQL문 만들기 등)이 섞이게된다. 위와 같은 DB관련 코드가 여러 로직에 중복해서 나타나게된다. 쿼리문 작성시 오타날 수 있는 가능성이 있고, 있어도 컴파일 시점에선 알 수 없다. statement에 파라미터 세팅시 해당 파라미터의 데이터 타입을 다 알고있어야한다. 즉, DB에 강하게 결합된 로직이 될 수밖에 없다. java.sql 패키지에 정의된 데이터 타입이 강제된다.MyBatis 의존성 추가 : implementation 'org.mybatis:mybatis:3.5.14' MyBatis 관련 설정 파일 세팅 : mybatis-config.xml&lt;?xml version = \"1.0\" encoding = \"UTF-8\"?&gt;&lt;!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-config.dtd\"&gt;&lt;configuration&gt; &lt;environments default = \"development\"&gt; &lt;environment id = \"development\"&gt; &lt;transactionManager type = \"JDBC\"/&gt; &lt;dataSource type = \"POOLED\"&gt; &lt;property name = \"driver\" value = \"com.mysql.cj.jdbc.Driver\"/&gt; &lt;property name = \"url\" value = \"jdbc:mysql://localhost:3306/STUDY\"/&gt; &lt;property name = \"username\" value = \"root\"/&gt; &lt;property name = \"password\" value = \"root\"/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;mappers&gt; &lt;mapper resource = \"mybatis-mapper/member.xml\"/&gt; &lt;mapper resource = \"mybatis-mapper/memberHistory.xml\"/&gt; &lt;/mappers&gt;&lt;/configuration&gt; 매퍼 파일 세팅&lt;!-- member.xml--&gt;&lt;?xml version = \"1.0\" encoding = \"UTF-8\"?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace = \"member\"&gt; &lt;insert id = \"save\" parameterType = \"hello.persistence.model.Member\"&gt; INSERT INTO member (member_id, member_name) VALUES (#{id}, #{name}) &lt;/insert&gt;&lt;/mapper&gt;&lt;!--memberHistory.xml--&gt;&lt;?xml version = \"1.0\" encoding = \"UTF-8\"?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace = \"memberHistory\"&gt; &lt;insert id = \"save\" parameterType = \"hello.persistence.model.MemberHistory\"&gt; INSERT INTO member_history (member_id, action_type, registered_at) VALUES (#{member.id}, #{actionType}, NOW()) &lt;/insert&gt;&lt;/mapper&gt; 상황1public class MybatisMemberQueryServiceImpl implements MemberQueryService { private static SqlSessionFactory sqlSessionFactory; private static Reader reader; static { try { reader = Resources.getResourceAsReader(\"mybatis-config.xml\"); sqlSessionFactory = new SqlSessionFactoryBuilder().build(reader); } catch (IOException e) { e.printStackTrace(); } } @Override public void createMember(String id, String name) { try(SqlSession sqlSession = sqlSessionFactory.openSession()) { // autoCommit : true (default) Member newMember = new Member(id, name); sqlSession.insert(\"member.save\", newMember); causeException(); sqlSession.insert(\"memberHistory.save\", MemberHistory.forJoin(newMember)); } catch (Exception e) { e.printStackTrace(); } }} 상황2public class MybatisMemberQueryServiceImpl implements MemberQueryService { ... @Override public void createMember(String id, String name) { try(SqlSession sqlSession = sqlSessionFactory.openSession(false)) { ... causeException(); sqlSession.insert(\"memberHistory.save\", MemberHistory.forJoin(newMember)); sqlSession.commit(); } catch (Exception e) { e.printStackTrace(); } }}JDBC와 비교해서 나아졌다고 느끼는 부분 비즈니스 로직과 관련없는 DB 관련 로직(커넥션 얻어오기, statement 만들기 등)이 어느정도 분리됨 if, choose 등의 기능을 통해 Dynamic 쿼리를 보다 쉽게 작성할 수 있다.생각하는 문제점 컴파일 시점에 쿼리 오류 알 수 없음 (+ mapper id도 “member.save”와 같이 문자열로 받는다.) 세션을 열고 닫는 부분을 신경써야한다. (비즈니스 로직에 DB 관련 로직이 남아있음) 매퍼 파일이 생길때마다 설정에 추가해줘야하는 번거로움MyBatis-Spring 의존성 추가 : implementation ‘org.mybatis:mybatis-spring:2.1.2’ 공식 문서 에 나와있는 버전 호환성 MyBatis-Spring MyBatis Spring Framework Spring Batch Java 3.0 3.5+ 6.0+ 5.0+ Java 17+ 2.1 3.5+ 5.x 4.x Java 8+ 2.0 3.5+ 5.x 4.x Java 8+ 1.3 3.4+ 3.2.2+ 2.1+ Java 6+ SqlSessionTemplate 주입@Configurationpublic class AppConfig { @Bean public DataSource dataSource() { String dbUrl = \"jdbc:mysql://localhost:3306/STUDY\"; String user = \"root\"; String pw = \"root\"; DriverManagerDataSource dataSource = new DriverManagerDataSource(dbUrl, user, pw); dataSource.setDriverClassName(\"com.mysql.cj.jdbc.Driver\"); return dataSource; } @Bean public SqlSessionFactory sqlSessionFactory() throws Exception { SqlSessionFactoryBean factoryBean = new SqlSessionFactoryBean(); factoryBean.setDataSource(dataSource()); factoryBean.setMapperLocations(new PathMatchingResourcePatternResolver().getResources(\"classpath:mybatis-mapper/*.xml\")); return factoryBean.getObject(); } @Bean public SqlSessionTemplate sqlSession() throws Exception { return new SqlSessionTemplate(sqlSessionFactory()); } @Bean public MemberQueryService memberQueryService() throws Exception { return new MybatisSpringMemberQueryServiceImpl(sqlSession()); }}public class PersistenceDemoApp { public static void main(String[] args) { ApplicationContext ctx = new AnnotationConfigApplicationContext(AppConfig.class); MemberQueryService memberQueryService = (MemberQueryService) ctx.getBean(\"memberQueryService\"); memberQueryService.createMember(\"user1\", \"Kevin\"); }} 상황1public class MybatisSpringMemberQueryServiceImpl implements MemberQueryService { private final SqlSession sqlSession; public MybatisSpringMemberQueryServiceImpl(SqlSession sqlSession) { this.sqlSession = sqlSession; } @Override public void createMember(String id, String name) { Member newMember = new Member(id, name); sqlSession.insert(\"member.save\", newMember); causeException(); sqlSessionTemplate.insert(\"memberHistory.save\", MemberHistory.forJoin(newMember)); }} 위 예제에서 sqlSession의 구현체인 sqlSessionTemplate에서 commit, rollback을 직접호출하면 UnsupportedOperationException 발생, 기본적으로는 autoCommit된다. 또한 SqlSession에는 setAutoCommit하는 부분도 없다.따라서, 상황2를 위해서는 org.springframework.transaction.TransactionManager가 필요 공식 문서에 따르면 “마이바티스 스프링 연동모듈을 사용하는 중요한 이유중 하나는 마이바티스가 스프링 트랜잭션에 자연스럽게 연동될수 있다는 것이다. 마이바티스에 종속되는 새로운 트랜잭션 관리를 만드는 것보다는 마이바티스 스프링 연동모듈이 스프링의 DataSourceTransactionManager과 융합되는 것이 좋다.” 라고 되어있는게 mybatis-spring 자체적으로는 트랜잭션 관리하는 부분을 제공하지 않고 스프링쪽의 TransactionManager가 필요하다는 얘기인 것 같기도하다. AppConfig.java에 PlatformTransactionManager 빈 선언 및 MemberQueryService에 주입@Beanpublic PlatformTransactionManager transactionManager() { return new DataSourceTransactionManager(dataSource());}@Beanpublic MemberQueryService memberQueryService() throws Exception { return new MybatisSpringMemberQueryServiceImpl(sqlSession(), transactionManager());} 상황2public class MybatisSpringMemberQueryServiceImpl implements MemberQueryService { private final SqlSession sqlSession; private final PlatformTransactionManager transactionManager; public MybatisSpringMemberQueryServiceImpl(SqlSession sqlSession, PlatformTransactionManager transactionManager) { this.sqlSession = sqlSession; this.transactionManager = transactionManager; } @Override public void createMember(String id, String name) { TransactionStatus txStatus = transactionManager.getTransaction(new DefaultTransactionDefinition()); Member newMember = new Member(id, name); sqlSession.insert(\"member.save\", newMember); causeException(); sqlSession.insert(\"memberHistory.save\", MemberHistory.forJoin(newMember)); transactionManager.commit(txStatus); }}MyBatis와 비교해서 나아졌다고 느끼는 부분 SqlSessionFactory 사용해서 SqlSession을 매번 생성하고 커밋/롤백후 닫는 작업을 안해줘도된다. SqlSessionTemplate은 Thread-safe하다. (참고)생각하는 문제점 트랜잭션 처리를 위해 TransactionManager, TransactionStatus 등 이전에 비해 의존해야될 객체들이 더 생긴다. 비즈니스 로직에 트랜잭션 관련 처리 코드가 섞인다. statement 문자열로 선언위 상황2 개선해보기 : @Transactional 사용 AppConfig.java에 @EnableTransactionManagement 선언public class MybatisSpringMemberQueryServiceImpl implements MemberQueryService { private final SqlSession sqlSession; public MybatisSpringMemberQueryServiceImpl(SqlSession sqlSession) { this.sqlSession = sqlSession; } @Override @Transactional public void createMember(String id, String name) { Member newMember = new Member(id, name); sqlSession.insert(\"member.save\", newMember); causeException(); sqlSession.insert(\"memberHistory.save\", MemberHistory.forJoin(newMember)); }}전과 비교해서 나아졌다고 느끼는 부분 TransactionStatus, PlatformTransactionManager에 의존하지 않아도된다. 비즈니스 로직에서 트랜잭션 관련 부분이 사라진다.Statement id 문자열로 입력하는 부분 개선하기 : 매퍼 인터페이스 사용public interface MemberMapper { void save(Member member);}public interface MemberHistoryMapper { void save(MemberHistory memberHistory);} AppConfig.java에 Mapper 빈 추가@Beanpublic MemberMapper memberMapper() throws Exception { MapperFactoryBean&lt;MemberMapper&gt; factoryBean = new MapperFactoryBean&lt;&gt;(MemberMapper.class); factoryBean.setSqlSessionFactory(sqlSessionFactory()); return factoryBean.getObject();}@Beanpublic MemberHistoryMapper memberHistoryMapper() throws Exception { MapperFactoryBean&lt;MemberHistoryMapper&gt; factoryBean = new MapperFactoryBean&lt;&gt;(MemberHistoryMapper.class); factoryBean.setSqlSessionFactory(sqlSessionFactory()); return factoryBean.getObject();}@Beanpublic MemberQueryService memberQueryService() throws Exception { return new MybatisSpringMemberQueryServiceImpl(memberMapper(), memberHistoryMapper());} 또는, @MapperScan을 사용하면 @Configuration@MapperScan(\"hello.persistence.mybatis.mapper\")public class AppConfig { @Bean public MemberQueryService memberQueryService() throws Exception { // TODO : MapperScan 사용할 때는 의존성 주입 어떻게하지 ?? }} 매퍼 파일의 namespace에는 매퍼 인터페이스의 full package 명을 적어야한다. member.xml : &lt;mapper namespace = \"hello.persistence.mybatis.mapper.MemberMapper\"&gt; memberHistory.xml : &lt;mapper namespace = \"hello.persistence.mybatis.mapper.MemberHistoryMapper\"&gt; public class MybatisSpringMemberQueryServiceImpl implements MemberQueryService { private MemberMapper memberMapper; private MemberHistoryMapper memberHistoryMapper; public MybatisSpringMemberQueryServiceImpl(MemberMapper memberMapper, MemberHistoryMapper memberHistoryMapper) { this.memberMapper = memberMapper; this.memberHistoryMapper = memberHistoryMapper; } @Override @Transactional public void createMember(String id, String name) { Member newMember = new Member(id, name); memberMapper.save(newMember); causeException(); memberHistoryMapper.save(MemberHistory.forJoin(newMember)); }}전과 비교해서 나아졌다고 느끼는 부분 DB 관련된 코드가 비즈니스 로직에서 완전히 분리된다. Statement id가 잘못 맵핑된 경우 컴파일 시점에서 발견될 수 있다. (즉, 문자열 오타 등으로 인해 없는 Statement 호출할 일은 없음)" }, { "title": "키릴문자를 아시나요 ?", "url": "/posts/cyrillic-alphabet-issue/", "categories": "경험하기, 이슈 노트", "tags": "", "date": "2023-11-28 10:25:00 +0900", "snippet": "상황 담당하는 서비스에서 발행한 쿠폰 번호를 사용자가 앱에서 입력했는데 등록되지 않는다는 CS가 접수 로그에 남은 해당 쿠폰의 앞자리를 복사하여 DB에서 조회해봤는데 결과가 없음 하지만, 사용자가 보내준 스크린샷에 있는 쿠폰 번호를 직접 입력해서 조회하면 결과가 있음원인 파악하기 아래 코드로 로그에서 복사한 경우와 직접 입력한 경우를 비교해보았...", "content": "상황 담당하는 서비스에서 발행한 쿠폰 번호를 사용자가 앱에서 입력했는데 등록되지 않는다는 CS가 접수 로그에 남은 해당 쿠폰의 앞자리를 복사하여 DB에서 조회해봤는데 결과가 없음 하지만, 사용자가 보내준 스크린샷에 있는 쿠폰 번호를 직접 입력해서 조회하면 결과가 있음원인 파악하기 아래 코드로 로그에서 복사한 경우와 직접 입력한 경우를 비교해보았다.import java.util.Scanner;public class Main { public static void main(String[] args) { Scanner sc = new Scanner(System.in); String s = sc.nextLine(); char[] chars = s.toCharArray(); for (char c : chars) { System.out.println(\"char to int for \"+ c + \"=\" +(int) c); } }} 좌 : 로그에서 복사, 우 : 직접 입력 앞 네 글자(M3AE)의 decimal code가 다른 것을 알 수 있다. 로그에서 복사한 M3AE는 키릴 문자라고 불리는 문자열이라고 한다. 왼쪽이 키릴 문자, 오른쪽이 라틴 문자 (일반 알파벳) - 참고 사이트상황재현1) (아이폰 기준) 키릴 문자 키보드 세팅 2) M3AE는 키릴 문자 키보드로, 나머지 문자는 일반 영어 키보드로 입력 3) 서버 로그에서도 M3AE 부분은 키릴문자로 찍힘하지만 실제 사용자가 이렇게 입력했을리는 절대 없을 것 같다.결론 사용자분께 확인한 결과 키보드 어플을 사용하셨고 기본 키보드로 입력했을 때 정상 등록됨을 확인했다." }, { "title": "javax.net.ssl.SSLHandshakeException - No appropriate protocol (protocol is disabled or cipher suites are inappropriate)의 원인을 찾아서", "url": "/posts/mysql-ssl-handshake-fail/", "categories": "경험하기, 이슈 노트", "tags": "MySQL, SSL/TLS", "date": "2023-09-05 10:25:00 +0900", "snippet": "상황 신규 이전한 서버에서 애플리케이션을 띄우는데 다음과 같은 에러를 마주하게됐다. 이전 서버에서와 달라진 점은 CentOS 7.4에서 7.9로 바뀐 것밖에 없는데 뭐가 문제일까 이해가 잘되지 않았다.또한 로컬 환경에서는 문제없이 실행되는 상황이라 추적이 더 어려웠다.Caused by: javax.net.ssl.SSLHandshakeException...", "content": "상황 신규 이전한 서버에서 애플리케이션을 띄우는데 다음과 같은 에러를 마주하게됐다. 이전 서버에서와 달라진 점은 CentOS 7.4에서 7.9로 바뀐 것밖에 없는데 뭐가 문제일까 이해가 잘되지 않았다.또한 로컬 환경에서는 문제없이 실행되는 상황이라 추적이 더 어려웠다.Caused by: javax.net.ssl.SSLHandshakeException: No appropriate protocol (protocol is disabled or cipher suites are inappropriate)\tat sun.security.ssl.HandshakeContext.&lt;init&gt;(HandshakeContext.java:171)\tat sun.security.ssl.ClientHandshakeContext.&lt;init&gt;(ClientHandshakeContext.java:103)\tat sun.security.ssl.TransportContext.kickstart(TransportContext.java:220)\tat sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:433)\tat com.mysql.jdbc.ExportControlled.transformSocketToSSLSocket(ExportControlled.java:186) 에러 로그를 봤을때 SSL 핸드셰이크하는 과정에서 적합한 프로토콜 또는 Cipher Suite가 없어서 발생한 에러로 보인다. 에러가 발생한 부분의 코드를 살펴보자 sun.security.ssl.HandshakeContext#HandshakeContext 결과적으로 activeProtocols가 빈 리스트라 발생한 에러임을 알 수 있다. 로컬 환경에서 디버거로 추적해보았을 때, getActiveProtocols 메서드의 첫 번째 파라미터인 enabledProtocols에 세팅된 프로토콜은 TLS1.0, TLS1.1이었다. 원인 파악을 위해서는 다음 두 가지 살펴봐야 할 것 같다. enabledProtocols엔 어떻게 TLS1.0, TLS1.1이 세팅된 것일까 왜 기존 서버와 로컬에서는 되고 신규 서버에서는 안될까 ? 그럼, 먼저 전반적인 호출 흐름을 살펴보자호출 흐름 살펴보기 에러 로그 콜 스택에서도 볼 수 있듯이, 객체 호출 흐름은 다음과 같다. com.mysql.jdbc.ExportControlled.transformSocketToSSLSocket-&gt; sun.security.ssl.SSLSocketImpl.startHandshake-&gt; sun.security.ssl.TransportContext.kickstart-&gt; sun.security.ssl.ClientHandshakeContext.&lt;init&gt;-&gt; sun.security.ssl.HandshakeContext.&lt;init&gt; 두 번째 단계인 SSLSocketImpl의 클래스를 보면 아래와 같이 SSLContextImpl, TransportContext 클래스를 인스턴스 변수로 선언하고 있고 final로 되어있는 것을 알 수 있다. 즉, SSLSocketImpl이 생성될 때, SSLContextImpl, TransportContext 객체도 반드시 함께 생성되는 것을 알 수 있다. public final class SSLSocketImpl extends BaseSSLSocketImpl implements SSLTransport { final SSLContextImpl sslContext; final TransportContext conContext; \t...} TransportContext.kickstart 이후의 흐름은 ClientHandshakeContext 객체를 생성하는 것인데, 맨 처음 sun.security.ssl.HandshakeContext#HandshakeContext을 다시 살펴보면 생성자 파라미터로 SSLContextImpl과 TransportContext가 필요한 것을 알 수 있다. 다시 거꾸로 정리하면, ClientHandshakeContext는 생성시 HandshakeContext 생성자를 호출하고, HandshakeContext를 생성하기 위해서는 SSLSocketImpl의 SSLContextImpl과 TransportContext가 반드시 필요하다. 그리고, 이 두 객체는 SSLSocketImpl이 생성될 때 함께 세팅된다. 따라서, ExportControlled.transformSocketToSSLSocket에서 어떻게 SSLSocketImpl를 생성하는지, 그리고 여기에 세팅되는 SSLContextImpl과 TransportContext중 특히 (getActiveProtocols의 파라미터가 되는) TransportContext의 SSLConfiguration이 어떤지가 중요할 것 같다. 이제 위 흐름을 생각하면서 원인 파악을 위한 두 가지 부분을 체크해보자.1. enabledProtocols엔 어떻게 TLS1.0, TLS1.1이 세팅된 것일까 ? ExportControlled.transformSocketToSSLSocket에서는 SocketFactory를 통해 SSLSocket을 생성한다. (mysqlIO.mysqlconnection = sslFact.connect) 그리고 맨 아래 ((SSLSocket) mysqlIO.mysqlConnection).setEnabledProtocols(allowedProtocols.toArray(new String[0])) 부분을 주목해서 보자. 에러가 발생한 애플리케이션의 DB 서버는 MySQL 5.7버전을 사용하고 있기 때문에, 따로 enabledTLSProtocols 옵션을 주지 않은 이상 tryProtocols = new String[] { TLSv1_1, TLSv1 }; 이 부분을 탈 것이고, configuredProtocols에 이 두 개의 프로토콜이 세팅된다. 결과적으로, if (jvmSupportedProtocols.contains(protocol) &amp;&amp; configuredProtocols.contains(protocol)) 조건을 만족하는 프로토콜은 TLS1.1, TLS1.0밖에 없기때문에 enabledTLSProtocols에 두 개의 프로토콜이 세팅된다.2. 왜 로컬에서는 되고 서버에서는 안될까 ? HandshakeContext의 getActiveProtocols 메서드에서 algorithmConstraints.permits 부분을 자세히 살펴보았다. sun.security.ssl.SSLAlgorithmConstraints#permits의 tlsDisabledAlgConstraints.permits 부분을 타는 것을 알 수 있었다. sun.security.util.DisabledAlgorithmConstraints#permits 요약하자면, checkAlgorithm에서 통과하지 못한 TLS 프로토콜은 available하지 않는 프로토콜이며, 이는 해당 프로토콜이 disabledAlgorithms 목록에 있는지로 판단한다. @Overridepublic final boolean permits(Set&lt;CryptoPrimitive&gt; primitives, String algorithm, AlgorithmParameters parameters) { if (!checkAlgorithm(disabledAlgorithms, algorithm, decomposer)) { return false; } if (parameters != null) { return algorithmConstraints.permits(algorithm, parameters); } return true;} disabledAlgorithms은 DisabledAlgorithmConstraints가 생성될때 세팅된다. PROPERTY_TLS_DISABLED_ALGS은 \"jdk.tls.disabledAlgorithms\"로 선언되어있다. final class SSLAlgorithmConstraints implements AlgorithmConstraints { private static final AlgorithmConstraints tlsDisabledAlgConstraints = new DisabledAlgorithmConstraints(PROPERTY_TLS_DISABLED_ALGS, new SSLAlgorithmDecomposer()); public DisabledAlgorithmConstraints(String propertyName, AlgorithmDecomposer decomposer) { super(decomposer); disabledAlgorithms = getAlgorithms(propertyName); // propertyName = \"jdk.tls.disabledAlgorithms\" getAlgorithms은 $JAVA_HOME/jre/lib/security/java.security 파일에서 propertyName인 jdk.tls.disabledAlgorithms에 정의된 알고리즘(프토토콜) 목록을 가져온다. 로컬에서 확인해보니 해당 프로퍼티는 다음과 같이 선언되어 있었다. (openjdk-1.8.0.242 버전) jdk.tls.disabledAlgorithms=SSLv3, RC4, DES, MD5withRSA, DH keySize &lt; 1024, \\ EC keySize &lt; 224, 3DES_EDE_CBC, anon, NULL 여기서 문득, 신규 서버의 자바 릴리즈 버전은 뭐지? 라는 생각이 들었고, 확인해보니 기존에 사용하던 openjdk-1.8.0.242이 아닌 openjdk8u362-b09 버전이 사용되고 있었다. 해당 버전을 로컬에 설치해서 살펴본 결과, jdk.tls.disabledAlgorithms 프로퍼티는 다음과 같이 선언되어 있었다. (TLSv1, TLSv1.1이 추가되어있다) jdk.tls.disabledAlgorithms=SSLv3, TLSv1, TLSv1.1, RC4, DES, MD5withRSA, \\ DH keySize &lt; 1024, EC keySize &lt; 224, 3DES_EDE_CBC, anon, NULL, \\ include jdk.disabled.namedCurves 이로써 기나긴 삽질의 시간을 끝낼 수 있게 되었다..!정리 MySQL 드라이버(mysql-connector-java 5.1.46 버전 기준)에서 enabledTLSProtocols과 같은 세팅이 별도로 정의되어있지 않으면, MySQL 5.7 서버와 SSL 핸드셰이크를 위한 프로토콜로 TLS1.0, TLS.1.1이 사용된다. 같은 자바 버전이더라도 릴리즈 버전에 따라 SSL 통신시 disable된 프로토콜이 다를 수 있다. 이로 인해, SSL 핸드셰이크가 제대로 이루어지지 않을 수 있다. (이전과 달라진게 없는데 왜 문제가 생기지..? 라고 섣불리 생각하지 말자)생각해본 조치 방안 MySQL 버전 업그레이드 enabledTLSProtocols 정의 jdbc:mysql://{ip}:{port}/{dbName}?&amp;enabledTLSProtocols={TLS_VERSION} 해당 MySQL 서버에서 지원하는 TLS 프로토콜이어야한다. 지원하는 프로토콜 확인 방법 : show variables like 'tls_%'; useSSL=false (해당 옵션을 주면 SSL 통신 자체를 하지 않는다.) jdbc:mysql://{ip}:{port}/{dbName}?&amp;useSSL=false com.mysql.jdbc.MysqlIO#doHandshake if (!this.connection.getUseSSL()) { ... } else { negotiateSSLConnection(user, password, database, packLength); " }, { "title": "HTTPS 요청이 HTTP로 리다이렉트 되는 현상", "url": "/posts/https-to-http-redirect/", "categories": "경험하기, 이슈 노트", "tags": "HTTP", "date": "2023-06-14 22:25:00 +0900", "snippet": "상황 서버 이전 작업 후 담당 웹사이트에서 URL 리다이렉트시 페이지 응답이 없는 경우가 발생함 특정 사용자에게서는 이런 현상이 발생하지 않음 확인해보니 HTTPS 요청이 HTTP로 바껴서 리다이렉트됨원인 파악하기 요청이 WAS에 도달하기까지의 흐름 (브라우저) ---- https ---- (L4/L7 스위치) ---- http...", "content": "상황 서버 이전 작업 후 담당 웹사이트에서 URL 리다이렉트시 페이지 응답이 없는 경우가 발생함 특정 사용자에게서는 이런 현상이 발생하지 않음 확인해보니 HTTPS 요청이 HTTP로 바껴서 리다이렉트됨원인 파악하기 요청이 WAS에 도달하기까지의 흐름 (브라우저) ---- https ---- (L4/L7 스위치) ---- https ---- (웹서버) ---- http ---- (WAS) 인터셉터에서 HttpServletResponse.sendRedirect 하는 부분을 타고 들어가다보면 헤더에 Location을 세팅하는 코드를 만날 수 있다. org.apache.catalina.connector.Response#sendRedirect else절에 toAbsoulte 메서드에서 요청 프로토콜은 request.getScheme() 을 통해 가져온다.org.apache.catalina.connector.Response#toAbsolute즉, WEB에서 WAS로 들어오는 요청은 http이므로 리다이렉트 url이 http로 만들어지게 되었던 것의문점 이전 서버 구성에서는 이런 이슈 제보된 적이 없었음 =&gt; 이전 L4/L7 스위치에서 80 포트가 허용 되어있었음 특정 사용자는 접근이 됨=&gt; HSTS로 인해 HTTPS로 리다이렉트 되었음 (웹서버 설정 : Strict-Transport-Security: max-age=31536000) ※ HSTS란 ? 공식문서 참고 HTTP Strict-Transport-Security 응답 헤더(종종 HSTS로 축약됨)는 사이트가 HTTPS를 통해서만 접근되어야 하며 향후 HTTP를 사용하여 사이트에 접근하려는 모든 시도는 자동으로 HTTPS로 변환되어야 함을 브라우저에 알립니다. HTTPS를 사용하여 사이트에 처음으로 접근하고 Strict-Transport-Security 헤더를 반환하면 브라우저가 이 정보를 기록하고, 이후에 HTTP를 사용하여 사이트를 로드하려고 시도할 때 자동으로 HTTPS를 대신 사용합니다.조치해결방법1 : 톰캣 use-relative-redirects 속성 사용 공식문서 설명 “sendRedirect 호출로 생성된 HTTP 1.1 이상의 Location 헤더가 상대 리디렉션을 사용할지 절대 리디렉션을 사용할지 여부 (기본값 : false)” server.tomcat.use-relative-redirects=true로 세팅하면 response.sendRedirect(\"/상대경로\")로 호출시 리다이렉트 url이 상대경로로 지정됨 (요청 프로토콜 그대로 사용) 관련 코드 (위에서 살펴본 org.apache.catalina.connector.Response#sendRedirect의 if절) 리다이렉트 url이 상대경로로 지정됐을때, https로 재요청하는 모습해결방법2 : 톰캣에서 X-Forwarded-Proto 인식할 수 있도록 하기 웹 서버에서 톰캣으로 요청시 X-Forwarded-Proto 헤더에 https를 담아서 보낸다. 관련 아파치 설정 :RequestHeader set X-Forwarded-Proto \"https\" 톰캣에서 리다이렉트 url 지정시 request에 담긴 X-Forwarded-Proto 활용하게 한다. 이렇게 하려면 response.sendRedirect시 response 구현체가 org.springframework.web.filter 패키지의 ForwardedHeaderFilter.ForwardedHeaderExtractingResponse가 되도록 해야함 이를 위해, 아래와 같이 필터를 빈으로 등록 (부트 2.1 버전부터는 직접등록하지 않고 톰캣에 관련 설정만 해주면 자동으로 세팅되는 것 같음 : 관련 글 - 담당 웹사이트는 2.0.3v 사용) @Configurationpublic class XForwardedFilterConfig { @Bean public FilterRegistrationBean&lt;ForwardedHeaderFilter&gt; forwardedHeaderFilter() { FilterRegistrationBean&lt;ForwardedHeaderFilter&gt; registrationBean = new FilterRegistrationBean&lt;&gt;(); registrationBean.setFilter(new ForwardedHeaderFilter()); return registrationBean; }} 리다이렉트 url이 https로 세팅됨참고. HTTP 리다이렉트 관련 상태 코드   301 302 303 307 308 Status Moved Permanently Found See Other Temporary Redirect Permanent Redirect 영구적 리다이렉션 O X X X O 기존 요청 변경 O (POST -&gt; GET) O (POST -&gt; GET) O (무조건 GET으로 변경) X X ※ 영구적 리다이렉션에서는 기존 URL에 대한 모든 SEO 값과 링크 리소스가 새 URL로 이전." }, { "title": "wait_timeout을 초과한 커넥션을 사용해서 겪은 이슈 (The last packet successfully received from the server was ... milliseconds ago)", "url": "/posts/wait-timeout-exceed-connection/", "categories": "경험하기, 이슈 노트", "tags": "Connection Pool, MySQL", "date": "2023-05-02 10:25:00 +0900", "snippet": "상황 최근 배치 애플리케이션의 커넥션 풀 관련 설정을 변경한 후 MMS 발송하는 job에서 아래와 같은 에러 발생 Cause: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException:The last packet successfully received from the server was 104,983,...", "content": "상황 최근 배치 애플리케이션의 커넥션 풀 관련 설정을 변경한 후 MMS 발송하는 job에서 아래와 같은 에러 발생 Cause: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException:The last packet successfully received from the server was 104,983,345 milliseconds ago.The last packet sent successfully to the server was 104,983,347 milliseconds ago.is longer than the server configured value of 'wait_timeout'. 커넥션 풀 관련 설정 변경 전 initial-size: 0 // 커넥션 풀 최초 사이즈max-active: 8 // 최대로 활성화할 커넥션 갯수max-idle: 8 // 커넥션 풀에 유지할 최대 커넥션 갯수min-idle: 0 // 커넥션 풀에 유지할 최소 커넥션 갯수max-wait : 3초 // DB 커넥션 얻기위해 대기하는 최대 시간 변경 후 initial-size: 20 // 커넥션 풀 최초 사이즈max-active: 20 // 최대로 활성화할 커넥션 갯수max-idle: 20 // 커넥션 풀에 유지할 최대 커넥션 갯수min-idle: 20 // 커넥션 풀에 유지할 최소 커넥션 갯수max-wait : 5초 // DB 커넥션 얻기위해 대기하는 최대 시간 에러 원인 찾기 출처 : https://kakaocommerce.tistory.com/45 MySQL 서버 입장에서 JDBC 커넥션이 Idle인 상태로 wait_timeout(default 28800초) 이상 사용되지 않을 경우해당 커넥션을 close 하여 할당된 리소스를 회수하게 된다.이런 경우 TCP 레벨에서 해당 socket이 정리가 되더라도 JDBC 드라이버나 커넥션 풀에서는 close 여부를 바로 알 수 없고해당 커넥션을 사용하려고 시도 할 때에서야 비로소 커넥션에 문제가 있는 것을 알게 된다.이런 상황일 경우 최근에 성공한 통신이 XXX,XXX milliseonds 이전이라고 위와 같은 에러 메시지를 출력하게 된다. ※ 참고 CUBRID는 자체적으로 커넥션을 관리하고 자동으로 다시 연결하도록 구현되어있다고 한다. (출처 : https://d2.naver.com/helloworld/5102792)에러 재현해보기 검증해보고 싶은 부분 idle 상태인 커넥션중 wait_timeout 지나면 mysql 서버에서 해당 커넥션 만큼의 세션(스레드) 갯수가 감소한 것을 확인할 수 있다. 닫힌 세션에 맵핑된 커넥션으로 질의 요청하는 경우 위 에러가 발생한다. 검증 코드 검증 과정 코드 실행 전 mysql workbench에서 확인한 현재 열려있는 커넥션 갯수 (show status where variable_name = 'Threads_connected';) 커넥션 연결 (두 개의 커넥션 중 하나의 커넥션에만 wait_timeout 5초 설정 : ①, 나머지 하나는 default 값인 28800초) wait_timeout 이상 대기 (②) “idle 상태인 커넥션중 wait_timeout 지나면 mysql 서버에서 해당 커넥션 만큼의 세션(스레드) 갯수가 감소한 것을 확인할 수 있다.” 에 대해 검증 완료 mysql 서버에서 닫힌 커넥션에 질의할시 동일한 에러 발생(③) “닫힌 세션에 맵핑된 커넥션으로 질의 요청하는 경우 위 에러가 발생한다.” 에 대해 검증 완료 Exception in thread \"main\" com.mysql.cj.jdbc.exceptions.CommunicationsException:The last packet successfully received from the server was 107,054 milliseconds ago. (중략 ...) 의문점 MMS 발송 job은 5분마다 돌면서 발송 대상이 있는지 조회한다. 그리고 발송 job 이외에도 주기적으로 도는 다른 job들도 있기 때문에 커넥션 풀에 있는 커넥션들이 주기적으로 사용될 것 같은데 왜 커넥션이 끊어진걸까 ?커넥션 풀 내부 살펴보기 tomcat-jdbc-7.0.56 org.apache.tomcat.jdbc.pool.ConnectionPool커넥션 풀 초기화 BlockingQueue 구현 클래스 할당 idle 커넥션 큐는 FairBlockingQueue(https://tomcat.apache.org/tomcat-7.0-doc/api/org/apache/tomcat/jdbc/pool/FairBlockingQueue.html) 실제 커넥션 할당 (busy 큐에 들어가게됨) busy 큐 -&gt; idle 큐로 이동 커넥션 가져오기 idle 큐의 맨 앞에 있는 커넥션 가져옴커넥션 반납 idle 큐의 offer 메서드 호출 반납된 커넥션이 맨 앞에 들어가는 것을 볼 수 있다. 의문점 해결하기 MMS 발송 job은 5분마다 돌면서 발송 대상이 있는지 조회한다. 그리고 발송 job 이외에도 주기적으로 도는 다른 job들도 있기 때문에 커넥션 풀에 있는 커넥션들이 주기적으로 사용될 것 같은데 왜 커넥션이 끊어진걸까 ? MMS 발송 로직 발송 대상 조회(커넥션1) -&gt; 발송 &amp; 발송 관련 정보 업데이트(비동기 호출, 커넥션2..n) 사용되는 커넥션 갯수 추정해보기 idle 큐에 보관되는 커넥션 중 MMS 발송 건이 없는 경우엔 맨 앞 커넥션 하나 또는 있더라도 맨 앞부터 n개의 커넥션이 사용될 것 (동시 발송 건수가 많지 않으면 n의 크기가 작을 것.) MMS 발송 job 이외에 커넥션을 여러 개 사용하는 job들은 거의 없고 또한 동시에 job들이 도는 경우도 많지 않기 때문에 n개의 커넥션이 사용된다 하더라도 n의 크기는 작을 것이다. 위에서 살펴보았듯이 커넥션 반납시 큐의 맨 뒤가 아닌 맨 앞에 추가되기 때문에, 사용되는 커넥션만 계속 사용될 것 결과적으로, 앞에 몇 개의 커넥션만 계속 사용되다가 MMS 발송건이 여러 건인 경우 8시간(wait_timeout)이상 사용되지 않았던 뒤쪽의 커넥션(이미 세션이 닫힌)까지 사용하기 때문에 에러가 발생했던 것문제 해결하기1. 커넥션 풀에 idle 커넥션 갯수 0개로 유지 커넥션 정리는 ConnectionPool.PoolCleaner에서 담당 (Evictor 스레드) 사실상 커넥션 풀 사용하는 이점이 사라지는 것(?) 트래픽 많지 않아서 현재로서 운영상 이슈는 없음2. PoolCleaner에서 주기적으로 커넥션 유효성 체크하도록 validationQuery, timeBetweenEvictionRunsMillis, testWhileIdle 등의 속성 활용하여 주기적으로 커넥션 유효성 검사 유의사항 (출처 : https://d2.naver.com/helloworld/5102792) - commons dbcp 1.x 기준 Evictor 스레드는 동작 시에 커넥션 풀에 잠금(lock)을 걸고 동작하기 때문에 너무 자주 실행하면 서비스 실행에 부담을 줄 수 있다. 또한 numTestsPerEvictionRun 값을 크게 설정하면 Evictor 스레드가 검사해야 하는 커넥션 개수가 많아져 잠금 상태에 있는 시간이 길어지므로 역시 서비스 실행에 부담을 줄 수 있다. 게다가 커넥션 유효성 검사를 위한 테스트 옵션(testOnBorrow, testOnReturn, testWhileIdle)을 어떻게 설정하느냐에 따라 애플리케이션의 안정성과 DBMS의 부하가 달라질 수 있다. 그러므로 Evictor 스레드와 테스트 옵션을 사용할 때는 데이터베이스 관리자와 상의해서 사용하는 DBMS에 최적화될 수 있는 옵션으로 설정해야 한다. 3. autoReconnect ? 동작 방식 커넥션이 끊어진 경우 예외를 발생시키고 해당 트랜잭션 종료시킴. 이후 새로운 트랜잭션에서 해당 커넥션 사용하는 경우 다시 연결 시도. 따라서, autoReconnect 속성이 true이더라도 커넥션이 끊겨서 발생하는 최초의 에러를 막을수는 없다. 공식 문서 에서는 데이터 일관성, 세션 상태 등의 이유로 사용하는 것을 추천하지 않는다." }, { "title": "MySQL에서 시간,날짜 데이터를 저장할 때 반올림되는 현상 (2)", "url": "/posts/mysql-round-up-part2/", "categories": "경험하기, 이슈 노트", "tags": "MySQL, MyBatis", "date": "2023-03-29 22:25:00 +0900", "snippet": "상황 이전 포스팅 에서 MySQL 반올림 현상으로 인해 겪었던 이슈를 살펴보았다. 이번 포스팅에서는 해당 이슈를 해결하면서 궁금했던 소수점 초가 제거되기까지의 자료형 변환 과정을 들여다보자.소수점 초가 제거되기까지의 과정 살펴보기public void 상품권생성() { 상품권생성정보 info = new 상품권생성정보(param1, param2, ...", "content": "상황 이전 포스팅 에서 MySQL 반올림 현상으로 인해 겪었던 이슈를 살펴보았다. 이번 포스팅에서는 해당 이슈를 해결하면서 궁금했던 소수점 초가 제거되기까지의 자료형 변환 과정을 들여다보자.소수점 초가 제거되기까지의 과정 살펴보기public void 상품권생성() { 상품권생성정보 info = new 상품권생성정보(param1, param2, ...); 중략 ... giftRepository.save(info);}public 상품권생성정보(param1, param2, ...) {\t\t... this.registerExpireYmdt = getExpirationDate();}private Date getExpirationDate() { LocalDateTime 등록만료일시 = LocalDateTime.now() .plusYears(1) .withMonth(12) .withDayOfMonth(31) .withHour(23) .withMinute(59) .withSecond(59) .withNano(0); // LocalDateTime -&gt; Date형으로 변환 Instant instant = 등록만료일시.atZone(ZoneId.systemDefault()).toInstant(); return Date.from(instant);} 실제 DB에 쿼리를 날리기 위해 JDBC 레벨에서 다루는 날짜 관련 자료형은 java.sql.Timestamp 따라서 만료일시가 쿼리에 세팅되기까지 자료형 변환은 java.time.LocalDateTime → java.util.Date → java.sql.Timestamp의 과정을 거침 LocalDateTime → Date로의 변환은 getExpirationDate에 있지만 Date → Timestamp로의 변환은 MyBatis에서 해준다.※ 참고 MySQL 8.0부터는 JDBC 드라이버 이름이 mysql-connector-java -&gt; mysql-connector-j로 변경됨 (https://dev.mysql.com/doc/relnotes/connector-j/8.0/en/news-8-0-31.html1. java.time.LocalDateTime → java.util.Date 변환Instant instant = 등록만료일시.atZone(ZoneId.systemDefault()).toInstant();return Date.from(instant); Instant instant = 등록만료일시.atZone(ZoneId.systemDefault()).toInstant(); LocalDateTime을 Instant로 변환 Instant 는 현재 순간(시각)을 seconds와 nanos로 표현함 seconds : 1970-01-01T00:00:00Z부터 현재까지 경과한 시간을 초로 나타낸 것. 참고 : Epoch Time nanos : 현재 시간을 나노초 정밀도(소수점 9자리까지)로 표현 Date.from(instant) java.util.Date의 정밀도는 밀리세컨드 public static Date from(Instant instant) { try { return new Date(instant.toEpochMilli()); } catch (ArithmeticException ex) { throw new IllegalArgumentException(ex); } } toEpochMilli () 1970-01-01T00:00:00Z부터의 epoch time을 밀리초 단위로 변환 즉, Instant의 정밀도는 나노초이지만 Date의 정밀도는 밀리초이기 때문에toEpochMilli()로 변환. public long toEpochMilli() { if (seconds &lt; 0 &amp;&amp; nanos &gt; 0) { long millis = Math.multiplyExact(seconds+1, 1000); long adjustment = nanos / 1000_000 - 1000; return Math.addExact(millis, adjustment); } else { long millis = Math.multiplyExact(seconds, 1000); return Math.addExact(millis, nanos / 1000_000); } } 요약 LocalDateTime, Instant는 나노초 정밀도(2024-12-31 23:59:59.123456789) 까지 표현할 수 있고 Date는 밀리초 정밀도(2024-12-31 23:59:59.123) 까지 표현할 수 있다. LocalDateTime의 nano가 0이면 Date가 표현할 수 있는 밀리초 부분도 0이 된다. 2. java.util.Date → java.sql.Timestamp 변환 org.apache.ibatis.type.DateTypeHandler가 중간에서 변환해줌 parameter.getTime()// java.util.Date public long getTime() { return getTimeImpl(); } private final long getTimeImpl() { if (cdate != null &amp;&amp; !cdate.isNormalized()) { normalize(); } return fastTime; } new Timestamp(parameter.getTime()) - (java.sql.Timestamp 는 나노초 정밀도를 갖는다.)// java.sql.Timestamp public Timestamp(long time) { super((time/1000)*1000); nanos = (int)((time%1000) * 1000000); if (nanos &lt; 0) { nanos = 1000000000 + nanos; super.setTime(((time/1000)-1)*1000); } } 정확히 이게 어떤 동작을 하는 것인지 파악하기가 어려워서 코드 실행결과를 확인해보았다. 예시 Date : 2024년 12월 31일 23시 59분 59.123초 Date.getTime() : 1735657199123 // Epoch Time (밀리초 단위까지 표현) new Timestamp(Date.getTime()) Timestamp.getTime() : 1735657199123 // Epoch Time (밀리초 단위까지 표현) Timestamp.getNanos() : 123000000 // 나노초 정밀도까지 표현 (밀리초 단위까지만 갖는 Date로부터 변환되었기 때문에 밀리초 이후로는 0) Date : 2024년 12월 31일 23시 59분 59초.000초 Date.getTime() : 1735657199000 new Timestamp(Date.getTime()) Timestamp.getTime() : 1735657199000 Timestamp.getNanos() : 0 LocalDateTime → Date → Timestamp 변환과정을 요약하면 “LocalDateTime에서의 nano초가 0이면 Date의 밀리초가 0이되고, 이를 기반으로 생성한 Timestamp의 nano초 또한 0이다.”3. 쿼리문에 값 세팅 로그에 찍혔던 2024-12-31 23:59:59, 즉 쿼리문에 실제로 세팅되는 값이 어떤식으로 만들어지는 것인지 확인해보기. 2번에서 살펴본 org.apache.ibatis.type.DateTypeHandler에서 ps.setTimestamp를 타고 들어가다보면 아래 코드가 나옴 com.mysql.jdbc.PreparedStatement.setTimestampInternal 에서 Timestamp의 nanos부분이 0인지 아닌지에 따라 소수점 초가 붙을지 안붙을지 결정된다. 5.6.4버전부터 fractional second 기능이 지원되는 것을 코드 레벨에서 확인할 수도 있다.정리 컴퓨터에서 시간을 다루기 위해 Epoch Time이란 개념을 사용한다. Epoch Time과 더불어 시간을 특정 정밀도 단위까지 표현할 수 있다. LocalDateTime와 Timestamp의 정밀도는 나노초, Date는 밀리초이다. Timestamp의 nanos가 0보다 크면 MySQL 5.6.4버전 이상부터는 소수점 초가 붙어서 쿼리 파라미터에 세팅된다. LocalDateTime의 nano초 부분이 0이면 결과적으로 Timestamp의 nano초 부분도 0이되었다. DATETIME과 같은 날짜/시간 자료형에 정의된 소수점 자릿수를 초과하는 부분은 반올림된다. 이전 포스팅 MySQL에서 시간,날짜 데이터를 저장할 때 반올림되는 현상 (1)" }, { "title": "MySQL에서 시간,날짜 데이터를 저장할 때 반올림되는 현상 (1)", "url": "/posts/mysql-round-up-part1/", "categories": "경험하기, 이슈 노트", "tags": "MySQL", "date": "2023-03-28 22:25:00 +0900", "snippet": "상황 회사에서 판매하는 상품권의 등록만료일시 변경 요청을 받아 작업한 뒤 테스트하던 중 24년 12월 31일 23시 59분 59초로 만료일시를 세팅하려는데 실제 DB에는 25년 1월 1일 0시 0분 0초로 저장되는 현상 발견기존 코드import org.apache.commons.lang.time.DateUtils;public void 상품권생성() ...", "content": "상황 회사에서 판매하는 상품권의 등록만료일시 변경 요청을 받아 작업한 뒤 테스트하던 중 24년 12월 31일 23시 59분 59초로 만료일시를 세팅하려는데 실제 DB에는 25년 1월 1일 0시 0분 0초로 저장되는 현상 발견기존 코드import org.apache.commons.lang.time.DateUtils;public void 상품권생성() { Date 등록만료일시 = DateUtils.setMilliseconds(DateUtils.setSeconds(DateUtils.setMinutes(DateUtils.setHours(DateUtils.addYears(new Date(), 5), 23), 59), 59), 0); 상품권생성정보 info = new 상품권생성정보(); info.setRegisterExpireYmdt(등록만료일시); 중략 ... giftRepository.save(info);}변경 코드 가독성 고려 및 java.util.Date 클래스의 사용은 권장되지 않기 때문에 LocalDateTime 사용 자바에서 다루는 날짜 클래스 관련 참고글 객체 외부에서 인스턴스 변수의 값을 할당하는 것을 지양하고자, setter 메서드 대신 생성자 내에서 값 할당public void 상품권생성() { 상품권생성정보 info = new 상품권생성정보(param1, param2, ...); 중략 ... giftRepository.save(info);}public 상품권생성정보(param1, param2, ...) {\t\t... this.registerExpireYmdt = getExpirationDate();}private Date getExpirationDate() { LocalDateTime 등록만료일시 = LocalDateTime.now() .plusYears(1) .withMonth(12) .withDayOfMonth(31) .withHour(23) .withMinute(59) .withSecond(59); // LocalDateTime -&gt; Date형으로 변환 Instant instant = 등록만료일시.atZone(ZoneId.systemDefault()).toInstant(); return Date.from(instant);} 상품권생성정보의 registerExpireYmdt(인스턴스 변수) 자료형 자체를 LocalDateTime으로 변경하고 싶었으나, 사용되고있는 곳이 많아 이 부분까지 수정하지는 못함원인 파악해보기1. 테이블 스키마 확인 DB : MySQL 5.7.35 등록만료일시 컬럼 자료형 : DATETIME2. 쿼리 로그 확인 상품권생성정보에 대한 INSERT 쿼리에 세팅되는 등록만료일시: 2024-12-31 23:59:59.5823. MySQL DATETIME 관련 공식 문서 확인 TIME, DATETIME, TIMESTAMP 자료형에 대해서 ‘fractional seconds’(분수 초 = 소수점 초?)를 지원한다. 최대 마이크로초(소수점 6자리)까지 지원 컬럼 자료형에 정의된 소수점 자릿수보다 실제 저장하려는 시간의 소수점 자릿수가 더 크면, 초과되는 부분에서 반올림된다. 예시 : 2018-09-08 17:51:04.777 저장 DATETIME(2) : 2018-09-08 17:51:04.78 -&gt; 소수점 세번째 자리에서 반올림됨 DATETIME : 2018-09-08 17:51:05 -&gt; 소수점 첫번쨰 자리에서 반올림됨 이슈 원인 등록만료일시를 저장하는 컬럼의 경우 소수점 초를 정의하지 않은 DATETIME이다. 따라서, 저장하려는 시간에 소수점이 붙으면 소수점 첫번째 자리에서 반올림된다. LocalDateTime.now()의 경우 현재 시각을 소수점 3자리까지 표현한다. (ex : 2024-12-31 23:59:59.582) DB에 2024-12-31 23:59:59로 저장된 경우 : LocalDateTime.now()가 호출된 시점에 소수점 초가 0.5보다 작았다. DB에 2025-01-01 00:00:00로 저장된 경우 : LocalDateTime.now()가 호출된 시점에 소수점 초가 0.5 이상이었다. LocalDateTime은 소수점 9자리까지 표현 가능 ※ 참고 이슈의 원인이었던 ‘fractional seconds’는 MySQL 5.6.4버전부터 지원되는 기능이다. 5.7버전 공식문서 8 버전 공식문서생각해본 해결 방안 쿼리에 세팅되는 등록만료일시의 소수점 초 부분을 0으로 만들기(소수점 부분 제거하기) 초 단위까지 반올림 되지 않도록 datetime 자료형을 datetime(n)으로 재정의하기 datetime(3)이어도 저장되는 시간이 23:59:59.9995 이런경우 결국 초단위까지 반올림된다. 쿼리에 세팅되는 등록만료일시의 소수점 부분 0으로 만들기 ver1LocalDateTime 등록만료일시 = LocalDateTime.now() .plusYears(1) .withMonth(12) .withDayOfMonth(31) .withHour(23) .withMinute(59) .withSecond(59) .withNano(0); // 추가 ver2int nextYear = LocalDateTime.now().getYear() + 1;LocalDateTime 등록만료일시 = LocalDateTime.of(nextYear, 12, 31, 23, 59, 59); // of 메서드를 타고 들어가다보면 결국 nanoSecond를 0으로 세팅하는 부분이 있음 INSERT 쿼리 로그 확인 변경 전 : 2024-12-31 23:59:59.582 변경 후 :2024-12-31 23:59:59 nano초를 일부러 0으로 세팅했다는 것이 명시적으로 좀 더 잘 드러나는 것 같아 ver1 선택 이로써 생각보다 간단히 이슈를 해결할 수 있었다.하지만, 한 가지 의문이 들었던 것은 INSERT 쿼리에 세팅되기 전 등록만료일시는 (jdbc에서 다루는) java.sql.Timestamp 타입이될텐데LocalDateTime의 nano초를 0으로 세팅한 것이 결과적으로 어떻게 Timestamp를 거쳐 쿼리 파라미터의 소수점 초 부분을 없앨 수 있는지였다.그래서 MyBatis와 JDBC 드라이버(mysql-connector-java)가 내부적으로 어떤식으로 값을 변환하고 있는지 디버거를 통해 따라가보았다.해당 글은 다음 포스팅을 통해 살펴보자.다음 포스팅 MySQL에서 시간,날짜 데이터를 저장할 때 반올림되는 현상 (2)" }, { "title": "POST 요청 사이즈 제한으로 인해 겪었던 이슈(http-max-post-size, max-http-form-post-size)", "url": "/posts/http-post-request-body-size-limit/", "categories": "경험하기, 이슈 노트", "tags": "Tomcat, HTTP", "date": "2023-01-10 10:25:00 +0900", "snippet": "상황 클라이언트로부터 상품 수신자 리스트 데이터를 받아 처리하는데, 수신자가 일정 수를 넘어가면 제대로 처리되지 않고 애플리케이션에서 예외 발생함로그 추적1. 아파치 로그Connection reset by peer : ... AH01084: pass request body failed to ...2. 애플리케이션 로그 컨트롤러에 파라미터가 null...", "content": "상황 클라이언트로부터 상품 수신자 리스트 데이터를 받아 처리하는데, 수신자가 일정 수를 넘어가면 제대로 처리되지 않고 애플리케이션에서 예외 발생함로그 추적1. 아파치 로그Connection reset by peer : ... AH01084: pass request body failed to ...2. 애플리케이션 로그 컨트롤러에 파라미터가 null로 넘어옴. 이로 인해 유효성 검사에서 예외 처리됨얻은 힌트 Connection reset by peer (Remote Sever, 즉 톰캣 서버에서 RST 패킷 보내서 커넥션 끊김) pass request body failed request body가 제대로 전달되지 못했다. (이로 인해 파라미터가 null로 넘어온 것 같다.) 추측 : 톰캣쪽에서 뭔가 문제가 발생하는 것 같다.원인 파악 (Spring Boot 내장) 톰캣 관련 설정을 살펴보자server: port: 포트번호 tomcat: uri-encoding: UTF-8 connection-timeout: ... min-spare-threads: ... max-threads: ... max-http-post-size: 3145728 max-connections: ... 왠지 max-http-post-size가 연관있는 설정일 것 같다. 그런데, post size라는게 정확히 어떤 것의 크기를 의미하는걸까 ? (request header를 포함한 전체 사이즈?) 톰캣 공식문서에서는 maxPostSize 속성을 다음과 같이 정의 The maximum size in bytes of the POST which will be handled by the container FORM URL parameter parsing.The limit can be disabled by setting this attribute to a value less than zero.If not specified, this attribute is set to 2097152 (2 megabytes).Note that the FailedRequestFilter can be used to reject requests that exceed this limit. 소스를 찾아보면 Content-Length(Request Body의 크기)라는 것을 알 수 있다. 내장 톰캣 라이브러리(tomcat-embed-core-8.5.29.jar)의 org.apache.catalina.connector.Request#parseParameters 또한, Content-Type이 multipart/form-data이거나 application/x-www-form-urlencoded인 경우에만 사이즈를 제한하는 로직을 타게된다. 확인 결과, 수신자 정보 세팅 요청의 Content-Type은 application/x-www-form-urlencoded이었음 (이름에서 유추할 수 있듯이) POST 요청인 경우에만 제한된다. org.springframework.web.filter.HiddenHttpMethodFilter#doFilterInternal cf) 내장 톰캣 아닌 경우 (server.xml에 maxPostSize지정)&lt;Connector port=\"8080\" protocol=\"HTTP/1.1\" redirectPort=\"8443\" maxPostSize=\"-1\"/&gt;문제 해결1. 수신자 정보 데이터 송신 구조 변경 불필요하게 중복되는 데이터 제거하여 RequestBody 크기 작게 만들기 (아래에서는 설명을 위해 json으로 표시. 실제로는 x-www-form-urlencoded, 즉 key-value 형태로 넘어감) before : 동일한 발신메세지 중복 (상품 발송시 발신메세지는 모두 동일) { \"상품코드\" : ... , \"송신자이름\" : ... , \"수신자목록\" : [ \t {\"예약일시\" : ... , \"휴대폰번호\" : ... , \"발신메세지\" : ...}, {\"예약일시\" : ... , \"휴대폰번호\" : ... , \"발신메세지\" : ...}, ]} after : 발신메세지 항목 따로 분리 { \"상품코드\" : ... , \"송신자이름\" : ... , \"발신메세지\" : ... , \"수신자목록\" : [ \t\t{\"예약일시\" : ... , \"휴대폰번호\" : ... }, {\"예약일시\" : ... , \"휴대폰번호\" : ... }, ]} 2. max-http-post-size 변경 개선된 송신 구조 기준으로 10000건 송신시 약 3.8MB인 것을 감안하여 4194304(bytes, 4MB)로 변경알게된 것 form 태그에 enctype 따로 지정하지 않으면, form submit시 default content-type은 x-www-form-urlencoded enctype 목록 application/x-www-form-urlencoded multipart/form-data text/plain POST 요청이면서 Content-Type이 x-www-form-urlencoded 또는 multipart/form-data인 경우에만 http-max-post-size 옵션이 적용됨 (json은 해당되지 않음) spring-boot의 max-http-post-size 속성은 2.1.x 버전에서부터 Deprecated 되었음 (max-http-form-post-size로 변경) 제기됐던 이슈 html form 요청과 관련해서 적용되는건데 http-max-post-size는 의미가 명확하지 않다. 2.0.x 버전 Document 2.1.x 버전 Document cf) 운영하는 서비스는 spring-boot v2.0.3 사용중이었음" }, { "title": "Java - JDBC 알아보기", "url": "/posts/what-is-jdbc/", "categories": "지식 더하기, 이론", "tags": "Java", "date": "2022-04-19 00:29:00 +0900", "snippet": "JDBC란? 공식 문서에서는 다음과 같이 설명한다. JDBC(Java Database Connectivity) API는 Java에서 범용 데이터 액세스를 제공한다.즉, JDBC API를 사용하면 관계형 데이터베이스, 스프레드시트 및 플랫 파일에 이르기까지 거의 모든 데이터 소스에 액세스할 수 있다. 한 마디로, JDBC는 데이터소스(특히 데이터베이...", "content": "JDBC란? 공식 문서에서는 다음과 같이 설명한다. JDBC(Java Database Connectivity) API는 Java에서 범용 데이터 액세스를 제공한다.즉, JDBC API를 사용하면 관계형 데이터베이스, 스프레드시트 및 플랫 파일에 이르기까지 거의 모든 데이터 소스에 액세스할 수 있다. 한 마디로, JDBC는 데이터소스(특히 데이터베이스)에 연결하여 쿼리를 실행하기 위한 API이다. JDBC API는 두 개의 패키지로 구성된다. java.sql javax.sql JDBC가 나오게 된 배경 모든 데이터베이스에는 해당 데이터베이스와 상호 작용하는 프로그램을 작성하기 위해 필요한 자체 API가 있다. 따라서 둘 이상의 공급업체에서 제공하는 데이터베이스와 상호 작용할 수 있는 코드를 작성하는 것은 어려운 과제다. 마이크로소프트의 ODBC API와 같은 데이터베이스 간 API는 존재했지만, 이들은 특정 플랫폼에 국한되는 경향이 있었다.JDBC 사용시 이점 JDBC는 데이터베이스와 Java 사이에 플랫폼 중립적인 인터페이스를 만들기 위함이다. 즉, JDBC API는 쿼리 실행, 결과 처리 및 구성 정보 결정을 포함하여 주요 데이터베이스 기능을 캡슐화하는 인터페이스 세트를 정의한다. 따라서, JDBC를 사용하면 표준 데이터베이스 액세스 기능과 SQL의 특정 하위 집합인 SQL-92를 사용할 수 있다. 데이터베이스 공급업체 또는 타사 개발자가 특정 데이터베이스 시스템에 대해 이러한 인터페이스를 구현하는 클래스 집합인 JDBC 드라이버를 작성한다. 결과적으로, 자바 애플리케이션은 여러 JDBC 드라이버를 바꿔가면서 사용할 수 있다. 출처 : https://flylib.com/books/en/2.177.1.75/1/코드 레벨에서 살펴보기 위 그림을 좀 더 간략하게 만들면 아래와 같을 것이다. 출처 : https://javapapers.com/jdbc/jdbc-introduction/ 예를 들어, 자바 프로그램을 MySQL DB와 연동하기 위해서는 MySQL을 위한 JDBC 드라이버가 필요할 것이며, 아래와 같이 (gradle)프로젝트에 해당 드라이버 의존성을 추가해 줄 수 있다. implementation 'mysql:mysql-connector-java:8.0.28' DB와 연결하는 다음 코드를 살펴보자.Connection con = DriverManager.getConnection(\"jdbc:mysql://localhost:3306/dbname\", \"user\", \"pw\")` DriverManager를 통해 얻어오는 Connection의 실제 구현체는 com.mysql.cj.jdbc 패키지(mysql-connector-java 라이브러리에 있는)의 ConnectionImpl객체이다. 실제로 해당 클래스의 클래스 다이어그램을 살펴보면, 상위에 JDBC 인터페이스(java.sql.Connection)를 구현하고 있음을 볼 수 있다. JDBC 버전 출시년도 JDBC 버전 JSR 스펙 JDK 2017 JDBC 4.3 JSR 221 Java SE 9 2014 JDBC 4.2 JSR 221 Java SE 8 2011 JDBC 4.1 JSR 221 Java SE 7 2006 JDBC 4.0 JSR 221 Java SE 6 2001 JDBC 3.0 JSR 54 JDK 1.4 1999 JDBC 2.1   JDK 1.2 1997 JDBC 1.2   JDK 1.1 JDBC 드라이버 위에서 그림으로 살펴봤듯이, JDBC 드라이버는 특정 유형의 데이터베이스에 연결하는 데 사용되는 JDBC API 구현이다. 특정 DBMS에서 JDBC API를 사용하려면 JDBC 기술과 데이터베이스 사이를 중재할 JDBC 드라이버가 필요하다. 다양한 요인에 따라 드라이버는 Java로만 작성되거나, Java와 JNI(Java Native Interface) 네이티브 메소드를 혼합하여 작성될 수 있다.네 가지 타입의 JDBC 드라이버Type 1 : JDBC-ODBC bridge driver JDBC-ODBC 브리지 드라이버는 ODBC 드라이버를 사용하여 데이터베이스에 연결한다. JDBC-ODBC 브리지 드라이버는 JDBC 메서드 호출을 ODBC 함수 호출로 변환한다. Thin 드라이버(Type4)의 출현으로 이제는 권장되지 않는 방법이다. Oracle은 Java 8부터 JDBC-ODBC Bridge를 지원하지 않는다. Oracle은 JDBC-ODBC Bridge 대신 데이터베이스 벤더에서 제공하는 JDBC 드라이버를 사용할 것을 권장한다. 출처 : https://www.javatpoint.com/jdbc-driverType 2 : Native-API driver (부분적으로 자바로 작성됨) Native API 드라이버는 데이터베이스의 클라이언트 측 라이브러리를 사용한다. 예를 들어, Oracle 데이터베이스의 경우 원래 C/C++ 프로그래머용으로 설계된 Oracle Call Interface(OCI) 라이브러리를 통해 액세스가 이루어질 수 있다. Native API 드라이버는 JDBC 메소드 호출을 API 데이터베이스의 기본 호출로 변환한다. 출처 : https://www.javatpoint.com/jdbc-driverType 3 : Network Protocol driver (자바로만 작성됨) 네트워크 프로토콜 드라이버는 JDBC 호출을 벤더별 데이터베이스 프로토콜로 직/간접적으로 변환하는 미들웨어(애플리케이션 서버)를 사용한다. 출처 : https://www.javatpoint.com/jdbc-driverType 4 : Thin driver (자바로만 작성됨) Thin 드라이버는 JDBC 호출을 벤더별 데이터베이스 프로토콜로 직접 변환한다. 따라서, 얇은(thin) 드라이버로 알려져 있다. 가장 많이 사용되는 타입으로, 플랫폼에 독립적이라는 장점이 있다. 또한, 데이터베이스 서버에 직접 접속하기 때문에 다른 타입에 비해 더 나은 성능을 제공한다. 데이터베이스별 네트워킹 프로토콜을 이해하고 추가 소프트웨어 없이 데이터베이스에 직접 액세스할 수 있다. 출처 : https://www.javatpoint.com/jdbc-driverJDBC URL JDBC 드라이버는 JDBC URL을 사용하여 특정 데이터베이스를 식별하고 연결한다. 이러한 URL은 일반적으로 다음과 같은 형식이다. jdbc:driver:databasename 예 : jdbc:mysql://localhost:3306/dbname 참고 자료 https://www.baeldung.com/java-jdbc https://en.wikipedia.org/wiki/Java_Database_Connectivity https://docs.oracle.com/javase/8/docs/technotes/guides/jdbc/ http://www.herongyang.com/JDBC/Overview-JDBC-Version.html https://flylib.com/books/en/2.177.1.75/1/ https://www.javatpoint.com/jdbc-driver" }, { "title": "Infra - 톰캣 구조 살펴보기", "url": "/posts/tomcat-server-xml-file/", "categories": "지식 더하기, 이론", "tags": "Infra", "date": "2022-04-07 22:00:00 +0900", "snippet": "톰캣 구조 출처 : https://howtodoinjava.com/tomcat/tomcats-architecture-and-server-xml-configuration-tutorial/ 이러한 구조는 일반적으로 Tomcat 설치 폴더의 /conf 하위 디렉토리에 있는 server.xml 파일에 정의된다.server.xml 예시&lt;?xml...", "content": "톰캣 구조 출처 : https://howtodoinjava.com/tomcat/tomcats-architecture-and-server-xml-configuration-tutorial/ 이러한 구조는 일반적으로 Tomcat 설치 폴더의 /conf 하위 디렉토리에 있는 server.xml 파일에 정의된다.server.xml 예시&lt;?xml version='1.0' encoding='utf-8'?&gt;&lt;Server port=\"8005\" shutdown=\"SHUTDOWN\"&gt; &lt;Listener className=\"org.apache.catalina.core.AprLifecycleListener\" SSLEngine=\"on\" /&gt; &lt;Listener className=\"org.apache.catalina.core.JasperListener\" /&gt; &lt;Listener className=\"org.apache.catalina.core.JreMemoryLeakPreventionListener\" /&gt; &lt;Listener className=\"org.apache.catalina.mbeans.GlobalResourcesLifecycleListener\" /&gt; &lt;Listener className=\"org.apache.catalina.core.ThreadLocalLeakPreventionListener\" /&gt; &lt;GlobalNamingResources&gt; &lt;Resource name=\"UserDatabase\" auth=\"Container\" type=\"org.apache.catalina.UserDatabase\" description=\"User database that can be updated and saved\" factory=\"org.apache.catalina.users.MemoryUserDatabaseFactory\" pathname=\"conf/tomcat-users.xml\" /&gt; &lt;/GlobalNamingResources&gt; &lt;Service name=\"Catalina\"&gt; &lt;Connector port=\"8080\" protocol=\"HTTP/1.1\" connectionTimeout=\"20000\" redirectPort=\"8443\" /&gt; &lt;Connector port=\"8009\" protocol=\"AJP/1.3\" redirectPort=\"8443\" /&gt; &lt;Engine name=\"Catalina\" defaultHost=\"localhost\"&gt; &lt;Realm className=\"org.apache.catalina.realm.LockOutRealm\"&gt; &lt;Realm className=\"org.apache.catalina.realm.UserDatabaseRealm\" resourceName=\"UserDatabase\"/&gt; &lt;/Realm&gt; &lt;Host name=\"localhost\" appBase=\"webapps\" unpackWARs=\"true\" autoDeploy=\"true\"&gt; &lt;Valve className=\"org.apache.catalina.valves.AccessLogValve\" directory=\"logs\" prefix=\"localhost_access_log.\" suffix=\".txt\" pattern=\"%h %l %u %t &amp;quot;%r&amp;quot; %s %b\" /&gt; &lt;/Host&gt; &lt;/Engine&gt; &lt;/Service&gt;&lt;/Server&gt;각 구성요소 살펴보기Context Context는 특정 가상 호스트 내에서 실행되는 웹 애플리케이션을 나타낸다. 웹 애플리케이션은 WAR(Web Application Archive) 파일 또는 unpack된 콘텐츠를 포함하는 디렉터리를 기반으로 한다. (버전 2.2 이상 서블릿 스펙에 정의되어 있음) 각 HTTP 요청을 처리하는 데 사용되는 웹 애플리케이션은 정의된 각 컨텍스트의 컨텍스트 경로(context path)에 대한 요청 URI의 가능한 가장 긴 접두사를 일치시키는 것을 기반으로 Catalina에 의해 선택된다. 일단 선택되면 해당 컨텍스트는 웹 애플리케이션 배포에 의해 정의된 서블릿 매핑에 따라 들어오는 요청을 처리할 적절한 서블릿을 선택한다. 원하는 만큼 Context 엘리먼트를 정의할 수 있다. 각각의 컨텍스트는 반드시 가상 호스트 내에서 고유한 컨텍스트 이름을 가져야 한다. Context는 길이가 0인 문자열(““)로 된 컨텍스트 경로가 있어야 한다. 이 컨텍스트는 이 가상 호스트의 기본 웹 애플리케이션이 되며, 다른 컨텍스트의 context path와 일치하지 않는 모든 요청을 처리하는 데 사용된다. 컨텍스트 정의하기 &lt;Context&gt; 엘리먼트를 server.xml 파일에 직접 배치하지 않는 것이 좋다. Tomcat을 재시작해야지만 conf/server.xml 파일을 다시 로드할 수 있기 때문에, Context 설정을 수정하기가 어려워진다. 개별 컨텍스트는 다음과 같이 명시적으로 정의될 수 있다. 애플리케이션 파일 내부의 /META-INF/context.xml에 있는 개별 파일에서. 선택적으로(호스트의 copyXML 속성에 따라) 이것은 $CATALINA_BASE/conf/[enginename]/[hostname]/에 복사될 수 있으며 애플리케이션의 기본 파일 이름과 .xml 확장자로 이름을 바꿀 수 있다. $CATALINA_BASE/conf/[enginename]/[hostname]/ 디렉토리의 개별 파일(.xml 확장자 포함). 컨텍스트 경로 및 버전은 파일의 기본 이름(파일 이름에서 .xml 확장자를 뺀 값)에서 파생된다. 이 파일은 웹 애플리케이션의 META-INF 디렉토리에 패키징된 모든 context.xml 파일보다 항상 우선된다. 메인 conf/server.xml의 Host 엘리먼트 내부에 Context 엘리먼트로 정의. Context 속성 살펴보기 docBase 웹 애플리케이션의 Document Base(Context Root) 디렉토리 또는 war 파일의 경로 이름(애플리케이션이 WAR 파일에서 직접 실행되는 경우). 이 디렉토리 또는 WAR 파일에 대한 절대 경로 이름을 지정하거나 소유 호스트의 appBase 디렉토리에 상대적인 경로 이름을 지정할 수 있다. Context 엘리먼트가 server.xml에 정의되어 있거나 docBase가 호스트의 appBase 아래에 있지 않은 경우 이 필드의 값을 설정해서는 안된다. docBase에 심볼릭 링크가 사용되는 경우 심볼릭 링크에 대한 변경 사항은 Tomcat을 다시 시작한 후 또는 컨텍스트를 배포 취소하고 재배포하는 경우에만 적용된다. path 처리할 적절한 웹 애플리케이션을 선택하기 위해, 각 요청 URI의 시작 부분과 일치하는 웹 애플리케이션의 컨텍스트 경로 특정 호스트 내의 모든 컨텍스트 경로는 고유해야 한다. 빈 문자열(““)의 컨텍스트 경로를 지정하면 다른 컨텍스트에 할당되지 않은 모든 요청을 처리하는 이 호스트에 대한 기본 웹 애플리케이션을 정의하는 것이다. path 속성은 server.xml에서 Context를 정적으로(statically) 정의할 때만 사용해야 한다. 다른 모든 상황에서 경로는 애플리케이션 별 .xml 파일 또는 docBase에 사용된 파일 이름에서 유추된다. Context를 server.xml에 정적으로 정의하는 경우에도 docBase가 호스트의 appBase 아래에 있지 않거나 deployOnStartup 및 autoDeploy가 모두 false인 경우가 아니면 이 속성을 설정해서는 안된다. 이 규칙을 따르지 않을 경우 이중 배포가 발생할 수 있다. Connector Connector는 클라이언트와의 통신을 처리한다. Tomcat에는 여러 커넥터를 사용할 수 있다. HTTP connector : 대부분의 HTTP 트래픽 처리 AJP connector : Tomcat을 Apache HTTPD 서버와 같은 웹 서버에 연결할 때 사용(AJP 프로토콜 구현) AJP 커넥터를 사용하면 Tomcat이 동적 웹 페이지만 처리하고, 순수 HTML 서버(예: Apache 웹 서버)가 정적 페이지에 대한 요청을 처리할 수 있다. 웹 서버를 Tomcat과 함께 사용할 계획이 없다면 이 커넥터를 주석 처리할 수 있다. AJP 커넥터를 사용하면 Tomcat이 동적 웹 페이지만 처리하고 순수 HTML 서버(예: Apache 웹 서버)가 정적 페이지에 대한 요청을 처리할 수 있다. Tomcat 자체가 오늘날 매우 빠르거나 단순히 웹 서버를 Tomcat과 함께 사용할 계획이 없다면 이 커넥터를 주석 처리할 수 있습니다. 모든 애플리케이션에 대한 요청은 커넥터의 단일 인스턴스를 통과한다. 요청을 받았을 때, 커넥터 내에 alive 상태로 남아있는 새 스레드가 인스턴스화된다(요청이 처리되는 동안). 커넥터는 “Coyote”로 불리기도 한다.Connector 속성 살펴보기 port 이 커넥터가 서버 소켓을 만들고 들어오는 연결을 기다리는 TCP 포트 번호 운영 체제는 하나의 서버 애플리케이션만 특정 IP 주소의 특정 포트 번호를 수신하도록 허용한다. 0을 사용하는 경우 Tomcat은 이 커넥터에 사용할 여유 포트를 임의로 선택한다. connectionTimeout 정의된 시간동안 활동이 없으면 세션이 종료됨을 의미한다. (단위 : 초) redirectPort SSL(Secure Socket Layer) 전송이 필요한 요청은 정의된 포트로 리디렉션 된다. address IP 주소가 두 개 이상인 서버의 경우, 지정된 포트에서 수신 대기하는 데 사용할 주소를 지정한다. 기본적으로 이 포트는 서버와 연결된 모든 IP 주소에서 사용된다. 127.0.0.1 값은 커넥터가 루프백 인터페이스에서만 수신 대기함을 나타낸다. secret(tomcat 9 이상) secret 키워드가 있는 worker의 요청만 수락한다. 기본값은 null이며, secretRequired가 명시적으로 false로 구성되지 않는 한 이 속성은 null이 아니고 길이가 0이 아닌 값으로 지정되어야 한다. 이 속성이 null, 길이가 0이 아닌 값으로 구성된 경우, 작업자는 worker는 일치하는 값을 제공해야 한다. 그렇지 않으면 secretRequired 설정에 관계없이 요청이 거부된다. secretRequired(tomcat 9 이상) 이 속성이 true이면 secret 속성이 null과 길이가 0이 아닌 값으로 구성된 경우에만 AJP 커넥터가 시작된다. 이 속성은 커넥터가 신뢰할 수 있는 네트워크에서 사용될 때만 false로 설정되어야 한다. Host Host는 서버의 네트워크 이름(예: www.example.com)을 Tomcat이 실행되고 있는 특정 서버와 연결하는 가상 호스트를 나타낸다. 클라이언트가 네트워크 이름을 사용하여 Tomcat 서버에 연결할 수 있으려면, 이 이름을 사용자가 속한 인터넷 도메인을 관리하는 DNS(Domain Name Service) 서버에 등록해야 한다. 대부분의 경우 시스템 관리자는 둘 이상의 네트워크 이름(예: www.mycompany.com 및 company.com)을 동일한 가상 호스트 및 애플리케이션에 연결하려고 합니다. 이 작업은 호스트 이름 별칭 기능을 사용하여 수행할 수 있다. 하나 이상의 Host 엘리먼트가 Engine 엘리먼트 내부에 정의될 수 있다. Host 엘리먼트 내에 가상 호스트와 연결된 웹 애플리케이션에 대한 Context 엘리먼트가 하나 이상 정의될 수 있다. 각 엔진과 관련된 호스트 중 정확히 하나는 반드시 해당 엔진의 defaultHost 속성과 일치하는 이름을 가져야 한다. 클라이언트는 일반적으로 호스트 이름을 사용하여 연결하려는 서버를 식별한다. 이 호스트 이름은 HTTP 요청 헤더에도 포함된다. Tomcat은 HTTP 헤더에서 호스트 이름을 추출하고 이름이 일치하는 호스트를 찾는다. 일치하는 항목이 없으면 요청이 기본 호스트로 라우팅된다. DNS 이름이 Host 엘리먼트의 이름과 일치하지 않는 모든 요청은 기본 호스트로 라우팅되기 때문에, 기본 호스트의 이름은 DNS 이름과 일치할 필요가 없다(가능하더라도). CATALINA_BASE 디렉토리를 설정하여 여러 인스턴스에 대해 Tomcat을 구성하지 않은 경우 $CATALINA_BASE는 Tomcat을 설치한 디렉토리인 $ CATALINA_HOME의 값으로 설정된다.Host 속성 살펴보기 appBase Tomcat 설치 폴더 내의 애플리케이션 디렉터리를 정의한다. 각 애플리케이션은 해당 디렉터리 내의 경로로 식별된다. 유일한 예외는 빈 문자열에 매핑되는 ROOT 경로입이다. localhost의 애플리케이션 기본 디렉토리는 webapps다. 즉, 톰캣 설치 경로\\webapps\\ROOT\\ 디렉토리에 있는 애플리케이션은 빈 문자열로 식별된다. ex : \"http://localhost:8080/ 톰캣 설치 경로\\webapps\\myapp\\와 같이 ROOT 이외의 디렉터리에 있는 다른 애플리케이션의 경우 URL은 http://localhost:8080/myapp/로 식별된다. The Application Base directory for this virtual host. This is the pathname of a directory that may contain web applications to be deployed on this virtual host. You may specify an absolute pathname, or a pathname that is relative to the $CATALINA_BASE directory. See Automatic Application Deployment for more information on automatic recognition and deployment of web applications. If not specified, the default of webapps will be used. unpackWARs true로 설정하고 war 파일을 appBase 디렉토리에 놓으면, Tomcat이 자동으로 압축을 해제하여 일반 폴더로 확장한다. false로 설정하면 애플리케이션이 war 파일에서 직접 실행된다. 애플리케이션 실행 시 압축을 풀어야 하기 때문에, true일 때 보다 실행 속도는 느려지게 된다. autoDeploy true로 설정되면 Tomcat이 실행되는 동안 appBase 디렉토리에 애플리케이션을 놓으면 자동으로 배포된다. Engine Engine은 특정 서비스에 대한 요청 처리 파이프라인을 나타낸다. 엔진은 서비스에 정의된 모든 커넥터의 요청을 수신 및 처리하고, 응답을 클라이언트로 전송하기 위해 적절한 커넥터에 다시 전달한다. 엔진은 하나 이상의 호스트를 포함해야 하며 그 중 하나는 기본 호스트로 지정된다. 기본 Tomcat 구성에는 호스트 localhost를 포함하는 엔진 Catalina가 포함된다.(기본 호스트가 유일한 호스트이기 때문에 기본 호스트로 지정됨) Catalina 엔진은 HTTP 커넥터를 통해 수신된 모든 수신 요청을 처리하고 해당 응답을 다시 보낸다. 요청 헤더에 포함된 정보를 기반으로 각 요청을 올바른 호스트 및 컨텍스트로 전달한다.Service Service는 요청을 처리하기 위해 엔진 엘리먼트를 공유하는 하나 이상의 커넥터 엘리먼트의 조합을 나타낸다. Tomcat의 기본 구성에는 HTTP 및 AJP 커넥터를 Catalina 엔진에 연결하는 Catalina 서비스가 포함된다. 기본 구현이 간단하고 충분하기 때문에 서비스는 사용자에 의해 거의 커스터마이징 되지 않는다.Server Server는 Catalina 서블릿 컨테이너를 나타낸다. 고유한 엔진과 커넥터가 있는 하나 이상의 서비스를 포함할 수 있다. conf/server.xml 구성 파일에서 가장 바깥쪽의 단일 요소여야 한다. 이 속성은 서블릿 컨테이너 전체의 특성을 나타낸다.Server 속성 살펴보기 port 서버가 종료(shutdown) 명령을 기다리는 TCP/IP 포트 번호 shutdown 포트를 비활성화하려면 -1로 설정한다. 종료 포트 비활성화는 Apache Commons Daemon을 사용하여 Tomcat을 시작할 때 잘 작동합니다(Windows에서 서비스로 실행되거나 un * xes에서 jsvc로 실행). 표준 셸 스크립트로 Tomcat을 실행할 때는 사용할 수 없다. 이는 shutdown bat|.sh 및 catalina.bat|.sh가 정상적으로 중지하는 것을 방지하기 때문이다. shutdown Tomcat을 종료하기 위해, 지정된 포트 번호에 TCP/IP 연결을 통해 수신해야 하는 명령 문자열 참고 자료 https://howtodoinjava.com/tomcat/tomcats-architecture-and-server-xml-configuration-tutorial/ https://tomcat.apache.org/tomcat-8.0-doc/config/context.html https://tomcat.apache.org/tomcat-8.0-doc/config/host.html https://tomcat.apache.org/tomcat-8.0-doc/config/server.html https://tomcat.apache.org/tomcat-8.0-doc/config/service.html https://tomcat.apache.org/tomcat-8.0-doc/config/ajp.html" }, { "title": "IntelliJ - 프로젝트, 모듈이란 ?", "url": "/posts/project-and-module/", "categories": "지식 더하기, 기타", "tags": "IDE", "date": "2022-04-02 20:25:00 +0900", "snippet": "Project 프로젝트는 애플리케이션을 구성하는 모든 것(모듈, 종속성, 공통 설정 등)을 보관하는 디렉토리이다. 일반적으로 프로젝트에는 관련 모듈, 애플리케이션, 라이브러리 등이 포함되며, 이를 통해 상호 의존적으로 협력하여 개발이 가능해진다. 출처 : https://www.jetbrains.com/help/idea/working-with...", "content": "Project 프로젝트는 애플리케이션을 구성하는 모든 것(모듈, 종속성, 공통 설정 등)을 보관하는 디렉토리이다. 일반적으로 프로젝트에는 관련 모듈, 애플리케이션, 라이브러리 등이 포함되며, 이를 통해 상호 의존적으로 협력하여 개발이 가능해진다. 출처 : https://www.jetbrains.com/help/idea/working-with-projects.html#settings-types Project 설정 프로젝트 설정은 .xml 형식으로 .idea 디렉토리에 다른 프로젝트 파일과 함께 저장된다. 예를 들어, VCS 설정, SDK, 코드 스타일 및 맞춤법 검사기 설정, 컴파일러 출력, 프로젝트 내의 모든 모듈에 사용할 수 있는 라이브러리 등을 세팅한다.Project SDK SDK는 특정 소프트웨어 프레임워크용 애플리케이션을 개발하는 데 필요한 도구 모음이다. 필요한 SDK가 컴퓨터에 설치되어 있지만 IDE에 정의되어 있지 않은 경우, Add SDK | 'SDK name'을 지정하고 SDK 홈 디렉터리의 경로를 지정한다. 예를 들어, Java 기반 애플리케이션을 개발하려면 JDK(Java Development Kit)가 필요하다.Project language level Language level은 편집기가 제공하는 코딩 지원 기능을 정의한다. 예를 들어 JDK 9를 사용하고 언어 레벨을 8로 설정할 수 있다. 이렇게 하면 바이트코드가 Java 8과 호환되는 반면, 검사(inspection)에서는 Java 9의 구성을 사용하지 않도록한다. Language level은 컴파일에도 영향을 준다. 컴파일러((Settings/Preferences | Build, Execution, Deployment | Compiler | Java Compiler))의 타겟 바이트코드 버전을 따로 세팅하지 않으면, 타겟 바이트코드 버전은 Language level로 간주된다. Project compiler output 컴파일러 출력 경로는 IntelliJ IDEA가 컴파일 결과를 저장하는 디렉토리 경로이다. 이 디렉터리에서 IDE는 두 개의 하위 디렉터리를 만든다. production : 프로덕션 코드 test : 테스트 코드 하위 디렉토리에서 각 모듈에 대한 개별 출력 디렉토리가 생성된다.Module IntelliJ IDEA에서 모듈은 모든 프로젝트의 필수적인 부분으로써, 프로젝트와 함께 자동으로 생성된다. 프로젝트는 여러 모듈을 포함할 수 있다(멀티 모듈 프로젝트). 또한, 새 모듈을 추가하고 그룹화하고 현재 필요하지 않은 모듈을 언로드할 수 있다. 일반적으로 모듈은 내부 설정을 유지하는 .iml 파일과 소스 코드, 리소스, 테스트 등을 저장하는 content root로 구성된다. content root 없이 모듈이 존재할 수도 있다. 각 모듈은 자체 프레임워크를 담당할 수 있다. 각 모듈은 별도의 라이브러리, webapp 등이 될 수 있다. 각 모듈은 패키지된 코드의 단일 유형(jar 파일, war 파일 등)용이다. 출처 : https://www.jetbrains.com/help/idea/working-with-projects.html#settings-types 참고 자료 https://www.jetbrains.com/help/idea/working-with-projects.html#settings-types https://www.jetbrains.com/help/idea/working-with-projects.html https://www.jetbrains.com/help/idea/project-settings-and-structure.html https://www.jetbrains.com/help/idea/creating-and-managing-modules.html https://intellij-support.jetbrains.com/hc/en-us/community/posts/206887325-Difference-between-project-and-module" }, { "title": "group by, order by, 드라이빙 테이블 수정을 통한 쿼리 성능 개선하기", "url": "/posts/query-tuning-1/", "categories": "경험하기, 작업 노트", "tags": "MySQL", "date": "2022-03-20 23:00:00 +0900", "snippet": "들어가기 전회사에서 개발하면서 처음으로 슬로우 쿼리를 개선해보았다. 제대로 한건지는 모르겠지만 나름대로 공부해서 7초에서 0.4초까지 개선되었다.그 과정을 간략하게 적어보고 앞으로 또 다른 슬로우 쿼리를 만나게되면 기록할 예정이다.쿼리 튜닝 과정 최초 쿼리 수행 시간 : 약 7초 최초 쿼리는 다음과 같다. (문제가 됐던 부분 기록, 테이블명, 컬럼...", "content": "들어가기 전회사에서 개발하면서 처음으로 슬로우 쿼리를 개선해보았다. 제대로 한건지는 모르겠지만 나름대로 공부해서 7초에서 0.4초까지 개선되었다.그 과정을 간략하게 적어보고 앞으로 또 다른 슬로우 쿼리를 만나게되면 기록할 예정이다.쿼리 튜닝 과정 최초 쿼리 수행 시간 : 약 7초 최초 쿼리는 다음과 같다. (문제가 됐던 부분 기록, 테이블명, 컬럼명은 보안상 변경) SELECT (SELECT GROUP_CONCAT(tmp.cnt) FROM (SELECT og.컬럼1, count(gu.컬럼2) cnt FROM tbl5 og LEFT OUTER JOIN (SELECT 컬럼1, 컬럼2 FROM tbl6 WHERE 컬럼3 IS NOT NULL) gu ON og.컬럼1 = gu.컬럼1 and og.컬럼2 = gu.컬럼2 GROUP BY og.컬럼2, og.컬럼1 ORDER BY og.컬럼2) tmp WHERE tmp.컬럼1 = tbl1.컬럼1) as regi_qty, ... FROM tbl1 LEFT OUTER JOIN tbl2 ON tbl1.컬럼1 = tbl2.컬럼1 LEFT OUTER JOIN tbl3 ON tbl1.컬럼1 = tbl3.컬럼1 LEFT OUTER JOIN tbl4 ON tbl1.컬럼4 = tbl4.컬럼4 WHERE 1=1 AND tbl1.등록일 &gt;= '2021-09-20' AND tbl1.등록일 &lt; ADDDATE('2022-03-20',1) AND tbl1.주문타입 IN ( '값1' , '값2' );실행계획 문제가 될만한 부분이라고 생각했던 부분에 표시를 했다. og 테이블 인덱스 풀스캔 그로 인해 임시 테이블인 derived7 테이블을 만들기 위해 조인시 많은 레코드에 접근 임시 테이블 생성 및 정렬 (Using temporary, Using filesort) 튜닝1 임시 테이블 생성 및 정렬하는 부분을 제거했다. (수행 시간 : 약 3.5초) MySQL은 GROUP BY, ORDER BY 처리를 위해 임시 테이블을 만들기 때문에, Using temporary, Using filesort가 나오게 된다. 인덱스는 이미 정렬되어 있기 때문에, GROUP BY, ORDER BY를 인덱스 기준으로 하면 위해 임시 테이블을 만들 필요가 없다. og 테이블의 인덱스는 (컬럼1,컬럼2)로 구성되었기 때문에, GROUP BY, ORDER BY에 명시된 컬럼 순서를 인덱스가 구성된 순서로 변경했다. 변경된 쿼리 SELECT (SELECT GROUP_CONCAT(tmp.cnt) FROM (SELECT og.컬럼1, count(gu.컬럼2) cnt FROM tbl5 og LEFT OUTER JOIN (SELECT 컬럼1, 컬럼2 FROM tbl6 WHERE 컬럼3 IS NOT NULL) gu ON og.컬럼1 = gu.컬럼1 and og.컬럼2 = gu.컬럼2 GROUP BY og.컬럼1, og.컬럼2 ORDER BY og.컬럼1, og.컬럼2) tmp WHERE tmp.컬럼1 = tbl1.컬럼1) as regi_qty, ... FROM tbl1 LEFT OUTER JOIN tbl2 ON tbl1.컬럼1 = tbl2.컬럼1 LEFT OUTER JOIN tbl3 ON tbl1.컬럼1 = tbl3.컬럼1 LEFT OUTER JOIN tbl4 ON tbl1.컬럼4 = tbl4.컬럼4 WHERE 1=1 AND tbl1.등록일 &gt;= '2021-09-20' AND tbl1.등록일 &lt; ADDDATE('2022-03-20',1) AND tbl1.주문타입 IN ( '값1' , '값2' ); Using temporary, Using filesort가 없어진 것을 볼 수 있다.튜닝2 드라이빙 테이블(og)에서 인덱스 풀스캔을 하지 않도록 변경했다. (수행 시간 : 약 0.4초) 드라이빙 테이블, 드리븐 테이블 중 하나라도 인덱스 풀스캔을 하게되면 결과적으로 조인 횟수가 많아지기 때문에, 풀스캔 하지 않는 방법을 찾아보았다. og, gu 테이블로 인해 생성되는 임시 테이블 tmp은 필터 조건(등록일, 주문타입)으로 많은 레코드가 걸러진 tbl1의 컬럼1이 필터조건이 된다. 즉, tmp 테이블은 tbl1을 컬럼1을 기준으로 많은 부분이 걸러지게된다. 따라서, og 테이블에서 애초에 모든 레코드를 가져올 필요 없이 tbl1의 필터 조건을 활용할 수 없을지 생각해보았다. og 테이블 인덱스는 (컬럼1,컬럼2)인데 컬럼1의 경우, 년월일+a의 규칙을 갖고 생성되기 때문에, 등록일 부분을 활용하면 될 것으로 생각했다. 변경된 쿼리 SELECT (SELECT GROUP_CONCAT(tmp.cnt) FROM (SELECT og.컬럼1, count(gu.컬럼2) cnt FROM (SELECT 컬럼1, 컬럼2 FROM tbl5 WHERE 컬럼1 BETWEEN '20210920' AND '20220321') og LEFT OUTER JOIN (SELECT 컬럼1, 컬럼2 FROM tbl6 WHERE 컬럼3 IS NOT NULL) gu ON og.컬럼1 = gu.컬럼1 and og.컬럼2 = gu.컬럼2 GROUP BY og.컬럼1, og.컬럼2 ORDER BY og.컬럼1, og.컬럼2) tmp WHERE tmp.컬럼1 = tbl1.컬럼1) as regi_qty, ... FROM tbl1 LEFT OUTER JOIN tbl2 ON tbl1.컬럼1 = tbl2.컬럼1 LEFT OUTER JOIN tbl3 ON tbl1.컬럼1 = tbl3.컬럼1 LEFT OUTER JOIN tbl4 ON tbl1.컬럼4 = tbl4.컬럼4 WHERE 1=1 AND tbl1.등록일 &gt;= '2021-09-20' AND tbl1.등록일 &lt; ADDDATE('2022-03-20',1) AND tbl1.주문타입 IN ( '값1' , '값2' ); 인덱스 레인지 스캔으로 변경됨과 동시에, (정확한 수치는 아니지만) 접근하는 레코드가 확연히 줄어든 것을 볼 수 있다." }, { "title": "Apache Http Server 2.2.x -> 2.4.x 버전 업그레이드 하면서 겪었던 이슈", "url": "/posts/apache-2.2-to-2.4-issue/", "categories": "경험하기, 이슈 노트", "tags": "Apache", "date": "2022-02-18 00:25:00 +0900", "snippet": "상황최근 개발 서버 이관 작업을 진행하면서 새로운 서버에는 기존에 사용하던 아파치 2.2.27 대신 2.4.33 버전을 설치해서 작업을 진행했다.그리고 기존 서버의 httpd.conf 파일과 가상 호스트 파일들(한 대의 서버에서 여러 서비스 제공)을 옮겨와서 세팅한 뒤, 새로운 서버의 아파치를 실행(apachectl start)하려고 하는데, 에러가 ...", "content": "상황최근 개발 서버 이관 작업을 진행하면서 새로운 서버에는 기존에 사용하던 아파치 2.2.27 대신 2.4.33 버전을 설치해서 작업을 진행했다.그리고 기존 서버의 httpd.conf 파일과 가상 호스트 파일들(한 대의 서버에서 여러 서비스 제공)을 옮겨와서 세팅한 뒤, 새로운 서버의 아파치를 실행(apachectl start)하려고 하는데, 에러가 발생했다.그리고 그 에러를 고치면 또 다른 에러들을 만났다.원인과 해결방법 아파치 2.2 -&gt; 2.4 버전으로 업그레이드 시 유의해야하는 부분들이 있었는데 그 부분을 알지 못했다. 아래 에러들 중엔 실행하기 전에(apachectl configtest로 문법 검사) 또는 실행하면서 알 수 있는 것들도 있고, ${APACHE_HOME}/logs/error.log를 살펴봐야하는 것들도 있다.또한, 2.4 버전을 설치할 때 세팅되는 httpd.conf 파일을 그대로 사용하면 겪지 않을 에러들도 있다.Cannot load modules/mod_authz_default.so 원인 2.4 버전에서 해당 모듈은 삭제됐다. 2.4 버전에서 제거된 모듈 : mod_authn_default, mod_authz_default, mod_mem_cache 조치 LoadModule authz_default_module modules/mod_authz_default.so 부분 제거 또는 LoadModule authz_core_module modules/mod_authz_core.so로 대체 SSLSessionCache: ‘dbm’ session cache not supported 원인 dbm 타입을 사용하려면 mod_socache_dbm 모듈이 로드되어야 한다. 조치 #LoadModule socache_dbm_module modules/mod_socache_dbm.so 부분 주석 해제 Either all Options must start with + or -, or no Option may. 원인 Options 속성에 +, -를 아예 사용하지 않거나 모두 사용해야 한다. 잘못된 예시 : Options -Indexes FollowSymLinks MultiViews (Indexes에만 -가 붙음) 조치 &lt;Directory&gt;섹션 내에 있는Options -Indexes FollowSymLinks MultiViews -&gt; Options FollowSymLinks MultiViews 주의해야 할 것은 -, +을 붙이면 이전에 지정되어있는 Options이 영향을 준다는 것이다. 예를 들어, 아래와 같은 경우 /web/docs/spec 디렉토리에는 위에서 세팅된 FollowSymLinks에 Includes 속성까지 추가가 된다.Indexes는 -가 붙어있으므로 빠지게된다. &lt;Directory \"/web/docs\"&gt; Options Indexes FollowSymLinks&lt;/Directory&gt;&lt;Directory \"/web/docs/spec\"&gt; Options +Includes -Indexes&lt;/Directory&gt; Invalid command ‘SSLMutex’, perhaps misspelled or defined by a module not included in the server configuration 원인 AcceptMutex, LockFile, RewriteLock, SSLMutex, SSLStaplingMutex, WatchdogMutexPath 지시어가 Mutex로 대체되었다. 조치 SSLMutex -&gt; Mutex로 변경 AH02432: Cannot find LB Method: byrequests 원인 2.3 이하 버전에서는 mod_proxy_balancer 모듈이면 충분했지만, 2.4 버전부터는 mod_proxy_balancer에서 mod_lbmethod_byrequests가 따로 분리되었다. 조치 #LoadModule lbmethod_byrequests_module modules/mod_lbmethod_byrequests.so 부분 주석 해제 기타 2.4 버전 업그레이드 관련 공식 문서에 보면 더 많은 사항들이 자세히 나와있으니 참고하면 좋을 것 같다.참고 자료 https://publib.boulder.ibm.com/httpserv/manual24/upgrading.html https://httpd.apache.org/docs/2.4/upgrading.html https://httpd.apache.org/docs/2.4/en/mod/mod_ssl.html#sslsessioncache https://httpd.apache.org/docs/2.4/mod/mod_lbmethod_byrequests.html)" }, { "title": "Infra - Tomcat & Apache HTTP Server 연동", "url": "/posts/tomcat-and-apache-connection-part1/", "categories": "지식 더하기, 이론", "tags": "인프라", "date": "2022-02-10 13:25:00 +0900", "snippet": "들어가기 전Tomcat과 Apache HTTP Server 연동과 관련된 기초 지식을 살펴보고, 다음 파트에서는 실제로 연동하는 작업을 진행해보려고 한다.Apache Tomcat Connectors Apache Tomcat Connectors 프로젝트는 Tomcat 프로젝트의 일부이며, 웹 서버를 Tomcat 및 기타 백엔드와 연결하기 위한 웹 서버...", "content": "들어가기 전Tomcat과 Apache HTTP Server 연동과 관련된 기초 지식을 살펴보고, 다음 파트에서는 실제로 연동하는 작업을 진행해보려고 한다.Apache Tomcat Connectors Apache Tomcat Connectors 프로젝트는 Tomcat 프로젝트의 일부이며, 웹 서버를 Tomcat 및 기타 백엔드와 연결하기 위한 웹 서버 플러그인을 제공한다. 지원되는 웹 서버는 다음과 같다. 웹 서버 플러그인 Apache HTTP Server mod_jk Microsoft IIS ISAPI redirector iPlanet Web Server NSAPI redirector 플러그인 대부분의 기능은 모든 웹 서버에서 동일하며, 세부 사항은 웹 서버별로 다를 수 있다. 모든 경우에 플러그인은 Apache JServ Protocol(AJP) 라는 특수 프로토콜을 사용하여 백엔드에 연결한다. AJP를 지원하는 것으로 알려진 백엔드는 Apache Tomcat, Jetty, JBoss 등이 있다. 프로토콜에는 ajp12, ajp13, ajp14의 3가지 버전이 있지만, 아직까지 대부분의 경우 ajp13만 사용한다. AJP 1.3 또는 AJPv13이라고 부르지만 주로 ajp13라는 이름을 사용한다. 톰캣은 3.2 버전 이후로 ajp13을 지원한다. Apache HTTP Server - Tomcat 연동 위에서 살펴봤듯이 mod_jk를 사용하여 연동한다. (최소 버전 : Apache 1.3, Tomcat 3.2) 출처 : https://www.akadia.com/download/soug/tomcat/html/tomcat_apache.html 웹 서버(Apache HTTP Server)는 클라이언트 HTTP 요청을 기다리고 있는다. 요청이 도착하면 서버는 필요한 콘텐츠를 제공하여 요청을 처리하는 데 필요한 모든 작업을 수행한다. 서블릿 컨테이너(위의 경우 Tomcat)를 추가하면 웹 서버는 다음과 같이 동작한다. 요청을 처리하기 전에, 서블릿 컨테이너 어댑터 라이브러리를 로드하고 초기화한다. 요청이 오면, 특정 요청이 서블릿에 속하는지 확인해야 하며, 서블릿에 속하는 경우 어댑터가 요청을 받아 처리하도록 한다. 어댑터는 일반적으로 요청 URL의 일부 패턴과 요청을 보낼 위치를 기반으로 어떤 요청을 처리할지 알아야한다. mod_jk 관련 사항 mod_jk는 두 개의 엔티티가 필요하다. mod_jk.xxx Apache HTTP Server 모듈 운영 체제에 따라 mod_jk.so, mod_jk.nlm 또는 MOD_JK.SRVPGM 먼저 Apache HTTP Server의 모듈 디렉토리(예:/usr/lib/apache)에 설치되어야 하며 httpd.conf 파일에서 로드되어야 한다. ex : LoadModule jk_module modules/mod_jk.so workers.properties Tomcat 프로세스가 사용하는 호스트, 포트 등을 정의하는 파일이다. (아래 Worker 파트 참조) workers.properties파일은 ${APACHE_HOME}/conf 디렉토리에 있다. httpd.conf에서 해당 파일의 경로를 정의해야 한다. ex : JkWorkersFile /etc/httpd/conf/workers.properties 아파치 httpd.conf mod_jk 관련 부분 예시# Load mod_jk moduleLoadModule jk_module modules/mod_jk.so# Where to find workers.propertiesJkWorkersFile /etc/httpd/conf/workers.properties# Where to put jk shared memoryJkShmFile /var/log/httpd/mod_jk.shm# Where to put jk logsJkLogFile /var/log/httpd/mod_jk.log# Set the jk log level [debug/error/info]JkLogLevel info# Send requests for context /examples to worker named worker1JkMount /examples/* worker1연동시 주의사항 Apache와 Tomcat이 동일한 파일 시스템 위치에서 콘텐츠를 제공하도록 구성된 경우, Apache가 WEB-INF 디렉토리 또는 JSP 소스 코드와 같은 콘텐츠(서블릿 컨테이너가 처리해야하는 콘텐츠)를 제공하지 않도록 해야한다. Apache DocumentRoot가 Tomcat Host의 appBase 또는 Context의 docBase와 겹치면 발생할 수 있다. Apache Alias 지시문을 Tomcat Host의 appBase 또는 Context의 docBase와 함께 사용하면 발생할 수 있다. WorkerTomcat Worker Tomcat Worker는 일부 웹 서버를 대신하여 서블릿 또는 기타 콘텐츠를 실행하기 위해 대기 중인 Tomcat 인스턴스이다. ※ 참고 : 공식 문서를 보면 Tomcat Worker를 다음과 같이 정의한다. “A Tomcat worker is a Tomcat instance that is waiting to execute servlets on behalf of some web server. For example, we can have a web server such as the Apache HTTP Server forwarding servlet requests to a Tomcat process (the worker) running behind it.” 대표적으로 다음과 같은 경우, 여러 개의 worker를 구성할 수도 있다. 모든 개발자가 동일한 웹 서버를 공유하지만, 자신의 worker를 소유하는 개발 환경이 필요한 경우. 하나의 웹 서버에서 여러 사이트를 제공하기 위해, 톰캣 프로세스별 가상 호스트가 필요한 경우. 로드 밸런싱을 제공하고자 하는 경우. 즉, 자체 머신에서 여러 worker를 실행하고 요청을 분배해야 한다. Worker 정의하기 workers.properties라는 속성 파일에 정의된다. (경로 : ${APACHE_HOME}/conf) worker.list로 worker를 정의한다. # worker 리스트worker.list= worker1, worker2 웹 서버를 시작할 때, 웹 서버 플러그인(예 : mod_jk)은 worker.list 속성에 나열된 worker를 인스턴스화한다. 인스턴스화된 worker는 자신에게 맵핑된 특정 요청을 전달받을 수 있다. worker.list는 여러 번 사용할 수 있다.Worker Type 현재 존재하는 Worker Type 다음과 같다. (JK 1.2.5): Type 정의 ajp13 이 타입의 worker는 ajp13 프로토콜을 사용하여 Tomcat worker에게 요청을 전달한다. lb 로드 밸런싱 worker로써, 특정 수준의 내결함성과 함께 유연한 로드 밸런싱을 제공한다. status 로드밸런서를 관리하기 위한 status worker이다. ajp12 이 타입의 worker는 ajp12 프로토콜을 사용하여 Tomcat worker에게 요청을 전달한다. ajp14 이 타입의 worker는 ajp14 프로토콜을 사용하여 Tomcat worker에게 요청을 전달한다. Worker Type별 속성 살펴보기 workers.properties파일에 세팅되는 속성과 관련해서 자세한 내용은 공식 문서 참조ajp13 Worker 살펴보기 TCP/IP 소켓을 통해 ajp13 프로토콜을 사용하여 Tomcat Worker에게 요청을 전달한다.# \"worker2\"는 3lb 팩터를 사용하여 www2.x.com가 올라간 서버의 포트 8009에서 수신 대기 중인 Tomcat과 통신한다.worker.worker2.type=ajp13worker.worker2.host=www2.x.comworker.worker2.port=8009worker.worker2.lbfactor=3lb Worker 살펴보기 로드 밸런싱 worker는 실제로 Tomcat worker와 통신하지 않고, 웹 서버 플러그인 worker 관리를 담당한다. 관리하는 부분은 다음과 같다. 웹 서버 플러그인 worker를 인스턴스화 관리되는 worker의 로드 밸런싱 factor(lbfactor)를 사용하여 가중치 라운드 로빈(weighted round-robin) 로드 밸런싱을 수행 가중치 라운드 로빈에서는 lbfactor가 높을수록 더 강력한 시스템(더 많은 요청을 처리함)을 의미 동일한 Tomcat worker에서 실행되는 동일한 세션에 속하는 요청 유지(세션 고정) 실패한 Tomcat worker를 식별하여 요청을 일시 중지하고, lb worker가 관리하는 다른 worker로 fail-over 결과적으로, lb worker가 관리하는 worker들은 로드 밸런싱되고(lbfactor 및 현재 사용자 세션을 기반으로) 장애 조치가 이루어지므로 단일 Tomcat 프로세스 종료로 인해 전체 사이트가 마비되지 않는다. 다음은 lb worker가 가질 수 있는 속성이다. balance_workers 로드 밸런서가 관리해야 하는 worker 목록이다. # The worker balance1 while use \"real\" workers worker1 and worker2worker.balance1.balance_workers=worker1, worker2 sticky_session SESSION ID가 있는 요청을 동일한 Tomcat worker로 다시 라우팅해야 하는지 여부를 지정한다. Tomcat이 여러 Tomcat 인스턴스에서 세션 데이터를 유지할 수 있는 세션 관리자를 사용하는 경우 sticky_session을 false로 설정한다. 기본적으로 true로 설정된다. 정리 아파치와 톰캣이 연동되려면 mod_jk 모듈이 필요하다. (운영체제 별로 이름은 다를 수 있다.) 해당 모듈은 Apache JServ Protocol(AJP) 프로토콜을 사용하여 아파치와 톰캣간 통신한다. mod_jk 모듈을 사용하려면 ? 먼저 아파치 모듈 디렉토리(예:/usr/lib/apache)에 설치되어야 있어야한다. 아파치 httpd.conf에 모듈이 로드되어야하고, JKMount 등 관련 옵션들이 정의되어야 한다. workers.properties 파일이 작성되어야한다. workers.properties는 서블릿 컨텐츠 요청 전달, 로드 밸런싱 등을 위한 Tomcat worker와 요청을 전달받을 톰캣 프로세스의 호스트 및 포트 등을 정의한 파일이다. 참고 자료 https://tomcat.apache.org/connectors-doc/ https://tomcat.apache.org/connectors-doc/webserver_howto/apache.html https://tomcat.apache.org/connectors-doc/common_howto/workers.html" }, { "title": "Infra - Apache httpd.conf 파일 속성 공부하기(1)", "url": "/posts/apache-httpd-conf-part1/", "categories": "지식 더하기, 이론", "tags": "인프라", "date": "2022-01-15 13:25:00 +0900", "snippet": "들어가기 전회사에서 서버 이관을 준비하고 있다. 기존에 각각의 서버에 흩어져 있던 애플리케이션을 한 서버로 모으는 등의 작업을 해야하는데, 그럴려면 기존에 세팅된 아파치 설정 파일을 제대로 이해하고 더 나아가 수정하는 등의 작업이 필요할 것 같다.따라서, httpd.conf 파일에 있는 속성들이 어떤 것을 의미하는지, 왜 필요한지 정리해보려고 한다.h...", "content": "들어가기 전회사에서 서버 이관을 준비하고 있다. 기존에 각각의 서버에 흩어져 있던 애플리케이션을 한 서버로 모으는 등의 작업을 해야하는데, 그럴려면 기존에 세팅된 아파치 설정 파일을 제대로 이해하고 더 나아가 수정하는 등의 작업이 필요할 것 같다.따라서, httpd.conf 파일에 있는 속성들이 어떤 것을 의미하는지, 왜 필요한지 정리해보려고 한다.httpd란 ? “Hypertext Transfer Protocol daemon” 데몬은 사용자가 직접적으로 제어하지 않고, 백그라운드에서 돌면서 여러 작업을 하는 프로그램을 말한다. (위키백과) httpd는 웹에서 들어온 요청을 처리하거나, 필요에 따라 다른 프로세스로 전달한다. 즉, 웹 서버에서 http 요청을 처리하기 위해 백그라운드에서 동작하는 프로그램이 httpd라고 생각하면 될 것 같다. 가장 대표적인 httpd 중 하나는 Apache HTTP Server이다. (참고)httpd.conf 파일 속성, 태그 살펴보기 내가 담당하는 서버의 httpd.conf에 있는 태그, 속성 위주로 정리해보려고 한다.ServerName 서버가 자신을 식별하는 데 사용하는 호스트 이름 및 포트 문법 : ServerName [scheme://]domain-name|ip-address[:port] ServerName은 이름 기반 가상 호스트(name-based virtual host)를 사용할 때, 가상 호스트를 고유하게 식별하는 데 사용된다. ServerName은 서버 정의 내 어디에나 나타날 수 있다. 만약 여러개가 있다면, 맨 마지막에 있는게 ServerName이 된다. ServerName이 지정되지 않은 경우, 서버는 먼저 운영 체제에 시스템 호스트 이름을 요청하여 클라이언트의 보이는 호스트 이름을 추론하려고 하고, 실패하면 시스템에 있는 IP 주소를 역방향으로 조회한다. 이름 기반 가상 호스트를 사용하는 경우, &lt;VirtualHost&gt; 섹션 안의 ServerName은 이 가상 호스트와 일치하도록, 요청의 Host: 헤더에 표시할 호스트 이름을 지정한다.참고 : Host vs Server 호스트 : 네트워크에 연결되는 컴퓨터 또는 다른 장치 서버 : 네트워크 내의 다른 프로그램이나 장치에 서비스를 제공하는 소프트웨어 또는 하드웨어 결국엔 서버도 하나의 호스트라고 생각할 수 있을 것 같다.&lt;VirtualHost&gt; 특정 호스트 이름 또는 IP 주소에만 적용되는 지시문을 포함 문법 : &lt;VirtualHost addr[:port] [addr[:port]] ...&gt; ... &lt;/VirtualHost&gt; 서버가 특정 가상 호스트에 대한 문서 요청을 받으면, &lt;VirtualHost&gt; 섹션에 포함된 구성 지시자(configuration directives)를 사용한다. addr은 다음 중 하나가 될 수 있으며, 선택적으로 뒤에 콜론과 포트 번호(또는 *)가 올 수 있다. 가상 호스트의 IP 주소 가상 호스트의 IP 주소에 대한 정규화된 도메인 이름(권장하지 않음) 와일드카드 역할을 하고 모든 IP 주소와 일치하는 * 문자 *의 별칭인 문자열 _default_ 각 &lt;VirtualHost&gt; 블록 안에 ServerName을 지정해야 한다. 없는 경우 메인 서버 구성의 ServerName이 상속된다. 요청이 수신되면 서버는 먼저 로컬 IP 주소와 포트를 기준으로 일치하는 &lt;VirtualHost&gt;를 찾는다. 와일드카드가 아닌 것이 더 높은 우선순위를 가진다. 일치하는 호스트를 찾을 수 없으면 않으면 메인 서버 구성이 사용된다. IP 주소 및 포트에 여러 호스트가 일치하는 경우, 서버는 요청된 호스트 이름을 기반으로 일치하는 호스트를 선택한다. 일치하는 가상 호스트가 없으면 IP 주소와 일치하는 첫 번째 나열된 가상 호스트가 사용된다. 즉, IP 주소 및 포트 조합에 대해 나열된 첫 번째 가상 호스트가 해당 IP 및 포트 조합에 대한 기본 가상 호스트이다. DocumentRoot 웹에서 볼 수 있는 main document tree 를 구성하는 디렉토리 문법 : DocumentRoot directory-path Default : DocumentRoot \"/usr/local/apache/htdocs\" 이 지시문은 httpd가 파일을 제공할 디렉토리를 설정한다. Alias와 같은 지시문과 일치하지 않는 한, 요청된 URL에서 문서 루트까지의 경로를 추가하여 문서의 경로를 만든다. 예를 들어, DocumentRoot \"/usr/web\"로 세팅되어 있다면 http://my.example.com/index.html 에 대한 액세스는 /usr/web/index.html을 참조한다. 디렉토리 경로가 절대 경로가 아니면, ServerRoot에 상대적인 것으로 간주된다. DocumentRoot는 후행 슬래시 없이 지정해야 한다.&lt;Directory&gt; 명명된 파일 시스템 디렉터리, 하위 디렉터리 및 해당 내용에만 적용되는 지시문 그룹을 묶는다. 문법 : &lt;Directory directory-path&gt; ... &lt;/Directory&gt;&lt;Directory \"/usr/local/httpd/htdocs\"&gt; Options Indexes FollowSymLinks&gt;&lt;/Directory&gt; Directory-path는 디렉토리의 전체 경로이거나 Unix 셸 스타일 매칭을 사용하는 와일드카드 문자열이다. 와일드카드 문자열에서 ?는 단일 문자와 일치하고 *는 모든 문자 시퀀스와 매칭된다. [] 문자 범위를 사용할 수도 있다. 와일드카드는 / 문자와 일치하지 않는다. 즉, &lt;Directory \"/*/public_html\"&gt;은 /home/user/public_html과 일치하지 않지만 &lt;Directory \"/home/*/public_html\"&gt;과는 일치한다. directory-path는 따옴표로 묶지 않아도 되지만, 경로에 공백이 포함된 경우는 반드시 따옴표로 묶어야 한다. 공백은 끝을 의미하기 때문이다. directory-path는 Apache httpd가 파일에 액세스하는데 사용하는 파일 시스템 경로와 문자 그대로 일치해야 한다. 즉, 동일한 디렉토리에서 다른 심볼릭 링크를 통해 액세스하는 것과 같은 다른 경로를 통해 액세스하는 파일에는 적용되지 않는다. &lt;Location&gt; 포함된 지시문을 일치하는 URL에만 적용 문법 : &lt;Location URL-path|URL&gt; ... &lt;/Location&gt; &lt;Location&gt; 지시문은 포함된 지시문의 범위를 URL로 제한한다. (&lt;Directory&gt; 지시문과 유사) &lt;Location&gt; 섹션은 설정 파일에 나타나는 순서대로, &lt;Directory&gt; 섹션과 .htaccess 파일을 읽은 후, &lt;Files&gt; 섹션 이후에 처리된다. &lt;Location&gt; 지시문이 파일 시스템 위치에 대한 액세스를 제어하는 데 사용되어서는 안된다. 여러 다른 URL이 동일한 파일 시스템 위치에 매핑될 수 있으므로 이러한 액세스 제어가 우회될 수 있다. 후행 슬래시가 사용되지 않은 아래 예에서 /private1, /private1/ 및 /private1/file.txt에 대한 요청에는 포함된 지시문이 적용되지만 /private1other에는 적용되지 않는다.&lt;Location \"/private1\"&gt; # ...&lt;/Location&gt; 후행 슬래시가 사용되는 아래 예에서 /private2/ 및 /private2/file.txt에 대한 요청에는 포함된 지시문이 적용되지만 /private2 및 /private2other에는 적용되지 않는다.&lt;Location \"/private2/\"&gt; # ...&lt;/Location&gt; 언제 &lt;Location&gt;을 사용하는게 좋을까 ? &lt;Location&gt;을 사용하여 파일 시스템 외부에 있는 콘텐츠에 지시문을 적용한다. 파일 시스템에 있는 콘텐츠의 경우 &lt;Directory&gt; 및 &lt;Files&gt;를 사용한다. 예외적으로, &lt;Location \"/\"&gt;은 구성을 전체 서버에 쉽게 적용할 수 있는 방법이다. 모든 원본(프록시가 아닌) 요청에 대해 일치할 URL은 /path/ 형식의 URL 경로이다. 즉, 스키마, 호스트 이름, 포트 또는 쿼리 문자열이 포함될 수 없다. 프록시 요청의 경우 일치하는 URL은 scheme://servername/path 형식이며, 반드시 접두사를 포함해야 한다. URL은 와일드카드를 사용할 수 있다. 와일드카드 문자열에서 ?는 단일 문자와 일치하고 *는 모든 문자 시퀀스와 일치한다. 와일드카드 문자는 URL 경로의 /와 일치하지 않는다. ~ 문자를 추가하여 정규식도 사용할 수 있다. 아래 예는 하위 문자열 /extra/data 또는 /special/data가 포함된 URL과 일치한다. &lt;Location ~ \"/(extra|special)/data\"&gt; #...&lt;/Location&gt; &lt;Location&gt; 기능은 SetHandler 지시문과 결합할 때 특히 유용하다. 예를 들어, 상태 요청을 활성화하되 example.com의 브라우저에서만 허용하려면 다음과 같이 사용할 수 있다. &lt;Location \"/status\"&gt; SetHandler server-status Require host example.com&lt;/Location&gt;&lt;LocationMatch&gt; 포함된 지시문을 정규식 일치 URL에만 적용 문법 : &lt;LocationMatch regex&gt; ... &lt;/LocationMatch&gt;&lt;LocationMatch \"/(extra|special)/data\"&gt; # ...&lt;/LocationMatch&gt; 지시문 &lt;LocationMatch&gt;는 &lt;Location&gt;의 정규식 버전과 동일하게 작동하며 많은 글꼴에서 ~가 -와 구별하기 어렵다는 단순한 이유로 선호된다.SetHandler 일치하는 모든 파일을 강제로 핸들러에서 처리 문법 : SetHandler handler-name|none|expression SetHandler가 .htaccess 파일이나 &lt;Directory&gt; 또는 &lt;Location&gt; 섹션에 배치될 때, 모든 일치하는 파일이 handler-name에 정의된 핸들러를 통해 구문 분석되도록 한다. http://servername/status 의 URL이 호출될 때마다 서버가 상태 보고서를 표시하도록 하려면 다음을 httpd.conf에 추가한다.&lt;Location \"/status\"&gt; SetHandler server-status&lt;/Location&gt; 특정 파일 확장자를 가진 파일에 대한 특정 핸들러를 구성할 수도 있다.&lt;FilesMatch \"\\.php$\"&gt; SetHandler application/x-httpd-php&lt;/FilesMatch&gt; None 값을 사용하여 이전에 정의된 SetHandler 지시문을 재정의할 수 있다.참고 : 핸들러란 ? 핸들러는 파일이 호출될 때 수행될 작업에 대한 Apache의 내부적인 표현이다. 일반적으로 파일에는 파일 유형에 따라 암시적 핸들러가 있다. 일반적으로 모든 파일은 단순히 서버에서 제공되지만 특정 파일 형식은 별도로 처리된다. 핸들러는 파일 유형과 관계없이 파일 이름 확장자 또는 위치에 따라 명시적으로 구성할 수도 있다. 핸들러는 서버에 구축하거나 모듈에 포함하거나 Action 지시문으로 추가할 수 있다. 표준 배포판에 내장된 핸들러는 다음과 같다. default-handler (core) 정적 콘텐츠를 처리하기 위해 기본적으로 사용되는 핸들러인 default_handler()를 사용하여 파일을 보낸다. send-as-is (mod_asis) HTTP 헤더가 파일을 있는 그대로 보낸다. cgi-script (mod_cgi) 파일을 CGI 스크립트로 취급한다. imap-file (mod_imagemap) 이미지맵 규칙 파일(imagemap rule file)로 파싱한다. server-info (mod_info) 서버의 구성 정보를 가져온다. server-status (mod_status) 서버의 상태 보고서를 가져온다. type-map (mod_negotiation) content negotiation을 위해 type map 파일로 파싱한다. 참고 자료 https://whatis.techtarget.com/definition/Hypertext-Transfer-Protocol-daemon-HTTPD https://httpd.apache.org/docs/2.4/en/mod/directives.html https://pediaa.com/difference-between-host-and-server/ https://httpd.apache.org/docs/2.4/en/handler.html" }, { "title": "Permissions 0644 for ~ are too open 그리고 Permission denied (publickey) 현상 해결하기", "url": "/posts/aws-ssh-pem-key-permission/", "categories": "경험하기, 이슈 노트", "tags": "AWS, SSH", "date": "2022-01-15 00:25:00 +0900", "snippet": "상황회사에서 리눅스 서버를 다뤄야할 일이 많아서 익숙해질겸 연습용으로 막 다뤄볼 수 있는 AWS 서버 하나를 구해봐야지 생각했다. 예전에 만들어놓은 AWS EC2 인스턴스가 있어서오랜만에 접속해보려고 했는데 Permissions 0644 for keyname.pem are too open., Permission denied (publickey) 두 가...", "content": "상황회사에서 리눅스 서버를 다뤄야할 일이 많아서 익숙해질겸 연습용으로 막 다뤄볼 수 있는 AWS 서버 하나를 구해봐야지 생각했다. 예전에 만들어놓은 AWS EC2 인스턴스가 있어서오랜만에 접속해보려고 했는데 Permissions 0644 for keyname.pem are too open., Permission denied (publickey) 두 가지 에러를 맞닥들였다.원인과 해결방법1. Permissions 0644 for keyname.pem are too open. 문장 그대로 파일 권한이 너무 공개되어 있다는 의미.@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ WARNING: UNPROTECTED PRIVATE KEY FILE! @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@Permissions 0644 for keyname.pem are too open.It is required that your private key files are NOT accessible by others.This private key will be ignored.Permissions 0644 ? Chmod 644 (chmod a+rwx,u-x,g-wx,o-wx) (U)ser : 파일 소유자는 읽기/쓰기만 가능 (G)roup : 그룹은 읽기만 가능 (O)thers : 외부에서도 읽기만 가능 출처 : https://chmodcommand.com/ 해결 방법 파일 소유자만 해당 파일을 읽을 수 있도록 권한을 변경한다. chmod 400 keyname.pem 2. Permission denied (publickey). 위 문제 해결 후, ssh -i [key] [username@host]로 접속 시도하니까 이러한 현상이 나타났다. 대표적으로 세 가지 이유가 있다고 한다. 잘못된 key 사용 잘못된 username 사용 잘못된 host로 접속 시도 해결 방법 해결이라고 하기도 민망하게 나의 경우는 다른 key를 사용하고 있었다… 알맞은 키를 사용하니 잘 접속됐다.※ 참고 : 인스턴스별 username AWS 공식 문서에 보면 인스턴스별 username이 나와있다. Amazon Linux 2 or the Amazon Linux AMI : ec2-user CentOS AMI : centos or ec2-user Debian AMI : admin Fedora AMI : fedora or ec2-user RHEL AMI : ec2-user or root SUSE AMI : ec2-user or root Ubuntu AMI : ubuntu Oracle AMI : ec2-user Bitnami AMI: bitnami참고 자료 https://99robots.com/how-to-fix-permission-error-ssh-amazon-ec2-instance/ https://bobbyhadz.com/blog/aws-ssh-permission-denied-publickey https://stackoverflow.com/questions/18551556/permission-denied-publickey-when-ssh-access-to-amazon-ec2-instance https://bobbyhadz.com/blog/aws-ssh-permission-denied-publickey" }, { "title": "SI에서 IT 서비스 기업으로 가기까지 (feat.회고록)", "url": "/posts/restrospect2021/", "categories": "끄적끄적", "tags": "", "date": "2022-01-01 01:00:00 +0900", "snippet": "지난 3년 되돌아보기2022년이다. 21년엔 목표하던 IT 서비스 회사로 이직을 하게되었다. 이직에 성공하면 회고록을 적어보고 싶다는 로망(?)이 있었는데, 미루다가 새해를 맞이하면서 써보려고한다.스스로를 되돌아보면서 생각을 정리하고, 새로운 한 해 또한 즐겁고 열심히 살 수 있길 바라본다.개발자로서 사회의 첫발을 내딛다.18년 10월 대기업(연봉은 ...", "content": "지난 3년 되돌아보기2022년이다. 21년엔 목표하던 IT 서비스 회사로 이직을 하게되었다. 이직에 성공하면 회고록을 적어보고 싶다는 로망(?)이 있었는데, 미루다가 새해를 맞이하면서 써보려고한다.스스로를 되돌아보면서 생각을 정리하고, 새로운 한 해 또한 즐겁고 열심히 살 수 있길 바라본다.개발자로서 사회의 첫발을 내딛다.18년 10월 대기업(연봉은 중견기업) IT 계열사에 입사하게 되었다.나는 전공자이지만 학부생 때 코딩을 엄청 열심히 한 것도 아니고, ‘개발자로서 성장하기 좋은 회사’에 대해 생각해본 적은 더더욱 없었기 때문에 그냥 전공을 살려서 취업을 했다는 사실과약 8개월 간의 취준을 마쳤다는 사실이 너무 기뻤다.앞서 말했듯 코딩 덕후는 아니었지만, 그래도 개발을 할 때면 재밌고 적성에 맞는 것 같다는 생각은 하고 있었는데, 일을 하다보니 ‘나에게 정말 잘 맞는 직업이구나’라고 느끼게 되었다.익숙함과 실력을 착각하다.내가 속한 프로젝트는 약 2년 동안 진행된 대규모 프로젝트였다. 내가 처음에 맡은 부분은 레거시 시스템의 화면 부분을 React 기반으로 바꾸는 것이었다.React는 고사하고 javascript에 대한 개념과 기초가 부족했기 때문에 코뿔소 책을 사서 나름대로 공부를 했다.또한 상태 관리 라이브러리인 redux에 대해 어느정도 이해가 되고나니 내가 맡은 부분에 있어서는 큰 어려움 없이 구현을 할 수 있었다.점점 익숙해지다보니 깊게 공부하기보다는 그냥 아는 지식으로 구현하는 것에 만족하면서 지냈던 것 같다. 지금 돌이켜 생각해보면 시간 내에 구현하는 것 = 실력이라고 착각했던 것 같다.물론 정해진 기간 내에 맡은 일을 마무리하는 것은 매우 중요하지만, 그것은 개발자의 기본 소양인 것 같다. 진짜 실력은 생산성과 더불어 얼마나 좋은 코드를 작성하는지에 달린 것 같다.내가 생각하는 ‘좋은 코드’의 특징은 크게 유지보수하기 좋다, 트레이드 오프가 적절하게 고려되었다 인 것 같다.하나의 프로젝트를 마치고 운영까지 해보니 프로그램을 유지보수하는데 드는 비용을 줄이는 것이 매우 중요한 것 같다는 생각이 들었다.특히, 요구사항이 변경되거나 추가될 때 ‘코드를 얼마나 적게 고치면서 반영 할 수 있는지’, ‘기존의 작성된 코드가 이해하기 얼마나 수월한지’ 등 실제로 지출되는 돈은 아니지만개발자가 인지하고 작업하는데 걸리는 시간을 비용으로 생각하고 이것을 고려하면서 코드를 작성하는게 매우 중요하다고 생각한다.\"은 탄환은 없다\"라는 말처럼, 모든 문제에 통용되는 해결방법은 없다. 예를 들어 ‘멀티 스레딩을 활용한 비동기 처리를 통해 성능을 향상시킨다’라는 해결법이 있을 때,단순히 성능 향상에만 초점을 맞추게 되면 성능이 안나오는 경우에 우선적으로 멀티 스레딩을 적용시키려고 할 것이다.하지만, 멀티 스레딩을 사용했을 때 발생할 수 있는 경합, 컨텍스트 스위칭 비용 등에 대해서도 고려한다면 오히려 멀티 스레딩이 역효과를 불러오는 상황도 분명 있을 것이다.이렇듯, 내가 생각하는 실력있는 개발자가 되기 위해서는 끊임없는 고민과 학습이 필요하다고 생각한다.지금 편할수록 나중에 개발자로서 설 자리는 점점 줄어든다는 생각으로 또한 IT 서비스로 세상에 좋은 영향을 끼치기 위해, 꾸준히 그리고 즐겁게 공부하자.끔찍한 평생공부 ?입사 후 약 1년 정도 지났을 때부터였나, 언제가 될지는 모르지만 개발자에게 좋은 회사로 이직하려면 퇴근 후 공부를 해야겠다는 생각이 들었던 것 같다.어떤 공부를 해야할까 고민하던 중 취준할 때 많은 도움을 받은 잇다를 찾게되었고, 여러 멘토님들께 사이드 프로젝트를 시작해보라는 조언을 듣게 되었다.딥러닝을 활용해서 운동화 커스터마이징을 해보면 재밌겠다는 생각에 GAN에 대해 공부해보았다. 하지만, 얼마 지나지 않아 사이드 프로젝트를 멈췄다. 야근도 많긴했지만 의지도 약했던 것 같고, 지금 생각해보면 가장 큰 문제는와닿지 않았기 때문이었던 것 같다. 쉽게 말해, 공부한 내용을 내가 현재 하고있는 실무에 적용할 수 있는 것도 아니고 부수입과 같은 어떤 부가적인 가치를 창출해내는 것이 아닌 소위 말해 ‘공부를 위한 공부’를 하고 있었던 것이다.사이드 프로젝트가 흐지부지되고는 찰스 펫졸드의 CODE와 같은 개발자 필독서라 불리는 책들을 간간히 읽었던 것 같다.하지만 지금 생각해보면 이 또한 그냥 ‘공부를 안하기는 좀 뭐하니까’라는 생각에 보냈던 시간같다.이러한 시간들을 통해 나는 공부를 위한 공부는 하기 힘들어하는 성향이고 이렇게 학습한 것은 머릿속에도 많이 남지 않는다는 것을 알 수 있었다.따라서 지금은 실무에 필요한 공부, 내가 부족한 부분에 대한 공부 등 나에게 와닿는 공부를 하면서 스스로를 좀 더 동기부여 할 수 있게 되었다.‘개발자는 평생 공부해야하는 직업이다’라는 말을 많이 들었고 공감하는 부분이다.그렇기 때문에 평생을 괴로워하면서 공부하고 싶은게 아니라면 자신에게 맞는 학습 방향과 방법을 찾는 것은 매우 중요하다고 생각한다.취준보다 간절했던 이직21년 한 해 나에게 가장 중요했던 키워드 중 하나는 이직이다. 이직 사유는 이전 회사가 마음에 들지 않아서가 아니었다.개발 프로젝트를 마치고 해당 시스템을 운영하다보니 개발할 일은 거의 없고, 그렇다고 기존의 코드를 리팩토링하면서 좋은 코드 품질을 위해 노력하는 분위기도 아니었기 때문에,개발자로서 성장하기에 한계가 있을 것 같다는 생각이들었다.또한 실력이 없으면 부당한 상황에서도 본인의 선택권 없이 회사가 하라는대로 할 수 밖에 없는 많은 상황들을 지켜보며 실력을 키워야겠다는 생각이 더욱 절실하게 들었다.그리고 항상 마음 한켠에는 일반 사용자를 대상으로 하는 서비스(나도 사용하는) 회사에 가서 프로덕트에 좀 더 애정을 갖고 일하고 싶다는 생각이 있었다. 따라서, 네카라쿠배가 아니더라도 반드시 IT 서비스 회사를 가야겠다고 생각했다.작년 초 결혼을 하고나서부터 이력서를 작성하고 코딩테스트를 준비하는 등 본격적으로 이직 준비를 시작했다. 그리고 3월에 네이버 계열사 중 한 곳의 면접을 볼 수 있게 되었다.단순히 불합격을 넘어 결과는 참담했고 그 면접을 통해 현재 내가 어떤 수준이고 얼마나 부족한지를 자각할 수 있었다. 가장 문제라고 생각된 부분은 면접시 내가 한 부분에 대해 자신있게 얘기할게 없었다는 것이었다.기술에 대한 깊은 이해, 좀 더 좋은 코드, 다양한 문제 해결 방법을 생각하며 일하지 않고 단순히 구현하기에 바빴던 나에게 어떻게 보면 당연한 결과였던 것 같다.전략적으로 준비할 필요가 있다고 생각이들어 일단 회사에 지원을 하는 대신 이력서에 채울 내용을 찾아야겠다고 생각했다.그리고 가장 좋은 컨텐츠는 내가 현재 유지보수 하고있는 시스템의 문제점들을 개선시키거나, 있었던 문제들을 어떻게 해결했는지 등에 대해 정리하고 다양한 해결 방법에 대해 생각해보는 것이었다.성능 개선, 테스트 코드 적용, 확장성 좋은 코드 등에 대해 학습하며 실무에 적용할 수 있었고, 부족함을 채우기 위해 퇴근 후 매일 새벽까지 공부를 했다.하지만, 공부를 할수록 ‘내가 모르는게 정말 많았구나’라는 생각이 들었고 분명 조금씩 나아지고 있지만 ‘내가 정말 서비스 회사에 갈 수 있을까?’라는 두려움이 엄습해오기도 했다.이번에도 잇다에서 멘토님께 조언을 구했고, 한 멘토님께서 모든 부분을 다 알 수는 없으니 본인이 한 부분에 대해 자신감을 갖고 도전해보는게 좋을 것 같다고 하셨다.두렵지만 다시 한 번 이력서를 재정비하고 여러 회사에 지원을 했다. 약 6개의 회사에 지원했고 그 중 4개 회사에 최종면접까지 갈 수 있었다.결과적으로 3개의 회사에 붙었고, 내가 실제로 많이 사용하는 서비스를 하는 회사로 최종 선택을 했다. 매일 밤에 혼자 방에서 공부하던 시간들이 헛되지 않았던 것 같아 정말 기뻤다.물론 수습 기간 3개월이 있기 때문에 아직 완전히 정착한 것은 아니지만 끝까지 좋은 결과가 있길 바란다.TMI로 탈락의 고배를 마셨던 나날들을 추억해본다… 마무리하며현재 직장에 11월 22일에 입사했으니 온지 벌써 한 달이 넘었다. 팀원분들 모두 너무 친절하시고 잘해주신다. 여기서는 깊게는 아니더라도 기본적으로 인프라 부분까지 서버 개발자가 담당하기 때문에 낯선 작업들이 많다.그리고 스프링 기본에 대한 부족함 또한 많이 느끼고 있다. 적응하는게 쉽지는 않지만 목표하던 곳에 온만큼, 열심히해서 1인분 이상의 몫을 하고 더 나아가 서비스를 발전시켜 사람들의 생활에 편리함을 줄 수 있으면 좋겠다.끝으로 아래 문구를 마음에 새기면서 이전처럼 업무가 익숙해진다고 안주하지 말고, 더욱 성장하고 잘하는 개발자가 되기 위해 꾸준히 공부하고 노력하자 !If you do nothing, nothing happens." }, { "title": "IE11에서 window.open()시 빈팝업과 새로운 탭 생기는 이슈", "url": "/posts/ie-popup-issue/", "categories": "경험하기, 이슈 노트", "tags": "IE11", "date": "2021-12-14 23:00:00 +0900", "snippet": "상황사내에서 사용하는 화면 중, 버튼을 누르면 window.open(url, target, options)으로 팝업을 띄우는 코드가 있었다. 하지만, 요구사항으로 인해 기존 로직을 POST 요청으로 변경해야했고 이를 위해 아래와 같은 방식으로 코드를 변경했다.const form = document.createElement(\"form\");form.tar...", "content": "상황사내에서 사용하는 화면 중, 버튼을 누르면 window.open(url, target, options)으로 팝업을 띄우는 코드가 있었다. 하지만, 요구사항으로 인해 기존 로직을 POST 요청으로 변경해야했고 이를 위해 아래와 같은 방식으로 코드를 변경했다.const form = document.createElement(\"form\");form.target=\"child1\";form.method=\"POST\";form.action=\"url\";form.param1=\"param1\";form.param2=\"param2\";window.open(\"\", \"child1\", \"some options\");form.submit();개발 환경에서 테스트를 잘 마치고 운영 환경 전단계에 배포해서 확인해보는데, 기존에 팝업으로 뜨던 화면이 새로운 탭에서 열리고 팝업창은 빈 상태로 떠있는 현상이 발생했다. ※ 테스트 브라우저는 IE11이고 개발 PC와 이상 현상을 발견한 PC는 다른 PC이다.원인 IE11의 특정 버전들에서 이러한 현상이 발생하는 것 같다. 처음엔 브라우저 설정도 바꿔보고 브랜치에 머지된 다른 자원들의 영향인지도 살펴보고 window.open() 자체에 대해서도 좀 더 찾아봤지만, 답이 나오질 않았다. 그러다 다음 두 개의 글을 통해 IE 문제인 것을 알 수 있었다. 같은 현상1 같은 현상2 테스트시 정상적으로 작동한 IE11 버전과 아닌 버전 몇 가지를 파악해봤다. 정상 작동 20H2(OS 빌드 19042.1348) 버전 1909(OS 빌드 18363.1916) 오작동 20H2(OS 빌드 19042.1288) 시도해본 것들 다른 브라우저로 바꿀 수 있으면 좋겠지만, 해당 시스템의 운영 환경은 IE에 최적화 되어있다..1. IE 탭 설정 바꾸기 인터넷 옵션 &gt; 일반 &gt; 탭 &gt; 팝업 표시 방법 : 항상 새 창에서 팝업 열기 이렇게 하면 새로운 탭에서 팝업창이 열리지는 않지만, window.open() 3번째 파라미터로 넘겨준 width, height에 세팅된 크기와는 다른 팝업창이 생기고, 빈팝업창 또한 그대로 남아있다.2. UAC (사용자 계정 컨트롤) 설정 위에서 언급한 글에서 제시하는 방법 중 하나로, “Enable UAC”가 있는데 나의 경우는 이 방법으론 해결되지 않았다. 내가 잘못한 것일 수도 있으니 좀 더 시도해봐야될 것 같다.3. window.open() 3번째 파라미터에 popup 추가 시도 예정참고 자료 https://answers.microsoft.com/en-us/ie/forum/all/open-new-popup-window-getting-a-blank-page-and-it/bab61949-2327-474d-b001-8f9f33562988 https://support.citrix.com/article/CTX206419" }, { "title": "maven-default-http-blocker ... Blocked mirror for repositories 에러", "url": "/posts/maven-http-block-issue/", "categories": "경험하기, 이슈 노트", "tags": "Maven, HTTP Block", "date": "2021-12-08 23:00:00 +0900", "snippet": "상황회사에서 지급받은 임시 PC를 2주 정도 사용하고, 앞으로 계속 사용하게될 새로운 PC를 지급받아 개발 환경을 다시 세팅하고 있었다.하지만, 세팅하는 프로젝트에서 의존성을 제대로 다운로드 받지 못하고 다음과 같은 에러 메시지를 출력했다. Could not transfer artifact ... from/to maven-default-http-blo...", "content": "상황회사에서 지급받은 임시 PC를 2주 정도 사용하고, 앞으로 계속 사용하게될 새로운 PC를 지급받아 개발 환경을 다시 세팅하고 있었다.하지만, 세팅하는 프로젝트에서 의존성을 제대로 다운로드 받지 못하고 다음과 같은 에러 메시지를 출력했다. Could not transfer artifact ... from/to maven-default-http-blocker (http://0.0.0.0/): Blocked mirror for repositories원인 Maven 3.8.1 버전부터 repository에 대한 HTTP 요청이 차단되고 HTTPS만 허용된다. (3.8.1 버전 릴리즈 노트 참고)하지만, 나는 메이븐 3.8.1 버전을 따로 설치한 적이 없는데 … ? IntelliJ를 설치할 때 메이븐은 플러그인으로 함께 설치된다. IntelliJ 다운로드 페이지에 가보면 아래와 같이 기본적으로 설치되는 플러그인들을 살펴볼 수 있다. 이전 PC와 새로운 PC 모두 IntelliJ IDEA 2021.3 버전인데, 이전 PC의 경우 함께 설치된 메이븐 버전은 3.6.3이었다.3.6.3 버전과 3.8.1 버전의 settings.xml 비교해보기 IntelliJ 설치시 함께 세팅되는 메이븐의 settings.xml 파일 경로는 다음과 같다. MacOS : /Applications/IntelliJ IDEA.app/Contents/plugins/maven/lib/maven3/conf/settings.xml Windows : IntelliJ 설치 경로\\plugins\\maven\\lib\\maven3\\conf\\settings.xml 3.6.3 버전 3.8.1 버전 (&lt;mirror&gt; 속성이 추가된걸 볼 수 있다) &lt;mirror&gt;란 ? 범용적으로 쓰이는 ‘미러’에 대한 위키 설명을 보면 미러 사이트는 다른 인터넷 사이트의 복사본이다. 라고 나와있다.settings.xml에 있는 &lt;mirror&gt;도 이러한 개념으로 이해하면 될 것 같다.즉, pom.xml에 정의된 의존성들을 내려받기 위해 특정 repository로 요청시, 해당 repository 대신 요청받을 mirror repository를 정의하는 것이다. &lt;mirrorOf&gt;에 정의된 repository(central)로 요청하면 &lt;url&gt;에 정의된 repository로 요청이 가게된다. central이 어디에 정의되어 있는지는 해당 문서의 Super POM 참조. &lt;settings&gt; ... &lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;other-mirror&lt;/id&gt; &lt;name&gt;Other Mirror Repository&lt;/name&gt; &lt;url&gt;https://other-mirror.repo.other-company.com/maven2&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; &lt;/mirrors&gt; ...&lt;/settings&gt; 공식 문서에서는 mirror를 사용하는 경우에 대해 다음과 같이 설명한다. 지리적으로 더 가깝고 더 빠른 동기화된 미러가 인터넷에 있을 때 특정 리포지토리를 자체 내부 리포지토리로 바꾸려고 할 때 리포지토리 관리자를 실행하여 미러에 로컬 캐시를 제공하고 미러의 URL을 대신 사용할 때 &lt;mirrorOf&gt;에 다음과 같이 세팅할 수 있다. * = 모든 레파지토리 external:* = localhost나 파일 기반 이외의 레파지토리 위에서 본 settings.xml에서 알 수 있듯, 3.8.1부터 external:http:*가 적용됐다. 즉, external:http:*는 localhost를 사용하는 리포지토리를 제외한 HTTP를 사용하는 모든 리포지토리를 의미한다. 여기에 &lt;blocked&gt;true&lt;/blocked&gt;와 같은 속성이 더해져 HTTP 요청을 차단한다. 맨 처음에 언급했던, Maven 3.8.1 버전부터 HTTP 요청이 차단되는 이유이다. repo,repo1 = repo, repo1 (여러 레파지토리 정의시) *,!repo1 = repo1을 제외한 모든 레파지토리 해결 방법 공식 문서에 보면 다음과 같은 해결책을 제시한다. upgrade the dependency version to a newer version that replaced the obsolete HTTP repository URL with a HTTPS one keep the dependency version but define a mirror in your settings. 두 방법에 대해 알아보고 정확히 이해하기에는 시간이 걸릴 것 같아 일단 3.6.3 버전을 설치하여 사용하는 방식을 선택했다.Maven 3.6.3 버전 적용 깔끔하게 모든 의존성이 다운로드 되었다.참고 자료 https://stackoverflow.com/questions/36757902/what-is-mirror-in-maven-settings-xml-file https://maven.apache.org/guides/mini/guide-multiple-repositories.html https://maven.apache.org/guides/mini/guide-mirror-settings.html" }, { "title": "IE에서 작동하지 않는 자바스크립트 코드", "url": "/posts/ie-doesnt-support-issue/", "categories": "경험하기, 이슈 노트", "tags": "Internet Explorer, ES6", "date": "2021-12-06 23:00:00 +0900", "snippet": "상황회사에서 맡은 작업을 로컬 환경에서 개발/테스트하고 개발 환경에 배포하여 잘 동작하는지 테스트해봤다. 로컬에서 여러 번의 테스트 결과 별이상 없었기 때문에 당연히 되겠지하고 ‘조회’ 버튼을 누르는데 아무런 반응이 없었다… IE 개발자 도구를 통해 콘솔을 확인해보니, 서버에서 응답받은 js 파일 중 하나에서 SCRIPT1004: ';'가 필요합니다....", "content": "상황회사에서 맡은 작업을 로컬 환경에서 개발/테스트하고 개발 환경에 배포하여 잘 동작하는지 테스트해봤다. 로컬에서 여러 번의 테스트 결과 별이상 없었기 때문에 당연히 되겠지하고 ‘조회’ 버튼을 누르는데 아무런 반응이 없었다… IE 개발자 도구를 통해 콘솔을 확인해보니, 서버에서 응답받은 js 파일 중 하나에서 SCRIPT1004: ';'가 필요합니다.라는 에러 메시지를 유발했다. 이 문제를 해결하고나니 콘솔 창에서 또 다른 에러 메세지인 개체가 'replaceAll' 속성이나 메서드를 지원하지 않습니다.를 맞닥들였다.원인 두 에러 메시지 모두 IE에서 지원하지 않는 자바스크립트 기능을 사용했기 때문에 발생했다. (로컬 환경에서는 크롬으로만 테스트를 해서 이상이 없었던 것이었다…) 다음은 for ... of에 대한 브라우저 호환성을 나타낸 표이다. 출처 : https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/for...of#browser_compatibility 다음은 replaceAll에 대한 브라우저 호환성을 나타낸 표이다. 출처 : https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/String/replaceAll 해당 사이트에 가보면 ES6부터는 IE11에서만 부분 지원이라고 하는데 이마저도 const, let 등의 키워드 Map, Set 정도인 것 같다. w3cschool에는 Internet Explorer does not support ECMAScript 2015.라고 되어있다.해결 방법 Babel과 같은 트랜스컴파일러를 사용하는 방법도 있겠지만, 학습이 부족하므로 일단 문법적으로 해결할 수 있는 방법에 대해 살펴보자. (그냥 IE를 버리자. )for … of 대신 for … in 사용 Before for(const order of orders) { ...} After for(const idx in orders) { const order = orders[idx]; ...} replaceAll 대신 replace 함수와 정규 표현식 사용 Before const price = value.replaceAll(',', ''); // \"10,000,000\" -&gt; 10000000 After const price = value.replace(/,/gi, ''); // \"10,000,000\" -&gt; 10000000 ※ 해당 사이트에서 아래와 같이 특정 기능에 대한 브라우저 호환성에 대해 확인해 볼 수 있다. 출처 : https://caniuse.com/ 참고 자료 https://caniuse.com/ https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/String/replaceAll https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/for…of#browser_compatibility" }, { "title": "스프링 배치 핵심 개념 살펴보기", "url": "/posts/spring-batch-domain-language/", "categories": "", "tags": "", "date": "2021-11-23 22:25:00 +0900", "snippet": "들어가기 전해당 글은 스프링 배치 공식 문서의 내용을 공부하면서 정리한 글입니다.일반적인 배치 구조 아래 다이어그램은 수십 년 동안 사용된 배치 참조 아키텍처를 단순하게 표현한 것으로, 배치 처리에 필요한 구성 요소의 개요를 보여준다. Job에는 하나의 단계부터 여러 단계가 있으며, 각 단계에는 하나의 ItemReader, ItemProcessor...", "content": "들어가기 전해당 글은 스프링 배치 공식 문서의 내용을 공부하면서 정리한 글입니다.일반적인 배치 구조 아래 다이어그램은 수십 년 동안 사용된 배치 참조 아키텍처를 단순하게 표현한 것으로, 배치 처리에 필요한 구성 요소의 개요를 보여준다. Job에는 하나의 단계부터 여러 단계가 있으며, 각 단계에는 하나의 ItemReader, ItemProcessor, ItemWriter가 있다. JobLauncher를 사용하여 작업을 시작하고, 현재 실행 중인 프로세스에 대한 메타데이터를 JobRepository에 저장해야 한다.Job Job은 전체 배치 프로세스를 캡슐화하는 엔티티이다. 다른 Spring 프로젝트와 마찬가지로 Job은 XML 구성 파일 또는 Java 기반 구성과 함께 연결된다. Job은 다음 다이어그램과 같이 전체 계층의 맨 위에 있다. Job은 Step 인스턴스를 위한 컨테이너 역할을 한다. 즉, 논리적으로 함께 속하는 여러 Step을 결합하고 ‘재시작 가능성’과 같은 모든 단계에 대한 속성을 전체적으로 구성할 수 있다. Job에 대한 속성에는 다음이 포함된다. Job의 간단한 이름 Step 인스턴스의 정의 및 순서 Job을 재시작할 수 있는지 여부 Java 구성을 사용하는 경우, Spring Batch는 Job 인터페이스 위에 일부 표준 기능을 더한 SimpleJob 클래스 형태로 제공한다.package org.springframework.batch.core;import org.springframework.lang.Nullable;public interface Job { String getName(); boolean isRestartable(); void execute(JobExecution var1); @Nullable JobParametersIncrementer getJobParametersIncrementer(); JobParametersValidator getJobParametersValidator();}package org.springframework.batch.core.job;public class SimpleJob extends AbstractJob { private List&lt;Step&gt; steps; public SimpleJob() { this((String)null); } public SimpleJob(String name) { super(name); this.steps = new ArrayList(); } public void setSteps(List&lt;Step&gt; steps) { this.steps.clear(); this.steps.addAll(steps); } public Collection&lt;String&gt; getStepNames() { List&lt;String&gt; names = new ArrayList(); Iterator var2 = this.steps.iterator(); while(var2.hasNext()) { Step step = (Step)var2.next(); names.add(step.getName()); if (step instanceof StepLocator) { names.addAll(((StepLocator)step).getStepNames()); } } return names; } public void addStep(Step step) { this.steps.add(step); } public Step getStep(String stepName) { Iterator var2 = this.steps.iterator(); while(var2.hasNext()) { Step step = (Step)var2.next(); if (step.getName().equals(stepName)) { return step; } if (step instanceof StepLocator) { Step result = ((StepLocator)step).getStep(stepName); if (result != null) { return result; } } } return null; } protected void doExecute(JobExecution execution) throws JobInterruptedException, JobRestartException, StartLimitExceededException { StepExecution stepExecution = null; Iterator var3 = this.steps.iterator(); while(var3.hasNext()) { Step step = (Step)var3.next(); stepExecution = this.handleStep(step, execution); if (stepExecution.getStatus() != BatchStatus.COMPLETED) { break; } } if (stepExecution != null) { if (logger.isDebugEnabled()) { logger.debug(\"Upgrading JobExecution status: \" + stepExecution); } execution.upgradeStatus(stepExecution.getStatus()); execution.setExitStatus(stepExecution.getExitStatus()); } }} 자바 기반 구성을 사용할 때, 아래의 예와 같이 Job의 인스턴스화에 빌더 컬렉션을 사용할 수 있다.@Beanpublic Job footballJob() { return this.jobBuilderFactory.get(\"footballJob\") .start(playerLoad()) .next(gameLoad()) .next(playerSummarization()) .build();}@Beanpublic Job importUserJob(JobCompletionNotificationListener listener, Step step1) { return jobBuilderFactory.get(\"importUserJob\") .incrementer(new RunIdIncrementer()) .listener(listener) .flow(step1) .end() .build();}JobInstance JobInstance는 논리적 작업 실행 단위이다. 위에서 살펴본 다이어그램의 ‘EndOfDay’ 작업과 같이 하루가 끝날 때 한 번 실행해야 하는 배치 작업을 생각해보자. ‘EndOfDay’ 작업이 하나 있지만 각 작업 실행은 개별적으로 추적해야 한다. 이러한 경우, 하루에 하나의 논리적 JobInstance가 있다. 예를 들어 1월 1일 실행, 1월 2일 실행 등이 있다. 1월 1일 실행이 처음에 실패하고 다음날 다시 실행되더라도 여전히 1월 1일 실행이다.(보통 1월 1일 실행이 1월 1일 데이터를 처리한다는 의미). 따라서 각 JobInstance는 여러 번의 실행을 가질 수 있으며, 특정 JobParameters를 식별하는 JobInstance 하나만 한 번에 실행할 수 있다. JobInstance는 로드할 데이터와 전혀 관련이 없다. 데이터가 로드되는 방법을 결정하는 것은 전적으로 ItemReader 구현에 달려 있다. 예를 들어 EndOfDay 시나리오에서는 데이터에 데이터가 속한 ‘유효 날짜’ 또는 ‘예약 날짜’를 나타내는 컬럼이 있을 수 있다. 위 케이스의 경우, 1월 1일 런은 1월 1일 데이터만 로드하고 1월 2일 런은 2일 데이터만 로드한다. 이러한 결정은 비즈니스 관점에서의 결정일 가능성이 높기 때문에, ItemReader의 구현에 달려 있다. 동일한 JobInstance를 사용하면 이전 실행의 ‘상태’(ExecutionContext)가 사용되는지 여부가 결정된다. 새로운 JobInstance를 사용하는 것은 ‘처음부터 시작’을 의미하며, 기존 인스턴스를 사용하는 것은 일반적으로 ‘멈춘 곳부터 시작’을 의미한다.JobParameters" }, { "title": "도메인 모델과 유스케이스", "url": "/posts/use-case-and-domain-model/", "categories": "", "tags": "", "date": "2021-10-31 23:00:00 +0900", "snippet": "들어가기 전지금까지는 애플리케이션을 만들 때, 다른 사람들이 해놓은 설계를 기반으로 만들거나 그냥 주먹구구식으로 필요할 것 같은 클래스를 만들고 그 안에 속성, 기능이 무엇일지 생각했던 것 같다.『객체지향의 사실과 오해』를 읽다보니 도메인 모델과 유스케이스라는 개념이 나오는데, 애플리케이션 제대로 설계하려면 꼭 알고있어야하는 개념인 것 같아 정리해보려...", "content": "들어가기 전지금까지는 애플리케이션을 만들 때, 다른 사람들이 해놓은 설계를 기반으로 만들거나 그냥 주먹구구식으로 필요할 것 같은 클래스를 만들고 그 안에 속성, 기능이 무엇일지 생각했던 것 같다.『객체지향의 사실과 오해』를 읽다보니 도메인 모델과 유스케이스라는 개념이 나오는데, 애플리케이션 제대로 설계하려면 꼭 알고있어야하는 개념인 것 같아 정리해보려고 한다.도메인 모델 시스템을 ‘기능’과 ‘구조’의 관점에서 생각했을 때, 도메인 모델은 시스템의 구조를 표현하기 위한 수단이다.도메인이란 ‘사용자가 프로그램을 사용하는 대상 분야’를 의미한다. 도메인의 쉬운 예로 금융, 게임, 쇼핑 등이 있을 것이다.도메인 모델 = 도메인 + 모델 모델은 어떤 대상을 추상화하고 단순화한 것이다. 따라서 도메인 모델은 도메인과 관련된 부분을 추상화한 것이다. 도메인과 관련되지 않은 사항들에 대해서는 무시함으로써, 현실 세계의 복잡성을 관리할 수 있다. 도메인 모델에는 도메인과 관련된 이해관계자들이 도메인에 대해 생각하는 관점이 반영되어야 한다. 즉, 도메인 모델은 이해관계자들이 해당 도메인을 바라보는 멘탈 모델(Mental Model)이다. 멘탈 모델이란 사람들이 자기 자신, 다른 사람, 환경, 자신이 상호작용하는 사물들에 대해 갖는 모형이다. 사용자들은 자신이 이용하는 시스템이 자신의 멘탈 모델과 유사한 방식으로 작동할 것으로 기대한다. 따라서 좋은 시스템 디자인이란 사용자가 예상하는 방식으로 반응하는 제품을 만드는 것이다. 도메인 모델과 객체지향 위에서 살펴본 것 처럼 우리가 만들 시스템(최종 제품)에는 도메인에 대한 사용자 관점이 반영되어야 한다.이를 소프트웨어 개발 관점에서 생각해보면, 애플리케이션은 도메인 모델을 기반으로 설계되어야 한다는 것을 의미한다. 객체지향 패러다임은 사용자가 도메인을 바라보는 관점 / 설계자가 시스템 구조를 바라보는 관점 / 구현된 코드의 모습을 모두 모델링 할 수 있는 강력한 모델링 패러다임이다. 즉 객체지향 패러다임은 사용자 관점, 설계자 관점, 코드의 모습을 모두 유사한 형태로 유지할 수 있게 하는 사고도구와 프로그래밍 기법을 제공한다. 표현적 차이 소프트웨어 객체는 현실 객체에 대한 추상화가 아니다. 즉, 소프트웨어 객체는 현실 객체를 모방한 것이 아니라 은유를 기반으로 재창조한 것이다.따라서, 소프트웨어 객체는 현실 객체가 갖지 못한 특성을 가질 수도 있고, 현실 객체가 하지 못하는 행동을 할 수도 있다. 은유(metaphor) : 경험, 생각, 감정, 행동 또는 대상 간의 유사성을 암시하는 상징적인 접근 방식 표현적 차이(의미적 차이) 소프트웨어 객체와 현실 객체 사이의 의미적 거리이다. 도메인 모델이 중요한 이유는 은유를 통해 표현적 차이를 줄일 수 있기 때문이다. 즉, 사용자의 멘탈 모델이 코드에 더 잘 녹아들 수 있다. 결과적으로 표현적 차이가 줄어들수록 코드의 구조가 도메인의 구조를 잘 반영하게된다. 이를 통해 이해하고 수정하기 쉬운 소프트웨어를 만들 수 있다. 안정적인 도메인 모델 도메인 모델을 기반으로 코드를 작성하면 상대적으로 안정적인 구조를 만들 수 있다. 도메인 모델에 도메인에 대한 사용자의 관점을 반영해야 하는 이유는 사용자들이 도메인의 본질에 대해 가장 잘 이해하기 때문이다. 본질은 변경이 적고 비교적 그 특성이 오랜 시간 유지된다는 것을 의미한다. 소프트웨어를 개발하고 유지보수함에 있어서 변경은 피할 수 없는 숙명같은 것이다. 하지만, 도메인 모델에 포함된 개념과 규칙은 도메인의 ‘본질’을 다루기 때문에 변경될 확률이 적다. 변경된다 하더라도 변경에 쉽게 대처할 수 있을 가능성이 커진다. 도메인 모델을 기반으로 소프트웨어 구조를 설계하면, 안정적인 구조 위에 (비교적) 자주 변경되는 기능을 배치할 수 있다. 이와 같은 설계는 변경에 유연한 소프트웨어를 만들 수 있게 해준다. 유스케이스 소프트웨어의 존재 이유는 사용자가 원하는 기능을 제공하기 위함이다. 유스케이스는 사용자와 시스템 간의 상호작용 흐름을 텍스트로 정리한 것이다. 유스케이스의 가치는 사용자 목표를 중심으로 시스템의 기능적인 요구사항들을 이야기 형식으로 묶을 수 있다는 점이다. 즉, 사용자 목표를 통해 각각의 기능을 유기적인 관계를 지닌 체계로 발전시킨다. 유스케이스의 특성1. 유스케이스는 사용자와 시스템간의 상호작용을 나타내는 ‘텍스트’이다. 유스케이스의 핵심은 다이어그램이 아닌 사용자와 시스템 간의 상호작용을 이야기 흐름으로 표현하는 것이다.2. 유스케이스는 여러 시나리오들의 집합이다. 예를 들어, ‘이자 계산’이라는 유스케이스는 다음 두 개의 시나리오로 구성할 수 있다. 예금주가 계좌를 선택하고 당일까지의 이자액을 계산 예금주가 계좌를 선택하고 특정 일자까지의 이자액을 계산 즉, 유스케이스는 특정한 사용자의 목표(위 예시에선 이자 계산)와 관련된 모든 시나리오의 집합이다.3. 유스케이스는 단순한 기능 목록이 아니다. 기능만 나열하는 것은 각각의 기능을 서로 연관이 없는 것처럼 보이게 만든다. 시스템은 정기예금 정보를 보여준다, 시스템은 당일이나 현재 일자의 이자를 계산한다 라는 두 기능을 중도 해지 이자액을 계산한다라는 유스케이스로 묶을 수 있다. 이를 통해, 시스템의 기능에 대해 의사소통할 수 있는 문맥을 얻을 수 있다. 4. 유스케이스는 사용자 인터페이스와 관련된 세부 정보를 포함하지 말아야 한다. 자주 변경되는 사용자 인터페이스 요소는 배제하고, 사용자 관점에서 시스템의 행위에 초첨을 맞춘다.5. 유스케이스는 내부 설계와 관련된 정보를 포함하지 않는다. 유스케이스의 목적은 내부 설계를 설명하는 것이 아니다. 과거에는 유스케이스에 나타나는 명사를 클래스로, 동사를 메서드로 대응시키는 방식으로 객체지향 설계를 설명하기도 했지만 객체지향 설계는 이렇게 단순하지 않다. 유스케이스에서 객체 설계로의 전환은 경험, 상식, 의사소통을 기반으로 한 창조 작업이다. 물론 유스케이스 안에서 도메인 모델에 사용할 용어에 대한 힌트를 얻을 수도 있다. 하지만 유스케이스 안에 도메인 모델을 구축할 수 있는 모든 정보가 포함되어 있진 않다. 기능과 구조의 통합 변경에 유연한 소프트웨어를 만들기 위해서는 유스케이스에 정리된 시스템의 기능을 도메인 모델을 기반으로 한 객체들의 책임으로 분배해야 한다.책임 주도 설계 책임 주도 설계 방법은 시스템의 기능을 역할과 책임을 수행하는 객체들의 협력 관계로 바라볼 수 있게 한다.이를 통해 유스케이스와 도메인 모델을 통합한다. 사용자의 관점에서 시스템은 자신이 전송한 메시지에 응답하는 데 필요한 책임을 수행하는 일종의 객체다. 시스템이 수행해야 하는 커다란 규모의 책임은 더 작은 크기의 객체들의 협력을 통해 구현될 수 있다. 객체들을 식별함에 있어 도메인 모델을 사용할 수 있다. 유스케이스를 통해 협력을 완성하는 데 필요한 메시지를 식별하면서 객체들에게 책임을 할당한다. 결과적으로, 코드는 불안정한 기능을 수용할 수 있는 안정적인 구조가 된다. " }, { "title": "숫자 야구 게임 구현하기 (2)", "url": "/posts/number-baseball-part2/", "categories": "경험하기, 작업 노트", "tags": "", "date": "2021-10-26 22:00:00 +0900", "snippet": "들어가기 전내 마음대로 구현해봤던 숫자 야구 게임 구현하기 1편에 이어, 박재성님의 피드백 강의를 듣고 느낀점과 TDD 방식으로 개발하면서 느낀점을 정리해보려고 한다.※ TDD 연습해본 코드피드백 강의를 통해 배운 것TDD 관련 테스트 코드 작성은 잘 정리된 요구사항을 기반으로 할 때 수월하다. DB, UI, 랜덤값 등과 같이 테스트하기 어려운 부...", "content": "들어가기 전내 마음대로 구현해봤던 숫자 야구 게임 구현하기 1편에 이어, 박재성님의 피드백 강의를 듣고 느낀점과 TDD 방식으로 개발하면서 느낀점을 정리해보려고 한다.※ TDD 연습해본 코드피드백 강의를 통해 배운 것TDD 관련 테스트 코드 작성은 잘 정리된 요구사항을 기반으로 할 때 수월하다. DB, UI, 랜덤값 등과 같이 테스트하기 어려운 부분보다는 util성 메서드, input/output 명확한 것 등 테스트하기 쉬운 영역부터 해나가는 것이 좋다. 큰 단위로 TDD를 하려고하기 때문에 힘든 것이다. 최대한 작은 단위로 쪼개보자. 테스트하기 어려운 영역과 테스트하기 쉬운 영역을 분리(격리)할 수 있어야한다.도메인 모델 관련 나는 게임에 참여하는 주체인 Player, Opponent 등에 초점을 맞췄었다. 강의에서는 게임의 참여하는 주체가 아닌, 야구 게임에서 활용되는 숫자에 초점을 맞췄다. 즉, 하나의 숫자를 Ball 객체로, 여러 개의 숫자를 Balls 객체로 바라보았다. 같은 애플리케이션을 어떤 관점에서 보는지에 따라 완전히 다른 설계가 가능하다는게 흥미로운 것 같다.기타 데이터를 가져와서 무언가 하려고하지 말고, 해당 데이터를 갖고있는 객체에게 메시지를 전달하는 방식으로 하자. 요구사항 분석을 통해 대략적인 설계 : 객체 추출 UI, DB 등과 의존관계를 가지지 않는 핵심 도메인 영역 집중 설계TDD로 구현해보기 1~4번을 반복하면서 개발한다. 요구사항을 기반으로한 테스트 코드 작성 컴파일 에러 해결 테스트 케이스를 어떻게든 통과하도록 로직 작성 리팩토링요구 사항 정리기본적으로 1부터 9까지 서로 다른 수로 이루어진 3자리의 수를 맞추는 게임이다. 같은 수가 같은 자리에 있으면 스트라이크, 다른 자리에 있으면 볼, 같은 수가 전혀 없으면 포볼 또는 낫싱이란 힌트를 얻고, 그 힌트를 이용해서 먼저 상대방(컴퓨터)의 수를 맞추면 승리한다. e.g. 상대방(컴퓨터)의 수가 425일 때, 123을 제시한 경우 : 1스트라이크, 456을 제시한 경우 : 1볼 1스트라이크, 789를 제시한 경우 : 낫싱 위 숫자 야구 게임에서 상대방의 역할을 컴퓨터가 한다. 컴퓨터는 1에서 9까지 서로 다른 임의의 수 3개를 선택한다. 게임 플레이어는 컴퓨터가 생각하고 있는 3개의 숫자를 입력하고, 컴퓨터는 입력한 숫자에 대한 결과를 출력한다. 이 같은 과정을 반복해 컴퓨터가 선택한 3개의 숫자를 모두 맞히면 게임이 종료된다. 게임을 종료한 후 게임을 다시 시작하거나 완전히 종료할 수 있다.느낀점 요구사항이 있어도 어떤 것 부터 테스트코드로 작성해야 할지 잘 모르겠다. 일단 작은 단위부터 테스트 하자. 예를 들어 아래와 같은 기능을 구현해야 한다면, 숫자 하나씩 있는 것 부터 도전해보자. com = Balls(Arrays.asList(1,2,3));user = Balls(Arrays.asList(1,3,2));com.play(user); // 1 strike, 2 ball 강의로 볼 땐 수월해보였는데 막상 혼자 하려니 막막하다. 스스로 고민도 많이 해보고 꾸준한 연습이 필요할 것 같다. 커밋 단위를 잘 조절하자. 하다보니 커밋하지 않고 너무 많은 코드를 작성했다." }, { "title": "숫자 야구 게임 구현하기 (1)", "url": "/posts/number-baseball-part1/", "categories": "경험하기, 작업 노트", "tags": "", "date": "2021-10-16 23:00:00 +0900", "snippet": "들어가기 전박재성님이 운영하시는 NextStep의 자바 플레이그라운드 with TDD, 클린코드를 몇 달 전에 끊어놨지만 이직 준비하느라 이제서야 실습해본다.첫 번째 단계인 숫자 야구 게임을 먼저 요구사항만 보고 TDD 없이 구현해보려고 한다. 그리고 조영호님께서 쓴 『객체지향의 사실과 오해』에서 읽은 유스케이스와 도메인 모델 개념을 활용해서 애플리케...", "content": "들어가기 전박재성님이 운영하시는 NextStep의 자바 플레이그라운드 with TDD, 클린코드를 몇 달 전에 끊어놨지만 이직 준비하느라 이제서야 실습해본다.첫 번째 단계인 숫자 야구 게임을 먼저 요구사항만 보고 TDD 없이 구현해보려고 한다. 그리고 조영호님께서 쓴 『객체지향의 사실과 오해』에서 읽은 유스케이스와 도메인 모델 개념을 활용해서 애플리케이션을 설계해보고자 한다.이 단계를 마치면 박재성님의 피드백 강의를 듣고 TDD 방식으로 다시 개발해보려고 한다.기능 요구 사항기본적으로 1부터 9까지 서로 다른 수로 이루어진 3자리의 수를 맞추는 게임이다. 같은 수가 같은 자리에 있으면 스트라이크, 다른 자리에 있으면 볼, 같은 수가 전혀 없으면 포볼 또는 낫싱이란 힌트를 얻고, 그 힌트를 이용해서 먼저 상대방(컴퓨터)의 수를 맞추면 승리한다. e.g. 상대방(컴퓨터)의 수가 425일 때, 123을 제시한 경우 : 1스트라이크, 456을 제시한 경우 : 1볼 1스트라이크, 789를 제시한 경우 : 낫싱 위 숫자 야구 게임에서 상대방의 역할을 컴퓨터가 한다. 컴퓨터는 1에서 9까지 서로 다른 임의의 수 3개를 선택한다. 게 임 플레이어는 컴퓨터가 생각하고 있는 3개의 숫자를 입력하고, 컴퓨터는 입력한 숫자에 대한 결과를 출력한다. 이 같은 과정을 반복해 컴퓨터가 선택한 3개의 숫자를 모두 맞히면 게임이 종료된다. 게임을 종료한 후 게임을 다시 시작하거나 완전히 종료할 수 있다.유스케이스 숫자야구 게임을 시작한다. 플레이어가 숫자를 제시했을 때, 상대방은 해당 숫자에 대한 결과를 리턴해준다. 제시할 수 있는 숫자는 1~9로만 구성된 세 자리 수이다. 도메인 모델의식의 흐름 1 Player / Messenger / Opponent / GameManager Player가 입력한 숫자를 Opponent가 받기 위해서는 중간에 매개하는 역할이 있다고 생각. 만약 직접 전달하는거라고 하면 Player와 Opponent에서 수행하는 로직에 System.out.print와 같은 메서드가 들어가게 될 것이다. 이는 비즈니스 로직의 흐름을 방해할 뿐더러, Player와 Opponent가 수행해야할 책임도 아니라고 생각했다. 또한 입,출력 방식이 바뀌는 경우 Player와 Opponent 곳곳에 산재되어 있는 관련 코드들을 일일이 수정해야 할 것 이다. 따라서, Messenger라는 매개체를 통해 Player와 Opponent는 소통하는 것이 좀 더 자연스럽고 유지보수하기에도 좋다고 생각했다. 서로의 메시지를 전달하는 수단이 현재는 console view 이지만, 추후에 수단이 바뀌더라도 Player와 Opponent의 코드는 변경될 필요가 없다. Player와 Opponent에는 비즈니스 로직과 관련된 코드만 있게된다. 이렇게 하고 나니, 게임을 시작하고 종료하는 것에 대한 책임은 어떤 객체가 가져야할지에 대한 것이 남게되었다. 게임을 중재하는 역할을 위한 객체를 별도로 만들어야하나? 라고 생각했지만 Messenger의 역할을 좀 더 넓은 범위로 추상화해서 GameManager라는 객체를 만드는게 어떨가 생각해봤다. 의식의 흐름 2 현실 세계에서는 게임을 조작하려면 사용자와 같은 누군가가 ‘실행’하고 ‘종료’하는 행위를 하기 때문에 위에서 GameManager를 생각하려 했던 것 같다. 하지만, ‘객체지향의 사실과 오해’에서 봤듯이 현실 세계와 객체는 완전히 일치하기가 어렵고 그럴 필요도 없다고 했다. (표현적 차이) 따라서, GameManager 대신 그냥 BaseBallGame이라는 게임 객체가 게임을 실행하고, 종료하는 역할을 하게 하면 될 것 같다. 책임 분배 Player 게임을 시작, 종료 의사를 밝힌다. 숫자를 Messenger에게 전달한다. 유효한 숫자인지 검증한다. BaseBallGame 게임 시작, 또는 종료 Player에게 시작/종료 의사를 묻는다. Player -&gt; Opponent로 숫자 전달 Opponent -&gt; View로 결과 전달 Opponent 전달받는 숫자에 대한 야구 결과를 리턴한다. 프로그래밍 요구사항 자바 코드 컨벤션을 지키면서 프로그래밍한다. 기본적으로 Google Java Style Guide을 원칙으로 한다. 단, 들여쓰기는 ‘2 spaces’가 아닌 ‘4 spaces’로 한다. indent(인덴트, 들여쓰기) depth를 2가 넘지 않도록 구현한다. 1까지만 허용한다. 예를 들어 while문 안에 if문이 있으면 들여쓰기는 2이다. 힌트: indent(인덴트, 들여쓰기) depth를 줄이는 좋은 방법은 함수(또는 메소드)를 분리하면 된다. else 예약어를 쓰지 않는다. 힌트: if 조건절에서 값을 return하는 방식으로 구현하면 else를 사용하지 않아도 된다. else를 쓰지 말라고 하니 switch/case로 구현하는 경우가 있는데 switch/case도 허용하지 않는다. 모든 로직에 단위 테스트를 구현한다. 단, UI(System.out, System.in) 로직은 제외 핵심 로직을 구현하는 코드와 UI를 담당하는 로직을 구분한다. UI 로직을 InputView, ResultView와 같은 클래스를 추가해 분리한다. 3항 연산자를 쓰지 않는다. 함수(또는 메소드)가 한 가지 일만 하도록 최대한 작게 만들어라. 구현시 어려웠던 부분 숫자에 대해 1스트라이크, 2볼 등에 대한 결과를 반환하는 메서드를 구현할 때 indent(인덴트, 들여쓰기) depth를 2가 넘지 않도록 구현한다. 1까지만 허용한다 이 규칙을 지키기가 어렵다. (3항 연산자를 쓰지 않는다. 라는 규칙도 있기 떄문에..) play() 내부에 UI 관련 로직이 들어가버렸다 … 분리할수는 없을까 구현해놓고 보니 테스트 하기가 어렵다 예를 들어 아래와 같은 메서드에 대해서는 어떻게 테스트해야할까 ?? 그리고 왜 현재는 테스트하기가 어려운 코드인걸까 ?? ‘테스트’라는 것은 결국 특정 input 일 때 특정 output이 나오는지 확인하는 것이라고 생각하는데 keepContinue() 메서드의 경우, input을 외부에서 입력할 수 없기 때문에 어떤 input에 대한 output이 무엇이다 라고 작성할 수가 없다. public class Player { private final Scanner sc = new Scanner(System.in); public boolean keepContinue() { int continueFlag = sc.nextInt(); if(continueFlag==1) { return true; } return false; } public int getPredictNumber() { return sc.nextInt(); }} 근데 테스트를 특정 input 일 때 특정 output이 나오는지 확인하는 것라고 정의하면 리턴값 void에 대한 테스트는 어떻게해야하지 ?? (setNumber이런거) " }, { "title": "MySQL - MySQL 실행 계획 (3)", "url": "/posts/execution-plan-part3/", "categories": "지식 더하기, 이론", "tags": "MySQL", "date": "2021-10-09 23:00:00 +0900", "snippet": "들어가기 전실행 계획을 나타내는 테이블의 다양한 컬럼들 중 possible_keys, key, key_len, ref, rows, Extra 컬럼에 대해 살펴볼 것이다. 또한 EXPLAIN 명령 이외에 EXPLAIN EXTENDED, EXPLAIN PARTITIONS 대해서도 알아보자.※ 실행 계획 1편, 실행 계획 2편을 먼저 읽으실 것을 권장합니다...", "content": "들어가기 전실행 계획을 나타내는 테이블의 다양한 컬럼들 중 possible_keys, key, key_len, ref, rows, Extra 컬럼에 대해 살펴볼 것이다. 또한 EXPLAIN 명령 이외에 EXPLAIN EXTENDED, EXPLAIN PARTITIONS 대해서도 알아보자.※ 실행 계획 1편, 실행 계획 2편을 먼저 읽으실 것을 권장합니다.possible_keys 컬럼 옵티마이저가 최적의 실행 계획을 만들기 위해 후보로 선정했던 접근 방식에서 사용되는 인덱스의 목록이다. 즉, 사용될 법했던 인덱스의 목록이다. 해당 테이블의 모든 인덱스가 포함되어 나오는 경우가 많기 때문에 쿼리를 튜닝하는 데 도움이 되지 않는다.key 컬럼 최종 선택된 실행 계획에서 사용하는 인덱스이다. 쿼리를 튜닝할 때는 key 컬럼에 의도했던 인덱스가 표시되는지 확인하는 것이 중요하다. 값이 ‘PRIMARY’인 경우에는 프라이머리 키를 사용한다는 의미이다. 그 외에는 모두 테이블이나 인덱스를 생성할 때 부여했던 고유 이름이다. 테이블 풀 스캔 방식 처럼 인덱스를 사용하지 못하는 경우에는 ‘NULL’로 표시된다. type 컬럼의 값이 ‘index_merge’인 경우에는 2개 이상의 값이 표시된다. key_len 컬럼 단일, 다중 컬럼으로 구성된 인덱스에서 몇 개의 컬럼까지 사용했는지 알려준다. 즉, 각 레코드에서 몇 바이트까지 사용했는지를 알려주는 값이다. dept_emp 테이블의 프라이머리 키는 dept_no, emp_no 두 개의 컬럼으로 구성되어있다. dept_no 컬럼의 타입은 CHAR(4)이며 UTF8 문자집합을 사용한다. 아래 쿼리는 조회시 dept_no만 사용하고 있으므로 결과적으로 key_len의 값은 12로 표시된다. 즉, UTF8 문자를 위한 메모리 공간은 3바이트이기 때문에 3*4=12(byte) 라는 값이 도출된다. EXPLAINSELECT * FROM dept_emp WHERE dept_no='d005'; id select_type table type key key_len ref rows Extra 1 SIMPLE dept_emp ref PRIMARY 12 const 53288 Using where 프라이머리 키를 구성하는 두 개의 컬럼을 다 사용한 경우이다. emp_no 컬럼 타입은 INTEGER이며 4바이트를 차지한다. EXPLAINSELECT * FROM dept_emp WHERE dept_no='d005' AND emp_no=10001; id select_type table type key key_len ref rows Extra 1 SIMPLE dept_emp ref PRIMARY 16 const,const 1   MySQL 5.0 vs MySQL 5.1 아래와 같은 쿼리가 있을 때 MySQL 5.0 버전과 5.1 버전에서의 결과가 어떤 차이가 있는지 살펴보자.dept_emp 테이블의 프라이머리 키는 dept_no, emp_no으로 구성된다.EXPLAINSELECT * FROM dept_emp WHERE dept_no='d005' AND emp_no &lt;&gt; 10001; MySQL 5.0 이하 프라이머리 키인 dept_no, emp_no을 모두 사용했지만 key_len은 16이 아닌 12로 표시된다. 그 이유는 key_len에 표시되는 값은 인덱스를 이용해 범위를 제한하는 조건의 컬럼까지만 포함되기 때문이다. 즉, 체크 조건(emp_no &lt;&gt; 10001)으로 사용되는 emp_no의 경우는 key_len에 포함되지 않는다. 결과적으로 5.0 이하 버전에서는 key_len 컬럼의 값으로 인덱스의 몇 바이트까지가 범위 제한 조건으로 사용됐는지 판단할 수 있다. id select_type table type key key_len ref rows Extra 1 SIMPLE dept_emp ref PRIMARY 12 const 53298 Using where MySQL 5.1 이상 체크 조건에 포함되는 컬럼까지 계산된 값이 key_len에 표시된다. 결과적으로 인덱스의 몇 바이트까지가 범위 제한 조건으로 사용됐는지 알 수 없다. id select_type table type key key_len ref rows Extra 1 SIMPLE dept_emp range PRIMARY 16   53298 Using where 컨디션 푸시 다운(Condition push down) 위에서 살펴본 차이는 버전이 올라가면서 MySQL 엔진과 InnoDB 스토리지 엔진의 역할 분담에 변화가 생겼기 때문이다. 5.0 버전까지는 범위 제한 조건으로 사용되는 컬럼만 스토리지 엔진으로 전달했다. 5.1 버전부터는 범위 제한 조건이든 체크 조건이든 인덱스를 이용할 수만 있다면 모두 스토리지 엔진으로 전달한다. 이를 컨디션 푸시 다운이라고 한다. ref 컬럼 type 컬럼의 값(접근 방식)이 ‘ref’이면 ref 컬럼에서는 어떤 값이 제공됐는지 표시된다. 상수 값을 지정했다면 ref 컬럼의 값은 const로 표시된다. 다른 테이블의 컬럼이라면 그 테이블 명과 컬럼 명이 표시된다.EXPLAINSELECT * FROM employees e, dept_emp deWHERE e.emp_no=de.emp_no; id select_type table type key key_len ref rows Extra 1 SIMPLE de ALL       334868   1 SIMPLE e eq_ref PRIMARY 4 de.emp_no 1   만약 ref 컬럼의 값이 ‘func’라면 이는 참조용으로 사용되는 값을 그대로 사용한 것이 아니라, 콜레이션 변환이나 값 자체의 연산을 거쳐서 참조됐다는 것을 의미한다. 사용자가 명시적으로 값을 변환할 때뿐만 아니라 MySQL 서버가 내부적으로 값을 변환해야 할 때도 ‘func’가 출력된다. 대표적인 경우는 다음과 같다. 문자집합이 일치하지 않는 두 문자열 컬럼을 조인하는 경우 숫자 타입의 컬럼과 문자열 타입의 컬럼으로 조인하는 경우 가능한 MySQL이 이런 변환을 하지 않아도 되도록 조인 컬럼의 타입은 일치시키는 것이 좋다.EXPLAINSELECT * FROM employees e, dept_emp deWHERE e.emp_no=(de.emp_no-1); id select_type table type key key_len ref rows Extra 1 SIMPLE de ALL       334868   1 SIMPLE e eq_ref PRIMARY 4 func 1   rows 컬럼 실행 계획의 효율성 판단을 위해 예측했던 레코드 건수를 보여준다. 각 스토리지 엔진별로 갖고 있는 통계 정보를 참조해 옵티마이저가 산출한 예상 값이라 정확하지는 않다. 각 실행 계획의 비용을 산정하는 방법은 각 처리 방식이 얼마나 많은 레코드를 읽고 비교해야 하는지 예측해 보는 것이다. 즉, 대상 테이블에 얼마나 많은 레코드가 포함되는지, 인덱스 값의 분포도가 어떤지를 기준으로 조사해서 예측한다. rows 컬럼에 표시되는 값은 반환하는 레코드의 예측치가 아니라, 쿼리를 처리하기 위해 얼마나 많은 레코드를 디스크로부터 읽고 체크해야 하는지를 의미한다. 아래 쿼리의 경우 ix_fromdate 인덱스를 사용할 수도 있었지만, 결과적으로는 테이블 풀 스캔(ALL) 방식을 사용한다. dept_emp 테이블의 전체 레코드는 331,603 건이라고 한다. rows 컬럼을 보면 옵티마이저는 이 쿼리를 처리하기 위해 대략 334,868건의 레코드를 읽어야 할 것이라고 예측했다. 즉, 옵티마이저는 전체 레코드의 대부분을 비교해봐야 한다고 판단했기 때문에 인덱스 레인지 스캔이 아닌 ‘ALL’을 선택했다. EXPLAINSELECT * FROM dept_emp WHERE from_date&gt;='1985-01-01'; id select_type table type possible_keys key key_len ref rows Extra 1 SIMPLE dept_emp ALL ix_fromdate       334868 Usingwhere 위의 쿼리에서 범위를 좁혀보자. 옵티마이저는 292 건의 레코드(전체 레코드의 8.8%)를 읽으면 아래 쿼리를 처리할 수 있을 것으로 예측했다. 따라서, 인덱스 레인지 스캔 방식을 선택했다. EXPLAINSELECT * FROM dept_emp WHERE from_date&gt;='2002-07-01'; id select_type table type possible_keys key key_len ref rows Extra 1 SIMPLE dept_emp range ix_fromdate ix_fromdate 3   292 Usingwhere Extra 컬럼컬럼의 이름과 달리, 성능에 관련된 중요한 내용이 자주 표시된다. 다 알아보기엔 너무 많기 때문에 카테고리 별로 나눠서 몇 가지만 살펴보자. 1. 쿼리가 요건을 제대로 반영하고 있는지 확인해야 하는 경우 아래와 같은 코멘트가 표시된다면 쿼리가 요건을 제대로 반영해서 작성됐는지, 버그가 생길 가능성은 없는지 확인해야 한다.즉, 아래 항목들은 “이런 레코드가 없음”이라는 의미가 강하기 때문에, 이로 인한 버그의 가능성에 대해 검토해야 한다.한 가지 주의할 점은 ‘Impossible WHERE …’, ‘No matching …’ 등의 메시지는 쿼리의 실행 계획을 산출하기 위한 기초 자료가 없음을 표현하는 것이므로실제 쿼리 오류가 발생한다고 생각해서는 안된다.Full scan on NULL key col1 IN (SELECT ... FROM ...)과 같은 조건에서 col1이 NULL인 경우 예비책으로 풀 테이블 스캔을 사용할 것이라는 사실을 알려주는 키워드이다. col1 IN (SELECT ... FROM ...)과 같은 조건이 포함된 쿼리에서 col1이 NULL이라면? 연산을 수행하기 위해 위 조건은 다음과 같이 비교돼야 한다. 서브 쿼리가 1건이라도 결과 레코드를 가진다면 최종 비교 결과는 NULL 서브 쿼리가 1건도 결과 레코드를 가지지 않는다면 최종 비교 결과는 FALSE 위와 같은 비교 과정은 col1이 NULL이기 때문에, 풀 테이블 스캔을 해야만 결과를 알아낼 수 있다. EXPLAINSELECT d.dept_no, NULL IN (SELECT id.dpet_name FROM departments id)FROM departments d; id select_type table type key key_len ref rows Extra 1 PRIMARY d index ux_deptname 123 NULL 9 Using index 1 DEPENDENTSUBQUERY id index_subquery ux_deptname 123 const 2 Using index;Full scan on NULL key 만약 컬럼이 NOT NULL로 정의되지는 않았지만 이러한 NULL 비교 규칙을 무시해도 된다면, col1이 절대 NULL이 될 수 없다는 것을 옵티마이저에게 알려주면 된다. 대표적으로 다음과 같은 방법이 있다. col1이 NULL이면 col1 IS NOT NULL 조건이 FALSE가 되기 때문에 그 아래 조건은 실행하지 않는다. EXPLAINSELECT * FROM tb_test1WHERE col1 IS NOT NULL AND col1 IN (SELECT col2 FROM tb_test2);Impossible HAVING (5.1 버전부터) 쿼리에 사용된 HAVING절의 조건을 만족하는 레코드가 없는 경우이다. 이런 경우, 쿼리가 제대로 작성되지 못한 경우가 대부분이므로 다시 검토해보는 것이 좋다. 아래 쿼리에서 emp_no 컬럼은 프라이머리 키이면서 NOT NULL 타입의 컬럼이다. 따라서, 절대 e.emp_no IS NULL 조건을 만족할 가능성이 없다.EXPLAINSELECT e.emp_no, COUNT(*) AS cntFROM employees eWHERE e.emp_no=10001GROUP BY e.emp_noHAVING e.emp_no IS NULL; id select_type table type key key_len ref rows Extra 1 SIMPLE             Impossible HAVING Impossible WHERE (5.1 버전부터) WHERE 조건이 항상 FALSE가 될 수 밖에 없는 경우이다.EXPLAINSELECT * FROM employees WHERE emp_no IS NULL; id select_type table type key key_len ref rows Extra 1 SIMPLE             Impossible WHERE Impossible WHERE noticed after reading const tables 쿼리에서 const 접근 방식이 필요한 부분은 실행 계획 수립 단계에서 옵티마이저가 직접 쿼리의 일부를 실행하고, 실행된 결과 값을 원본 쿼리의 상수로 대체한다.이 과정을 마친 뒤, 불가능한 조건으로 판단되는 경우이다. 아래의 경우, 실행 계획만 확인했을 뿐인데, 옵티마이저는 사번이 0번인 사원이 없다는 것까지 확인한다.EXPLAINSELECT * FROM employees WHERE emp_no=0; id select_type table type key key_len ref rows Extra 1 SIMPLE             Impossible WHERE noticedafter reading const tables No matching min/max row (5.1 버전부터) MIN(), MAX()와 같은 집합 함수가 있는 쿼리의 조건절에 일치하는 레코드가 한 건도 없는 경우이다, 집합 함수의 결과로는 NULL이 반환된다.EXPLAINSELECT MIN(dept_no), MAX(dept_no)FROM dept_emp WHERE dept_no=''; id select_type table type key key_len ref rows Extra 1 SIMPLE             No matching min/max row No matching row in const table (5.1 버전부터) 조인에 사용된 테이블에서 const 방식으로 접근할 때, 일치하는 레코드가 없는 경우이다.EXPLAINSELECT *FROM dept_emp de,(SELECT emp_no FROM employees WHERE emp_no=0) tb1WHERE tb1.emp_no=de.emp_no AND de.dept_no='d005'; id select_type table type key key_len ref rows Extra 1 PRIMARY             Impossible WHERE noticedafter reading const tables 2 DERIVED             no matching row in const table Unique row not found (5.1 버전부터) 두 개의 테이블이 각각 유니크(프라이머리 키 포함) 컬럼으로 아우터 조인을 수행하는 쿼리에서, 아우터 테이블에 일치하는 레코드가 존재하지 않는 경우이다. t2 테이블에 프라이머리 키인 fdpk의 값이 1인 레코드만 있다고 가정해보자.EXPLAINSELECT t1.fdpkFROM tb_test1 t1 LEFT JOIN tb_test2 t2 ON t2.fdpk=t1.fdpkWHERE t1.fdpk=2; id select_type table type key key_len ref rows Extra 1 SIMPLE t1 const PRIMARY 4 const 1 Using index 2 SIMPLE t2 const PRIMARY 4 const 0 unique row not found 2. 쿼리의 실행 계획이 좋지 않은 경우 아래와 같은 코멘트가 표시된다면 쿼리를 더 최적화할 수 있는지 검토해보는 것이 좋다. 특히, ‘Using where’의 경우 대부분의 쿼리에서 표시되기 때문에 그냥 지나치기 쉬운데,만약 실행 계획의 rows 컬럼 값이 실제 SELECT되는 레코드 건수보다 훨씬 더 높은 경우에는 둘의 차이를 줄이는 것이 중요하다.Range checked for each record (index map:N) 매 레코드마다 인덱스 레인지 스캔을 할지, 풀 테이블 스캔을 할지 결정한다. 아래 쿼리 처럼 조인 조건에 상수가 없고 둘 다 변수인 경우, e1 테이블의 레코드를 하나씩 읽을 때 마다 e1.emp_no 값이 계속 바뀌므로 쿼리의 비용 계산을 위한 기준값이 계속 변한다. 따라서, 어떤 접근 방법으로 e2 테이블을 읽는 것이 좋을지 판단할 수 없는 것이다. ‘(index map: 0x1)’은 사용할지 말지를 판단하는 후보 인덱스의 순번을 나타낸다. 어떤 인덱스인지 확인하려면 16진수를 2진수로 바꿔야한다. 0x1은 이진수 1이기 때문에 이는 첫 번째 인덱스를 의미힌다. SHOW CREATE TABLE employees 명령어를 통해 인덱스 순서를 확인할 수 있다. type에 ‘ALL’로 표시되었지만 무조건 풀 테이블 스캔을 하는 것은 아니다. 즉, ‘Range checked for each record’인 경우에는 후보 인덱스를 사용할지를 검토해서, 인덱스를 사용하지 않는 경우 ‘ALL’ 접근 방법을 사용한다. EXPLAINSELECT *FROM employees e1, employees e2WHERE e2.emp_no &gt;= e1.emp_no; id select_type table type key key_len ref rows Extra 1 SIMPLE e1 ALL   3   300584 Using index 2 SIMPLE e2 ALL   3   300584 Range checked foreach record(index map: 0x1) Using filesort ORDER BY 처리가 인덱스를 사용하지 못하는 경우이다. 조회된 레코드를 정렬하기 위해, 정렬용 메모리 버퍼(sort buffer)에 복사해서 퀵 소트 알고리즘을 수행한다. 이러한 경우는 많은 부하를 일으킬 수 있으므로, 쿼리를 튜닝하거나 인덱스를 생성하는 것이 좋다.Using join buffer (5.1 버전부터) 조인 버퍼가 사용되는 실행 계획이다. 조인 버퍼는 읽은 레코드를 임시로 보관해두는 메모리 공간이다. 실제로 조인에 필요한 인덱스는 조인에서 뒤에 읽는 테이블(드리븐 테이블)의 컬럼에만 필요하다. 드리븐 테이블은 검색 위주로 사용되기 때문에, 인덱스가 없으면 성능에 미치는 영향이 매우 크다. 드리븐 테이블에 적절한 인덱스가 없다면, 드라이빙 테이블로부터 읽은 레코드의 건수만큼 매번 드리븐 테이블을 풀 테이블 스캔이나 인덱스 풀 스캔해야 할 것이다. 이러한 비효율성을 보완하기 위해, MySQL 서버는 드라이빙 테이블에서 읽은 레코드를 임시 공간에 보관해두고 필요할 때 재사용할 수 있게 해준다. join_buffer_size 시스템 설정 변수를 활용하여 버퍼 크기를 설정할 수 있다. 옵티마이저는 조인되는 두 테이블에 있는 인덱스를 조사하고, 인덱스가 없는 테이블이 있으면 그 테이블을 먼저 읽어서 조인을 실행한다. 다음과 같은 카테시안 조인을 수행하는 쿼리는 항상 조인 버퍼를 사용한다.EXPLAINSELECT *FROM dept_emp de, employees eWHERE de.from_date&gt;'2005-01-01' AND e.emp_no&lt;10904; id select_type table type key key_len ref rows Extra 1 SIMPLE de range ix_fromdate 3   1 Using where 2 SIMPLE e range PRIMARY 4   1520 Using where;Using join buffer Using temporary 쿼리를 처리하기 위해 임시 테이블을 사용한 것이다. MySQL은 쿼리를 처리하는 동안 중간 결과를 담아 두기 위해 임시 테이블을 사용한다. 임시 테이블이 메모리에 생성됐는지, 디스크에 생성됐는지 여부는 실행 계획만으로는 판단할 수 없다. 아래 쿼리는 GROUP BY 컬럼과 ORDER BY 컬럼이 다르기 때문에 임시 테이블이 필요한 작업이다. id select_type table type key key_len ref rows Extra 1 SIMPLE employees ALL       300584 Using temporary;Using filesort Extra 컬럼에 ‘Using temporary’가 표시되지는 않지만, 내부적으로 임시 테이블을 사용할 때도 많다. FROM 절에 사용된 서브쿼리는 무조건 임시 테이블(파생 테이블)을 생성한다. COUNT(DISTINCT col1)을 포함하는 쿼리는 인덱스를 사용할 수 없는 경우 임시 테이블을 생성한다. UNION, UNION ALL이 사용된 쿼리도 임시 테이블을 사용하여 결과를 병합한다. 정렬에 버퍼가 사용되는 경우, 버퍼의 실체도 결국은 임시 테이블이다. Using where MySQL 엔진 레이어에서 별도의 가공을 해서 필터링 작업을 처리한 경우이다. 스토리지 엔진은 디스크나 메모리상에서 필요한 레코드를 읽거나 저장한다. MySQL 엔진은 스토리지 엔진으로부터 받은 레코드를 가공 또는 연산한다. 작업 범위 제한 조건은 스토리지 엔진 레벨에서, 체크 조건은 MySQL 엔진 레벨에서 처리된다. 아래 쿼리의 경우, 스토리지 엔진에서 100개의 레코드를 MySQL 엔진으로 넘겨준다. MySQL 엔진은 체크 조건(gender='F')을 통해 레코드를 필터링한다. ‘Using where’는 필터링돼서 레코드를 버리는 처리를 의미한다. EXPLAINSELECT * FROM employeesWHERE emp_no BETWEEN 10001 AND 10100 AND gender='F'; id select_type table type key key_len ref rows Extra 1 SIMPLE employees range PRIMARY 4 NULL 100 Using where ‘Using where’가 성능상의 문제를 일으킬지 아닐지는 5.1 버전부터 추가된 ‘Filtered’ 컬럼을 통해 확인할 수 있다.3. 쿼리의 실행 계획이 좋은 경우Distinct 아래 쿼리는 departments 테이블과 dept_emp 테이블에 모두 존재하는 dept_no만 유니크하게 가져오기 위한 쿼리이다. DISTINCT를 처리하기 위해 조인하지 않아도 되는 항목은 모두 무시하고 필요한 것만 조인한다(필요한 레코드만 읽는다).EXPLAINSELECT DISTINCT d.dept_noFROM departments d, dept_emp de WHERE de.dept_no=d.dept_no; id select_type table type key key_len ref rows Extra 1 SIMPLE d index ux_deptname 123 NULL 9 Using index;Using temporary 1 SIMPLE de ref PRIMARY 12 d.dept_no 18603 Using index;Distinct Using index(커버링 인덱스) 인덱스만 읽어서 쿼리를 모두 처리할 수 있는 경우이다. 인덱스를 이용해 처리하는 쿼리에서 가장 큰 부하를 차지하는 부분은 인덱스를 검색해 일치하는 레코드의 나머지 컬럼 값을 가져오기 위해 데이터 파일을 찾아서 가져오는 작업이다. 최악의 경우에는 인덱스를 통해 검색된 결과 레코드 한 건 한 건마다 디스크를 한 번씩 읽어야 할 수도 있다. InnoDB의 모든 테이블은 클러스터링 인덱스로 구성돼 있다. 즉, InnoDB 테이블의 모든 보조 인덱스는 데이터 레코드의 주소 값으로 프라이머리 키 값을 가진다. 이러한 특성 때문에 쿼리가 커버링 인덱스로 처리될 가능성이 상당히 높다. 하지만 무조건 커버링 인덱스로 처리하려고 인덱스에 많은 컬럼을 추가하게 되면, 과도하게 인덱스의 컬럼이 많아져 메모리 낭비가 심해질 수 있다. 또한, 레코드를 저장하거나 변경하는 작업이 매우 느려질 수 있다. Using index for group-by GROUP BY 처리가 인덱스를 이용하는 경우이다. 이러한 방법을 ‘루스 인덱스 스캔’이라고 한다. 루스 인덱스 스캔은 인덱스에서 필요한 부분만 읽는다. salaries 테이블의 인덱스는 emp_no, from_date로 구성되어 있다. 아래의 쿼리의 경우 emp_no 그룹별로 첫 번째 from_date 값(최솟값)과 마지막 from_date 값(최댓값)을 인덱스로부터 읽으면 된다. 따라서, ‘루스 인덱스 스캔’ 방식으로 처리할 수 있다. EXPLAINSELECT emp_no, MIN(from_date) AS first_changed_date, MAX(from_date) AS last_changed_dateFROM salariesGROUP BY emp_no; id select_type table type key key_len ref rows Extra 1 SIMPLE salaries range PRIMARY 4   711129 Using index forgroup-by EXPLAIN EXTENDED MySQL 5.1.12 미만의 버전에서는 MySQL 엔진에 의해 필터링 과정을 거치면서 얼마나 많은 레코드가 버려졌는지 알 수 없었다.5.1.12 버전부터는 필터링이 얼마나 효율적으로 실행됐는지 알려주기 위해 ‘Filtered’ 컬럼이 추가되었다.이를 확인하려면 EXPLAIN EXTENDED 명령어를 사용한다. 필터링된 레코드는 제외하고 최종적으로 레코드가 얼마나 남았는지 비율이 표시된다. 즉, 전체 레코드 100건 중 20% (20건)만이 남았다는 의미다. 이 값은 실제 값이 아닌 통계 정보로부터 예측된 값이다.EXPLAIN EXTENDEDSELECT * FROM employeesWHERE emp_no BETWEEN 10001 AND 10100 AND gender='F'; id select_type table type key key_len ref rows filtered Extra 1 SIMPLE employees range PRIMARY 4 NULL 100 20 Using where EXPLAIN PARTITIONS EXPLAIN PARTITIONS 명령을 통해 파티션 테이블의 실행 계획 정보를 좀 더 자세히 확인할 수 있다. 파티션이 여러 개인 테이블에서 불필요한 파티션을 빼고 쿼리를 수행하기 위해 접근해야 할 것으로 판단되는 테이블만 골라내는 과정을 ‘파티션 프루닝(Partition pruning)’이라고 한다. 이를 확인하기 위해 옵티마이저가 실제로 접근하는 파티션 테이블을 확인해 볼 수 있다. id select_type table partitions type key key_len ref rows Extra 1 SIMPLE tb_partition p3 ALL       2 Using where 관련 글 MySQL 실행 계획 (1) MySQL 실행 계획 (2)참고 자료 이성욱, 『개발자와 DBA를 위한 Real MySQL』, 위키북스(2012), 6장" }, { "title": "MySQL - MySQL 실행 계획 (2)", "url": "/posts/execution-plan-part2/", "categories": "지식 더하기, 이론", "tags": "MySQL", "date": "2021-10-07 23:00:00 +0900", "snippet": "들어가기 전실행 계획을 나타내는 테이블의 다양한 컬럼들 중 먼저 id, select_type, table, type 컬럼에 대해 살펴볼 것이다. ※ 이전 글을 읽지 않으신 분들은 먼저 읽으실 것을 권장합니다.id 컬럼 단위 SELECT 쿼리별로 부여되는 식별자 값을 나타내는 컬럼이다.SELECT...FROM (SELECT ... FROM tb_tes...", "content": "들어가기 전실행 계획을 나타내는 테이블의 다양한 컬럼들 중 먼저 id, select_type, table, type 컬럼에 대해 살펴볼 것이다. ※ 이전 글을 읽지 않으신 분들은 먼저 읽으실 것을 권장합니다.id 컬럼 단위 SELECT 쿼리별로 부여되는 식별자 값을 나타내는 컬럼이다.SELECT...FROM (SELECT ... FROM tb_test1) tb1, tb_test2 tb2WHERE tb1.id=tb2.id; 위 쿼리는 아래와 같이 SELECT 단위로 분리해서 생각해볼 수 있다.SELECT ... FROM tb_test1;SELECT ... FROM tb1, tb_test2 tb2 WHERE tb1.id=tb2.id; 하나의 SELECT 문장 안에서 여러 개의 테이블을 조인하면 각 레코드별로 같은 id가 부여된다.EXPLAINSELECT e.emp_no, e.first_name, s.from_date, s.salaryFROM employees e, salaries sWHERE e.emp_no=s.emp_noLIMIT 10; id select_type table type key key_len ref rows Extra 1 SIMPLE e index ix_firstname 44   300584 Using index 1 SIMPLE s ref PRIMARY 4 employees, e.emp_no 4   다음과 같이 3개의 단위 SELECT 쿼리로 구성된 경우는 아래와 같은 실행 계획을 나타낼 것이다.EXPLAINSELECT( (SELECT COUNT(*) FROM employees) +(SELECT COUNT(*) FROM departments) ) AS total_count; id select_type table type key key_len ref rows Extra 1 PRIMARY             No tables used 2 SUBQUERY employees index ix_hiredate 3   300584 Using index 3 SUBQUERY departments index ux_deptname 123   9 Using index select_type 컬럼 각 단위 SELECT 쿼리가 어떤 타입의 쿼리인지 표시되는 컬럼이다.SIMPLE UNION이나 서브 쿼리를 사용하지 않는 단순한 SELECT 쿼리인 경우이다. 실행 계획에서 select_type이 ‘SIMPLE’인 단위 쿼리는 반드시 하나만 존재한다. 일반적으로 제일 바깥 SELECT 쿼리의 select_type이 ‘SIMPLE’로 표시된다.PRIMARY UNION이나 서브 쿼리가 포함된 SELECT 쿼리의 실행 계획에서 가장 바깥쪽(Outer)에 있는 단위 쿼리인 경우이다. 실행 계획에서 select_type이 ‘PRIMARY’인 단위 쿼리는 반드시 하나만 존재한다.UNION UNION으로 결합하는 단위 SELECT 쿼리 가운데 첫 번째를 제외한 두 번째 이후 단위 SELECT 쿼리는 UNION으로 표시된다. UNION의 첫 번째 단위 SELECT는 UNION 쿼리로 결합된 전체 집합의 select_type이 표시된다.EXPLAINSELECT * FROM ( (SELECT emp_no FROM employees e1 LIMIT 10) UNION ALL (SELECT emp_no FROM employees e2 LIMIT 10) UNION ALL (SELECT emp_no FROM employees e3 LIMIT 10)) tb; id select_type table type key key_len ref rows Extra 1 PRIMARY &lt; derived2 &gt; ALL       30   2 DERIVED e1 index ix_hiredate 3   300584 Using index 3 UNION e2 index ix_hiredate 3   300584 Using index 4 UNION e3 index ix_hiredate 3   300584 Using index   UNION RESULT &lt;union2,3,4&gt; ALL           DEPENDENT UNION UNION이나 UNION ALL로 집합을 결합하는 쿼리에서 표시된다. ‘DEPENDENT’는 UNION이나 UNION ALL로 결합된 단위 쿼리가 외부의 영향을 받은 것을 의미한다. 외부의 영향이란, 내부 쿼리가 외부의 값을 참조해서 처리하는 것을 의미한다. 일반적으로 외부 쿼리보다 서브 쿼리가 먼저 실행되며, 대부분 이러한 방식이 반대의 경우보다 더 빠르다. 하지만 ‘DEPENDENT’ 키워드가 포함되는 경우, 서브 쿼리는 외부 쿼리에 의존적이므로 절대 외부 쿼리보다 먼저 실행될 수가 없다. 따라서 이러한 쿼리는 비효율적인 경우가 많다. 아래 쿼리에서는 내부 쿼리에서 외부에 있는 employees 테이블의 emp_no 컬럼이 사용된다.EXPLAINSELECT e.first_name, ( SELECT ... FROM salaries s WHERE s.emp_no = e.emp_no UNION SELECT ... FROM dept_emp de WHERE de.emp_no = e.emp_no ) AS msgFROM employees eWHERE e.emp_no=10001; id select_type table type key key_len ref rows Extra 1 PRIMARY e const PRIMARY 4 const 1   2 DEPENDENTSUBQUERY s ref PRIMARY 4 const 17 Using index 3 DEPENDENTUNION de ref ix_empno_fromdate 4   1 Using where;Using index   UNION RESULT &lt;union2,3&gt; ALL           UNION RESULT MySQL에서 UNION이나 UNION ALL 쿼리는 모두 병합 결과를 임시 테이블로 생성한다. ‘UNION RESULT’는 이러한 임시 테이블을 의미한다. 단위 쿼리가 아니기 때문에 별도 id 값은 부여되지 않는다. table 컬럼의 &lt;union n,m&gt;의 의미는 id가 n번, m번인 단위 쿼리의 결과를 UNION 했다는 것을 의미한다.SUBQUERY 여기서 ‘SUBQUERY’라고 하는 것은 FROM 절 이외에서 사용되는 서브 쿼리만을 의미한다. FROM 절에 사용된 서브 쿼리는 ‘DERIVED’라고 표시된다.EXPLAINSELECT e.first_name, ( SELECT ... FROM dept_emp de, dept_manager dm WHERE ...) AS cntFROM employees eWHERE e.emp_no = 10001; id select_type table type key key_len ref rows Extra 1 PRIMARY e const PRIMARY 4 const 1   2 SUBQUERY dm index PRIMARY 16   24 Using index 2 SUBQUERY de ref PRIMARY 12 dm.dept_no 18603 Using index DEPENDENT SUBQUERY 서브 쿼리가 바깥쪽(Outer) SELECT 쿼리에서 정의된 컬럼을 사용하는 경우이다. ‘DEPENDENT UNION’처럼 ‘DEPENDENT SUBQUERY’ 또한 외부 쿼리가 먼저 수행된 후 내부 쿼리(서브 쿼리)가 실행되어야 하므로 일반 서브 쿼리보다는 처리 속도가 느릴 때가 많다.EXPLAINSELECT e.first_name, ( SELECT ... FROM dept_emp de, dept_manager dm WHERE ... AND de.emp_no=e.emp_no) AS cntFROM employees eWHERE e.emp_no = 10001; id select_type table type key key_len ref rows Extra 1 PRIMARY e const PRIMARY 4 const 1   2 DEPENDENTSUBQUERY de ref ix_empno_fromdate 4   1 Usingindex 2 DEPENDENTSUBQUERY dm ref PRIMARY 12 dm.dept_no 1 Usingindex DERIVED 서브 쿼리가 FROM 절에 사용된 경우이다. ‘DERIVED’인 경우, 쿼리의 실행 결과를 메모리나 디스크에 임시 테이블로 생성한다. 이러한 임시 테이블을 ‘파생 테이블’이라고도 한다. 파생 테이블에는 인덱스가 없으므로 다른 테이블과 조인할 때 성능상 불리할 때가 많다. MySQL은 이러한 서브 쿼리를 최적화하지 못할 때가 대부분이다. (MySQL 5 기준) 쿼리를 튜닝하기 위해 가장 먼저하는 것 중 하나가 select_type 값이 ‘DERIVED’인 것이 있는지 찾는 것이다. 이 경우, 조인으로 해결할 수 있는 경우라면 서브 쿼리보다는 조인을 사용하는 것이 권장된다. EXPLAINSELECT *FROM (SELECT de.emp_no FROM dept_emp de) tb, employees eWHERE e.emp_no=tb.emp_no; id select_type table type key key_len ref rows Extra 1 PRIMARY &lt; derived2 &gt; ALL       331603   1 PRIMARY e eq_ref PRIMARY 4 tb.emp_no 1 Usingindex 2 DERIVED de index ix_fromdate 3   334868 Usingindex UNCACHEABLE SUBQUERY 일반적으로 조건이 똑같은 서브 쿼리가 실행될 때, 이전의 실행 결과를 그대로 사용한다. 재사용을 위해 서브 쿼리의 결과를 내부적인 캐시 공간에 담아둔다. 쿼리 캐시나 파생 테이블과는 무관하다. 하지만 ‘UNCACHEABLE SUBQUERY’로 표시될 때는 캐싱된 결과를 사용할 수 없는 경우이다. 사용자 변수가 서브 쿼리에 사용된 경우 NOT-DETERMINISTIC 속성의 스토어드 루틴이 서브 쿼리에 내에 사용된 경우 UUID()나 RAND()와 같이 결과값이 호출할 때마다 달라지는 함수가 서브 쿼리에 사용된 경우 UNCACHEBLE UNION UNION을 사용한 쿼리 중 위에서 언급한 캐싱할 수 없는 조건에 해당하는 경우이다. MySQL 5.1부터 추가된 select_type이다.table 컬럼 MySQL의 실행 계획은 테이블 기준으로 표시된다. 별도의 테이블을 사용하지 않는 경우에는 NULL이 표시된다. table 컬럼에 “&lt;&gt;”로 둘러싸인 이름이 표시되는 경우는 임시 테이블을 의미한다. 지금까지 공부한 내용을 토대로 다음 실행 계획을 분석해보자. id select_type table type key key_len ref rows Extra 1 PRIMARY &lt;derived2&gt; ALL       10420   1 PRIMARY e eq_ref PRIMARY 4 de1.emp_no 1   2 DERIVED dept_emp range ix_fromdate 3   20550   첫 번째 라인의 테이블이 &lt;derived2&gt;이므로 id가 2번인 라인이 먼저 실행되고 그 결과로 파생 테이블이 만들어진다. id 2번에 table이 dept_emp인 것으로 보아, dept_emp 테이블을 읽어 파생 테이블을 생성한다. id가 1번으로 같은 두 개의 테이블 &lt;derived2&gt;, e는 조인됐다는 것을 알 수 있다. &lt;derived2&gt;가 e보다 먼저 표시됐기 때문에, &lt;derived2&gt;가 드라이빙 테이블, e가 드리븐 테이블이 된다. 즉, &lt;derived2&gt; 테이블을 먼저 읽고 이 결과를 기준으로 e와 조인을 한다. type 컬럼 type 컬럼과 그 이후의 컬럼들은 MySQL 서버가 각 테이블의 레코드를 어떤 방식으로 읽었는지를 나타낸다. 즉, 인덱스를 사용해 읽었는지, 풀 테이블 스캔으로 읽었는지 등을 의미한다.일반적으로 쿼리를 튜닝할 때 인덱스를 효율적으로 사용하는지 확인하는 것이 중요하기 때문에, type 컬럼은 반드시 체크해야 할 중요한 정보이다.이제 type 컬럼에서 나타날 수 있는 값들에 대해 살펴보자.system 레코드가 1건 이하인 테이블을 참조하는 형태의 접근 방법이다. MyISAM이나 MEMORY 테이블에서만 사용되는 접근 방법이다.EXPLAINSELECT * FROM tb_dual; id select_type table type key key_len ref rows Extra 1 SIMPLE tb_dual system       1   const 쿼리가 프라이머리 키나 유니크 키 컬럼을 이용하는 WHERE 조건절을 가지고 있으며, 반드시 1건을 반환하는 쿼리의 처리 방식이다. 쿼리의 해당 값은 옵티마이저에 의해 상수(const)화된 다음 쿼리 실행기로 전달된다. 다른 DBMS에서는 ‘UNIQUE INDEX SCAN’이라고도 표현한다.EXPLAINSELECT * FROM employees WHERE emp_no=10001; id select_type table type key key_len ref rows Extra 1 SIMPLE employees const PRIMARY 4 const 1   다중 컬럼으로 구성된 프라이머리 키, 유니크 키 중에서 인덱스의 일부 컬럼만 조건으로 사용할 때는 const 타입의 접근 방법을 사용할 수 없다.eq_ref 조인에서 처음 읽은 테이블의 컬럼 값을 그다음 읽어야 할 테이블의 프라이머리 키나 유니크 키 컬럼의 검색 조건에 사용하는 경우이다. 다중 컬럼으로 만들어진 프라이머리 키나 유니크 인덱스라면 인덱스의 모든 컬럼이 비교 조건에 사용돼야만 eq_ref 접근 방법이 사용될 수 있다. 즉, 조인에서 두 번째 이후에 읽는 테이블에서 반드시 1건만 존재한다는 보장이 있어야한다. EXPLAINSELECT * FROM dept_emp de, employees eWHERE e.emp_no=de.emp_no AND de.dept_no='d005'; id select_type table type key key_len ref rows Extra 1 SIMPLE de ref PRIMARY 12 const 53288 Usingwhere 1 SIMPLE e eq_ref PRIMARY 4 employees.de.emp_no 1   ref 인덱스의 종류와 관계없이 동등 조건으로 검색할 때 ref 접근 방법이 사용된다. eq_ref와는 달리 조인의 순서와 관계없이 사용된다. 프라이머리 키나 유니크 키 등의 제약 조건도 없다. 반환되는 레코드가 반드시 1건이라는 보장이 없으므로 const나 eq_ref보다는 빠르지 않다. const, eq_ref, ref 모두 인덱스의 분포도가 나쁘지 않다면 성능 문제를 일으키지 않는 좋은 접근 방법이다.EXPLAINSELECT * FROM dept_emp WHERE dept_no='d005'; id select_type table type key key_len ref rows Extra 1 SIMPLE dept_emp ref PRIMARY 12 const 53288 Usingwhere fulltext MySQL의 전문 검색(Fulltext) 인덱스를 사용해 레코드를 읽는 접근 방법을 의미한다. 전문 검색 인덱스는 통계 정보가 관리되지 않는다. 따라서, 옵티마이저는 전문 인덱스를 사용할 수 있는 쿼리에서는 비용과는 관계 없이 거의 fulltext 접근 방법을 사용한다. 물론, 성능상 더 빠른 const, eq_ref, ref 접근 방법을 사용할 수 있는 경우에는 굳이 fulltext를 사용하지 않는다. 전문 검색은 MATCH .. AGAINST ... 구문을 사용해서 실행하며, 반드시 해당 테이블에 전문 검색용 인덱스가 준비돼 있어야 한다.EXPLAINSELECT *FROM employee_name WHERE emp_no=10001 AND emp_no BETWEEN 10001 AND 10005 AND MATCH(first_name, last_name) AGAINST('Facello' IN BOOLEAN MODE);ref_or_null ref 접근 방식과 같은데, NULL 비교가 추가된 형태다.EXPLAINSELECT * FROM titlesWHERE to_date='1985-03-01' OR to_date IS NULL; id select_type table type key key_len ref rows Extra 1 SIMPLE titles ref_or_null ix_todate 4 const 2 Using where;Using index unique_subquery WHERE 조건절에서 사용될 수 있는 IN (subquery) 형태의 쿼리를 위한 접근 방식이다. 서브 쿼리에서 중복되지 않은 유니크한 값만 반환할 때 사용한다. 아래 쿼리의 경우, dept_emp 테이블의 프라이머리 키가 (dept_no, emp_no)이므로 emp_no=10001인 레코드 중에서 dept_no는 중복이 없다.EXPLAINSELECT * FROM departments WHERE dept_no IN ( SELECT dept_no FROM dept_emp WHERE emp_no=10001); id select_type table type key key_len ref rows Extra 1 PRIMARY departments index PRIMARY 123   9 Using index;Using where 2 DEPENDENTSUBQUERY dept_emp unique_subquery PRIMARY 16 func,const 1 Using index;Using where index_subquery IN (subquery)에서 서브 쿼리가 중복된 값을 반환할 수는 있지만 중복된 값을 인덱스를 이용해 제거할 수 있을 때 이 접근 방법이 사용된다.EXPLAINSELECT * FROM departments WHERE dept_no IN ( SELECT dept_no FROM dept_emp WHERE dept_no BETWEEN 'd001' AND 'd003'); id select_type table type key key_len ref rows Extra 1 PRIMARY departments index ux_deptname 122   9 Using where;Using index 2 DEPENDENTSUBQUERY dept_emp index_subquery PRIMARY 12 func 18626 Using index;Using where range 인덱스 레인지 스캔 형태의 접근 방법이다. 인덱스를 하나의 값이 아닌 범위로 검색하는 경우이다. &lt;, &gt;, IS NULL, BETWEEN, IN, LIKE 등의 연산자를 이용해 검색하는 경우 일반적으로, const, ref, range 세 가지 접근 방법을 모두 ‘인덱스 레인지 스캔’ 방식이라고 한다.EXPLAINSELECT dept_no FROM dept_emp WHERE dept_no BETWEEN 'd001' AND 'd003'; id select_type table type key key_len ref rows Extra 1 SIMPLE dept_emp range PRIMARY 12   121890 Using where;Using index index_merge 유일하게 2개 이상의 인덱스를 이용하는 접근 방식이다. 각각의 인덱스를 이용해 검색 결과를 만들어낸 후 그 결과를 병합한다. ‘index_merge’ 접근 방식에는 다음과 같은 특징이 있다. 여러 인덱스를 읽어야 하므로 일반적으로 range 접근 방식보다는 효율성이 떨어진다. AND, OR 연산이 복잡하게 연결된 쿼리에서는 최적화되지 못할 때가 많다. 전문 검색 인덱스를 사용하는 쿼리에서는 적용되지 않는다. 병합된 처리 결과는 항상 2개 이상의 집합이 되기 때문에 교집합, 합집합 또는 중복 제거와 같은 부가적인 작업이 더 필요하다. EXPLAINSELECT * FROM employeesWHERE emp_no BETWEEN 10001 AND 11000 OR first_name='Smith'; id select_type table type key key_len ref rows Extra 1 SIMPLE employees index_merge PRIMARY, ix_firstname 4,44   1521 Using union(PRIMARY ,ix_firstname); Using where index 인덱스를 처음부터 끝까지 읽는 ‘인덱스 풀 스캔’ 방식을 의미한다. 많은 사람들이 ‘index’라는 이름 때문에 효율적이라고 오해하지만, range 접근 방식과 같이 인덱스의 필요한 부분만 읽는 것이 아니기 때문에 비효율적일 수 있다. 테이블을 처음부터 끝까지 읽는 ‘풀 테이블 스캔’ 방식과 비교하는 데이터 건수는 같다. 하지만, 인덱스는 일반적으로 데이터 파일 전체 크기보다는 작아서 풀 테이블 스캔 보다는 빠르다. 다음 조건 중, (1,2) 또는 (1,3)인 경우 index 접근 방법이 사용된다. range나 const 또는 ref와 같은 접근 방식으로 인덱스를 사용하지 못하는 경우 인덱스에 포함된 컬럼만으로 처리할 수 있는 쿼리인 경우(데이터 파일 읽지 않아도 되는 경우) 인덱스를 이용해 정렬이나 그룹핑 작업이 가능한 경우(별도의 정렬 작업 필요 없는 경우) EXPLAINSELECT * FROM departmentsORDER BY dept_name DESC LIMIT 10; id select_type table type key key_len ref rows Extra 1 SIMPLE departments index ux_firstname 123   9 Using index ALL 테이블을 처음부터 끝까지 읽는 ‘풀 테이블 스캔’ 방식이다. 위에서 살펴본 모든 방법을 사용할 수 없는 경우 마지막으로 선택되는 가장 비효율적인 방법이다. 일반적으로 DBMS에는 이러한 풀 스캔 방식으로 인한 대량의 디스크 I/O를 유발하는 작업을 위해 Read Ahaed라는 기능을 제공한다. Read Ahead : 한 번에 여러 페이지를 읽어서 처리하는 기능 쿼리를 튜닝한다는 것이 무조건 인덱스 풀 스캔이나, 테이블 풀 스캔을 사용하지 못하게 하는 것은 아니다.다음 파트에서는 실행 계획의 나머지 컬럼들에 대해 살펴볼 것이다.관련 글 MySQL 실행 계획 (1) MySQL 실행 계획 (3)참고 자료 이성욱, 『개발자와 DBA를 위한 Real MySQL』, 위키북스(2012), 6장" }, { "title": "MySQL - MySQL 실행 계획 (1)", "url": "/posts/execution-plan-part1/", "categories": "지식 더하기, 이론", "tags": "MySQL", "date": "2021-10-07 23:00:00 +0900", "snippet": "들어가기 전슬로우 쿼리를 고쳐보고 싶은데 어떤 부분에서 비효율이 발생하는지는 “실행 계획”을 보면 알 수 있다고 들었다. 실행 계획이 무엇이고 어떻게 활용해야 하는 것인지 공부해보자.실행 계획 DBMS의 쿼리 실행은 같은 결과를 만들어 내는 다양한 방법이 있다. 그 중 어떤 방법이 최적이고 최소의 비용이 소모될지 결정해야 한다.이러한 최적의 실행 계...", "content": "들어가기 전슬로우 쿼리를 고쳐보고 싶은데 어떤 부분에서 비효율이 발생하는지는 “실행 계획”을 보면 알 수 있다고 들었다. 실행 계획이 무엇이고 어떻게 활용해야 하는 것인지 공부해보자.실행 계획 DBMS의 쿼리 실행은 같은 결과를 만들어 내는 다양한 방법이 있다. 그 중 어떤 방법이 최적이고 최소의 비용이 소모될지 결정해야 한다.이러한 최적의 실행 계획을 수립하는 역할은 옵티마이저가 담당한다. 실행 계획을 이해할 수 있어야 불합리한 부분을 찾아내고, 최적화된 방법으로 실행 계획을 수립하도록 유도할 수 있다. MySQL 서버에서 쿼리가 실행되는 과정은 크게 다음과 같이 구분할 수 있다. 파싱(SQL 파서) 쿼리 문장을 MySQL 서버가 이해할 수 있는 수준으로 분리(파스 트리 생성) 최적화 및 실행 계획 수립(옵티마이저) 파스 트리를 기반으로 어떤 테이블부터 읽고 어떤 인덱스를 이용해서 테이블을 읽을지 선택 2번 단계가 완료되면 실행 계획이 만들어진다. 수립된 실행 계획대로 스토리지 엔진에 레코드를 읽어오도록 요청. MySQL 엔진에서는 스토리지 엔진으로부터 받은 레코드를 조인하거나 정렬하는 작업 수행옵티마이저의 최적화 방법 현재 많은 DBMS가 선택하고 있는 비용 기반 최적화(Cost-based optimizer, CBO) 방법과 예전 오라클에서 많이 사용했던 규칙 기반 최적화(Rule-based optimizer, RBO)로 나눌 수 있다. 규칙 기반 최적화 오직 옵티마이저에 내장된 우선순위에 따라 실행 계획을 수립하는 방식이다. 이 방식에서는 통계 정보(대상 테이블의 레코드 건수, 컬럼 값의 분포도 등)를 고려하지 않고 실행 계획이 수립된다. 각 테이블이나 인덱스의 통계 정보가 거의 없고, 상대적으로 느린 CPU 연산 탓에 비용 계산 과정이 부담스러웠기 때문에 사용하던 방식이다. 이미 오래 전부터 많은 DBMS에서 더 이상 지원하지 않는다. 비용 기반 최적화 산출된 각 실행 계획별 최소 비용이 소요되는 처리 방식을 선택한다. 쿼리를 처리하기 위한 여러 방법을 수립한다. 각 단위 작업의 비용(부하) 정보와 대상 테이블의 예측된 통계 정보를 이용해 각 실행 계획별 비용을 산출한다. 통계 정보 비용 기반 최적화 방식에서 가장 중요한 것은 ‘통계 정보’이다. 통계 정보가 정확하지 않으면 쿼리가 매우 비효율적으로 실행될 수 있다. 기본적으로 MySQL에서 관리되는 통계 정보는 대략의 레코드 건수와 인덱스의 유니크한 값의 개수정도이다. 레코드 건수가 많지 않으면 정보가 부정확한 경우가 많으므로 ANALYZE 명령을 이용해 강제적으로 통계 정보를 갱신해야 할 때도 있다. 오라클과 같은 DBMS에서는 통계 정보가 상당히 정적이고 수집에 많은 시간이 소요되기 때문에 통계 정보만 따로 백업하기도 한다.실행 계획 분석 MySQL에서는 기본적으로 EXPLAIN 명령을 사용해서 실행 계획을 확인할 수 있다. 또한, EXPLAIN EXTENDED, EXPLAIN PARTITIONS 명령을 이용해 더 상세한 실행 계획을 확인할 수도 있다. EXPLAIN을 실행하면 다음과 같이 표 형태로된 결과가 표시된다. 표의 각 라인은 쿼리 문장에서 사용된 테이블(서브 쿼리를 통해 생성된 임시 테이블도 포함)의 개수만큼 출력된다. 표의 위쪽에 있을수록 먼저 접근한 테이블이다. 다른 DBMS와는 달리 MySQL에서는 필요에 따라 실행 계획을 산출하기 위해 쿼리의 일부분을 직접 실행할 때도 있다. 따라서 쿼리 자체가 상당히 복잡하고 무거운 경우에는 실행 계획의 조회 또한 느려질 가능성이 있다. SELECT 이외의 DML(UPDATE, INSERT, DELETE)에 대해서는 실행계획을 확인할 방법이 없다. 이를 위해서는 WHERE 조건절만 같은 SELECT 쿼리를 만들어서 대략적으로 계획을 확인해 볼 수 있다. 다음 파트에서부터 실행 계획의 각 컬럼이 어떤 것을 의미하는지, 어떤 값이 나올 수 있는지 등에 대해 살펴볼 것이다.관련 글 MySQL 실행 계획 (2) MySQL 실행 계획 (3)참고 자료 이성욱, 『개발자와 DBA를 위한 Real MySQL』, 위키북스(2012), 6장" }, { "title": "MySQL - MySQL 내부 구조 간단히 살펴보기", "url": "/posts/mysql-architecture/", "categories": "지식 더하기, 이론", "tags": "MySQL", "date": "2021-10-06 23:00:00 +0900", "snippet": "들어가기 전슬로우 쿼리를 고쳐보고 싶은데 고치려면 어떻게 해야할지에 대한 지식이 전혀없다. 근데 슬로우 쿼리는 차치하고, 쿼리가 어떻게 동작하고 DB 내부는 어떻게 생긴지에 대해서도 공부해보지 않았다.그래서 먼저 MySQL을 기준으로 내부 구조가 어떤지 세부적인 내용을 제외하고 간단하게 정리해보려고 한다.MySQL의 전체적인 구조 MySQL 고유의 ...", "content": "들어가기 전슬로우 쿼리를 고쳐보고 싶은데 고치려면 어떻게 해야할지에 대한 지식이 전혀없다. 근데 슬로우 쿼리는 차치하고, 쿼리가 어떻게 동작하고 DB 내부는 어떻게 생긴지에 대해서도 공부해보지 않았다.그래서 먼저 MySQL을 기준으로 내부 구조가 어떤지 세부적인 내용을 제외하고 간단하게 정리해보려고 한다.MySQL의 전체적인 구조 MySQL 고유의 C API부터 JDBC, ODBC, .NET의 표준 드라이버 등을 제공한다. 이러한 드라이버는 다양한 언어(C/C++, 자바 등)가 MySQL 서버에서 쿼리를 사용할 수 있게 지원한다.MySQL 서버는 크게 MySQL 엔진과 스토리지 엔진으로 구분할 수 있다. 출처 : https://dev.mysql.com/doc/refman/8.0/en/pluggable-storage-overview.html MySQL 엔진 요청된 SQL 문장을 분석, 최적화하는 등 DBMS의 두뇌 역할을 수행한다. 커넥션 핸들러 클라이언트로부터의 접속 및 쿼리 요청을 처리 SQL 파서 쿼리를 토큰(MySQL이 인식할 수 있는 최소 단위의 어휘, 기호)으로 분리해 트리 형태의 구조(파서 트리)로 만든다. 쿼리의 문법 오류를 이 과정에서 발견된다. 전처리기 파서 트리를 기반으로 쿼리 문장에 구조적인 문제가 있는지 확인한다. 토큰을 테이블 이름, 칼럼 등에 매핑해 해당 객체의 존재 여부와 접근 권한 등을 확인한다. 옵티마이저 쿼리 문장을 저렴한 비용으로 가장 빠르게 처리할 수 있도록 결정한다. 캐시, 버퍼 성능 향상을 위해 MyISAM의 키 캐시나 InnoDB의 버퍼 풀과 같은 보조 저장소 기능이 포함된다. 실행 엔진 쿼리에 기반하여 만들어진 실행 계획대로 각 핸들러(스토리지 엔진)에게 요청해서 받은 결과를 또 다른 핸들러 요청의 입력으로 연결하는 역할을 수행한다. 예를 들어, GROUP BY를 처리하는 과정을 살펴보자. 실행 엔진은 핸들러에게 임시 테이블을 만들라고 요청 실행 엔진은 WHERE 절에 일치하는 레코드를 읽어오라고 핸들러에게 요청 읽어온 레코드를 1번에서 준비한 임시 테이블로 저장하라고 핸들러에게 요청 데이터가 준비된 임시 테이블에서 필요한 방식으로 데이터를 읽어 오라고 핸들러에게 요청 실행 결과를 사용자나 다른 모듈로 넘김 스토리지 엔진 실제 데이터를 디스크 스토리지에 저장하거나, 디스크 스토리지로부터 데이터를 읽어온다. InnoDB, MyISAM 등 MySQL 엔진은 하나이지만 스토리지 엔진은 여러 개를 동시에 사용할 수 있다. 아래와 같이 TEST 테이블을 생성한다면, 해당 테이블에 CRUD 작업 수행시 InnoDB 스토리지 엔진이 처리하게 된다.CREATE TABLE TEST ~~ ENGINE = INNODB핸들러 API 핸들러 요청 MySQL 엔진의 쿼리 실행기에서 데이터를 쓰거나 읽어야 할 때, 각 스토리지 엔진에게 쓰기 또는 읽기를 요청하는 것. 핸들러 요청시 사용되는 API를 핸들러 API라고 한다. InnoDB 스토리지 엔진 또한 핸들러 API를 이용해 MySQL 엔진과 데이터를 주고받는다. SHOW GLOBAL STATUS LIKE 'Handler%' 핸들러 API를 통해 얼마나 많은 데이터(레코드) 작업이 있었는지 확인할 수 있다. 스레딩 구조 MySQL 서버는 스레드 기반으로 작동하며, 크게 Foreground 스레드와 Background 스레드로 구분된다.Foreground 스레드(클라이언트 스레드) 클라이언트가 요청하는 쿼리 문장을 처리하는 것이 주 역할이다. 최소한 MySQL 서버에 접속된 클라이언트의 수만큼 존재한다. 클라이언트가 작업을 마치고 커넥션을 종료하면, 해당 커넥션을 담당하던 스레드는 다시 스레드 캐시로 되돌아간다. 일정 개수의 스레드를 스레드 캐시에 유지하려면 thread_cache_size 파라미터를 사용한다. 데이터를 MySQL의 데이터 버퍼나 캐시로부터 가져온다. 버퍼나 캐시에 데이터가 없는 경우, 디스크나 인덱스 파일로부터 데이터를 읽어온다. MyISAM 테이블은 디스크 쓰기 작업까지 포그라운드 스레드가 처리한다. InnoDB 테이블은 데이터 버퍼나 캐시까지만 포그라운드 스레드가 처리하고, 디스크까지 쓰는 작업은 백그라운드 스레드가 처리한다. Background 스레드 특히 InnoDB는 인서트 버퍼를 병합하는 스레드, 로그를 디스크로 기록하는 스레드, 버퍼 풀의 데이터를 디스크에 기록하는 스레드 등을 통해 여러 가지 작업이 백그라운드로 처리된다. 쓰기 스레드 MySQL 5.0(윈도우 기준, 유닉스는 5.1)부터 innodb_write_io_threads 파라미터를 사용하여 쓰기 스레드 개수를 1개 이상 지정할 수 있다. 일반적인 내장 디스크를 사용할 때는 2~4개, DAS나 SAN과 같은 스토리지를 사용할 때는 4개 이상으로 충분히 설정해 해당 스토리지 장비가 충분히 활용될 수 있게 하는 것이 좋다. 메모리 할당 및 사용 구조 크게 글로벌 메모리 영역과 로컬 메모리 영역으로 구분된다. 각 영역은 여러 스레드가 공유해서 사용하는지 아닌지에 따라 구분된다.글로벌 메모리 영역 글로벌 영역의 모든 메모리 공간은 MySQL 서버가 시작되면서 운영체제로부터 할당된다. 일반적으로 하나의 메모리 공간만 할당되지만, 필요에 따라 2개 이상의 메모리 공간을 할당받을 수도 있다.클라이언트 스레드 수와는 무관하며, 생성된 글로벌 영역이 N개라 하더라도 모든 스레드에 의해 공유된다. 구성 요소 키 캐시(MyISAM) / 버퍼 풀(InnoDB) / 쿼리 캐시 / 바이너리 로그 버퍼 / 로그 버퍼 / 테이블 캐시 운영체제의 메모리 할당 방식은 매우 복잡하기 때문에 MySQL 서버가 사용하는 정확한 메모리 양을 측정하는 것은 쉽지 않다. 일단은 MySQL 파라미터로 설정해 둔 만큼 운영체제로부터 메모리를 할당받는다고 생각하자.로컬(세션) 메모리 영역 클라이언트 스레드가 쿼리를 처리하는 데 사용하는 메모리 영역이다. 각 클라이언트 스레드별로 독립적으로 할당되기 때문에 절대 공유되지 않는다. 구성 요소 커넥션 버퍼 / Result 버퍼 / Read 버퍼 / 조인 버퍼 / 랜덤 Read 버퍼 / Sort 버퍼 로컬 메모리 영역의 경우, 각 쿼리의 용도별로 필요할 때만 공간이 할당되고 필요하지 않은 경우에는 MySQL이 메모리 공간을 할당조차 하지 않을 수도 있다. 커넥션 버퍼, Result 버퍼는 커넥션이 열려 있는 동안 계속 할당된 상태로 남는다. Sort 버퍼, 조인 버퍼는 쿼리를 실행하는 순간에만 할당했다가 해제한다.참고 자료 이성욱, 『개발자와 DBA를 위한 Real MySQL』, 위키북스(2012), 3장" }, { "title": "Java - 자바 애플리케이션이 실행되는 과정 살펴보기", "url": "/posts/java-how-java-works/", "categories": "지식 더하기, 이론", "tags": "Java", "date": "2021-09-27 00:29:00 +0900", "snippet": "들어가기 전자바 애플리케이션이 실행되는 과정을 간단히 도식화 하면 아래처럼 표현할 수 있을 것이다. 이를 컴파일과 실행이라는 두 단계로 나눠서, 각 단계별로 어떤 과정이 일어나는지 살펴보자.※ 컴파일 파트는 HomoEfficio 님의 블로그를 바탕으로 정리하였음을 미리 밝힙니다.1. 컴파일 javac 컴파일러에 의해 자바 소스 코드 파일(.java)...", "content": "들어가기 전자바 애플리케이션이 실행되는 과정을 간단히 도식화 하면 아래처럼 표현할 수 있을 것이다. 이를 컴파일과 실행이라는 두 단계로 나눠서, 각 단계별로 어떤 과정이 일어나는지 살펴보자.※ 컴파일 파트는 HomoEfficio 님의 블로그를 바탕으로 정리하였음을 미리 밝힙니다.1. 컴파일 javac 컴파일러에 의해 자바 소스 코드 파일(.java) → JVM 바이트코드(.class)로 변환되는 과정 C와 같은 대부분의 다른 언어는 Intel 또는 HP 프로세서 관련 명령어와 같은 컴퓨터 관련 명령어로 컴파일된다. 하지만, Java 코드는 컴파일되면 플랫폼에 독립적인 표준 바이트코드 세트로 변환되며, 이 바이트코드는 JVM(Java Virtual Machine)에 의해 실행된다. Java에서는 각 명령어가 1바이트의 균일한 크기를 가지므로 이러한 명령어를 바이트코드라고 한다. JVM 스펙의 .class 파일 구조에 맞는 바이트코드를 만들어 낼 수 있다면 어떤 언어든 JVM에서 실행될 수 있다. 클로저(Clojure)나 스칼라, 코틀린 등이 JVM에서 실행될 수 있는 이유가 바로 여기에 있다. 컴파일 과정 컴파일을 크게 두 파트로 나누면 다음과 같다. 자바 언어 스펙에 따라 자바 코드 분석/검증 JVM 스펙의 class 파일 구조에 맞는 바이트코드를 생성 1. 자바 언어 스펙에 따라 분석/검증 어휘 분석 (Lexical Analysis) Lexical Analyzer(Lexer 또는 Tokenizer라고도 한다)가 소스 코드에서 문자 단위로 읽어서 어휘소(lexeme)를 식별하고 어휘소를 설명하는 토큰 스트림(Token Stream)을 생성한다. 구문 분석 (Syntax Analysis) Syntax Analyzer(구문 분석기, Parser 라고도 한다)가 어휘 분석 결과로 나온 토큰 스트림이 언어의 스펙으로 정해진 문법 형식에 맞는지 검사해서, 맞지 않으면 컴파일 에러를 내고, 맞으면 Parse Tree를 생성한다. 의미 분석 (Symantic Analysis) 타입 검사, 자동 타입 변환 등이 수행된다. 예를 들어 다음과 같은 코드(int a = \"Hello\";)는 구문 분석 단계에서는 에러가 나지 않지만, 의미 분석 단계에서는 타입 검사가 수행되면서 에러가 발생한다. 의미 분석 단계를 거치면서 Parse Tree에 타입 관련 정보 등이 추가된다. 2. JVM 스펙의 class 파일 구조에 맞는 바이트코드를 생성 중간 코드 생성 (Intermediate Code Generation) 의미 분석 단계를 통과한 파스 트리를 바탕으로 기계어로 변환하기 좋은 형태의 중간 언어로 된 중간 코드를 생성한다. 중간 단계를 하나 둬서 간접화를 통해 경우의 수를 낮추고 효율을 높일 수 있다. 자바의 바이트코드가 중간 코드에 해당한다고 볼 수 있다. 다음 그림에서 4개의 언어를 나타내는 네모를 각각 자바, 클로저(Clojure), 스칼라, 코틀린이라면, 녹색 네모는 바이트코드라고 할 수 있다. 출처 : https://www.slideshare.net/RamchandraRegmi/intermediate-code-generationramchandra-regmi/ 중간 코드 최적화 (Code Optimization) 중간 코드가 더 효율적인 기계어로 변환되도록 최적화하는 과정이다. 다음과 같이 매우 다양한 최적화 기법이 사용된다. 핍홀(Peephole) 최적화 지역 최적화 루프 최적화 전역 최적화 2. 실행 java [options] mainclass [args...] 로 자바 애플리케이션을 실행할 수 있다. java 명령어를 실행하게 되면 JRE(Java Runtime Environment)를 시작 (바이트코드 실행) 지정된 클래스를 로드 해당 클래스의 main() 메서드를 호출하여 애플리케이션 실행 위에서 살펴본 과정은 JVM이 담당하게 된다. JVM이 무엇이고 어떻게 해당 과정을 수행하는지 살펴보자. JVM(Java Virtual Machine)이란 ? 공식 문서에서는 JVM을 다음과 같이 표현한다. A JVM is a separate program that is optimized for the specific platform on which you run your Java code. → “JVM은 Java 코드를 실행하는 특정 플랫폼에 최적화된 별도의 프로그램이다.” 자바 애플리케이션을 개발할 때 자바 언어로 작성된 사전 정의된 core class library를 사용한다. core class library는 일반적으로 사용되는 기능을 제공하는 패키지이다. 예를 들어, 기본 언어 지원은 java.lang, I/O 지원은 java.io, 네트워크 접근은 java.net 패키지를 통해 제공된다. JVM과 core class library는 자바를 지원하는 모든 운영 체제에서 자바 애플리케이션을 개발할 수 있는 플랫폼을 제공한다. 이를 통해 자바의 핵심 사상인 “write once, run anywhere”(WORA)가 가능해진다. 자바를 다운받을 때, 운영체제 별로 나뉘어져 있는 것을 생각해보면 쉽게 와닿을 것 같다.JVM이 바이트코드를 실행하기까지 JVM의 구성요소를 살펴보면서 어떻게 실행되는지 알아보자. 출처 : https://www.geeksforgeeks.org/jvm-works-jvm-architecture/1. Class Loader : Loading, Linking, Initializing JVM은 클래스와 인터페이스를 동적으로 로드, 연결 및 초기화한다.Loading 특정 이름을 가진 클래스, 인터페이스 타입의 이진 표현을 바탕으로 클래스, 인터페이스를 만드는 과정이다. 클래스 로더는 .class 파일을 읽고 해당 파일에 대한 이진 데이터를 Method Area에 저장한다. JVM은 각 .class 파일에 대해 다음 정보를 Method Area에 저장한다. 로드된 클래스 및 해당 직계 부모 클래스의 정규화된(fully qualified) 이름 public class Demo { public static void main(String[] argv) throws Exception { Class c = java.util.ArrayList.class; String className = c.getName(); System.out.println(\"The fully-qualified name of the class is: \" + className); // The fully-qualified name of the class is: java.util.ArrayList }} .class 파일이 Class, Interface 또는 Enum과 관련이 있는지 여부 수정자(public, final, static, …), 변수, 메서드 정보 등 .class 파일을 로드한 후 JVM은 이 파일을 힙 메모리에 나타내기 위해 Class 타입의 객체를 생성한다. 이 객체는 java.lang 패키지에 미리 정의된 java.lang.Class&lt;T&gt;이다. Class 객체는 클래스 이름, 부모 이름, 메서드 및 변수 정보 등과 같은 클래스 레벨의 정보를 얻는 데 사용할 수 있다. 이 객체 참조를 얻으려면 Object.getClass() 메서드를 사용한다. 로드된 모든 .class 파일에 대해 하나의 클래스 객체만 생성된다.Linking 클래스, 인터페이스를 결합하여 JVM이 실행할 수 있는 상태로 만드는 과정이다. verification, preparation, (선택적으로) resolution을 수행한다. Verification .class 파일의 정확성을 보장한다. 즉, 올바른 형식으로 올바른 컴파일러에 의해 생성되었는지 여부를 확인한다. 검증에 실패하면 런타임 예외 java.lang.VerifyError가 발생한다. 이 과정은 ‘ByteCodeVerifier’ 컴포넌트에 의해 수행된다. 검증이 완료되면 class.파일을 (기계어로) 컴파일할 준비가 된 것이다. Preparation JVM은 클래스 변수에 대한 메모리를 할당하고 메모리를 기본값으로 초기화한다. Resolution 타입의 심볼릭 레퍼런스를 direct 레퍼런스로 바꾸는 과정이다. 참조된 엔터티를 찾기 위해 메서드 영역을 탐색한다. Initialization 클래스 또는 인터페이스의 초기화 메서드를 실행하는 것이다. 이 단계에서 모든 static 변수는 코드 및 static block(있는 경우)에 정의된 값으로 할당된다. 클래스에서는 위에서 아래로, 클래스 계층에서는 부모에서 자식 순서로 실행된다.참고. 클래스 로더 구성 일반적으로, 세 가지 클래스 로더로 구성된다. Bootstrap class loader JAVA_HOME/jre/lib 디렉토리에 있는 핵심 Java API 클래스를 로드한다. 이 경로는 일반적으로 부트스트랩 경로로 알려져있다. C, C++ 등의 native 언어로 구현되어 있다. 모든 JVM 구현에는 신뢰할 수 있는 클래스를 로드할 수 있는 부트스트랩 클래스 로더가 있어야한다. Extension class loader 부트스트랩 클래스 로더의 자식이다. JAVA_HOME/jre/lib/ext(확장 경로) 또는 java.ext.dirs 시스템 속성에 의해 지정된 다른 디렉토리에 있는 클래스를 로드한다. 자바에서는 sun.misc.Launcher$ExtClassLoader 클래스에 의해 구현된다. System/Application class loader 확장 클래스 로더의 자식입니다. 애플리케이션 classpath에서 클래스를 로드하는 역할을 수행한다. 내부적으로 java.class.path에 매핑된 환경 변수를 사용한다. 자바에서는 sun.misc.Launcher$AppClassLoader 클래스에 의해 구현된다. public class Test { public static void main(String[] args) { // String class is loaded by bootstrap loader, and // bootstrap loader is not Java object, hence null System.out.println(String.class.getClassLoader()); // null // Test class is loaded by Application loader System.out.println(Test.class.getClassLoader()); // jdk.internal.loader.ClassLoaders$AppClassLoader@8bcc55f }} JVM은 Delegation-Hierarchy 원칙에 따라 클래스를 로드한다. 시스템 클래스 로더는 확장 클래스 로더에 로드 요청을 위임하고, 확장 클래스 로더는 부트스트랩 클래스 로더에 요청을 위임한다. 부트스트랩 경로에 클래스가 있는 경우 해당 클래스가 로드되지 않으면 다시 요청을 확장 클래스 로더로 전송한 다음 시스템 클래스 로더로 전송한다. 마지막으로 시스템 클래스 로더가 클래스 로드에 실패하면 런타임 예외 java.lang.ClassNotFoundException이 발생한다. 출처 : https://www.geeksforgeeks.org/jvm-works-jvm-architecture/2. Runtime Data Arae(JVM Memory)해당 내용과 관련하여 정리했던 포스팅을 참조하면 될 것 같다.3. Execution Engine 바이트 코드를 한 줄씩 읽어들여 다양한 메모리 영역에 있는 데이터와 정보를 바탕으로 명령을 실행한다. 크게 Interpreter, JIT Compiler, Garbage Collector 세 부분으로 구성된다.Interpreter 바이트코드를 한 줄씩 해석하여 실행한다. 같은 메서드를 여러 번 호출하는 경우, 매번 해석이 필요하기 때문에 비효율적이다.JIT Compiler 인터프리터의 효율성을 높이기 위해 사용합니다. 전체 바이트 코드를 컴파일하여 네이티브 코드로 변경한다. 인터프리터가 반복되는 메서드를 호출할 때마다 해당 부분에 대해 JIT가 네이티브 코드를 제공한다. 결과적으로, 재해석이 필요하지 않으므로 효율성이 향상된다. Garbage Collector 더 이상 참조되지 않는 객체를 제거한다.4. Java Native Interface (JNI) Native Method Libraries와 연동하여 실행에 필요한 Native Library(C, C++)를 제공하는 인터페이스이다. JVM이 C/C++ 라이브러리를 호출할 수 있고, 하드웨어 전용 C/C++ 라이브러리에 의해 호출될 수도 있다.5. Native Method Libraries Execution Engine에서 필요로 하는 Native Libraries(C, C++)의 모음이다.더 공부할 부분 JIT Compiler Garbage Collector 바이트코드참고 자료 https://docs.oracle.com/cd/E11882_01/java.112/e10588/chone.htm#JJDEV13018 https://homoefficio.github.io/2019/01/31/Back-to-the-Essence-Java-%EC%BB%B4%ED%8C%8C%EC%9D%BC%EC%97%90%EC%84%9C-%EC%8B%A4%ED%96%89%EA%B9%8C%EC%A7%80-1/ https://homoefficio.github.io/2019/01/31/Back-to-the-Essence-Java-%EC%BB%B4%ED%8C%8C%EC%9D%BC%EC%97%90%EC%84%9C-%EC%8B%A4%ED%96%89%EA%B9%8C%EC%A7%80-2/ https://docs.oracle.com/en/java/javase/11/tools/java.html#GUID-3B1CE181-CD30-4178-9602-230B800D4FAE https://docs.oracle.com/javase/specs/jvms/se11/html/jvms-5.html#jvms-5.2 https://www.geeksforgeeks.org/jvm-works-jvm-architecture/" }, { "title": "Java - 자바 버전별 특징 살펴보기(JAVA 7~17)", "url": "/posts/java-spec-per-version/", "categories": "지식 더하기, 이론", "tags": "Java", "date": "2021-09-25 22:25:00 +0900", "snippet": "들어가기 전글을 작성하는 현재(21.09.25) 자바 최신버전은 JDK17이다. 각 버전별 특징을 살펴보면서 이런게 있구나 정도를 인지하고, 추후에 공부가 필요한 부분은 좀 더 상세히 공부해보자.History 9 버전부터는 6개월 단위로 새로운 버전이 출시되고 있다. 버전 출시일 JDK 1....", "content": "들어가기 전글을 작성하는 현재(21.09.25) 자바 최신버전은 JDK17이다. 각 버전별 특징을 살펴보면서 이런게 있구나 정도를 인지하고, 추후에 공부가 필요한 부분은 좀 더 상세히 공부해보자.History 9 버전부터는 6개월 단위로 새로운 버전이 출시되고 있다. 버전 출시일 JDK 1.0 1996년 1월 JDK 1.1 1997년 2월 J2SE 1.2 1998년 12월 J2SE 1.3 2000년 5월 J2SE 1.4 2002년 2월 J2SE 5.0 2004년 9월 Java SE 6 2006년 12월 Java SE 7 (LTS) 2011년 7월 Java SE 8 (LTS) 2014년 3월 Java SE 9 2017년 9월 Java SE 10 2018년 3월 Java SE 11 (LTS) 2018년 9월 Java SE 12 2019년 3월 Java SE 13 2019년 9월 Java SE 14 2020년 3월 Java SE 15 2020년 9월 Java SE 16 2021년 3월 Java SE 17 (LTS) 2021년 9월 Java SE 18 (예정) 2022년 3월 Java SE 19 (예정) 2022년 9월 Java SE 20 (예정) 2023년 3월 Java SE 21 (예정, LTS) 2023년 9월 LTS (Long Term Support) Oracle은 Oracle Lifetime 지원 정책에 설명된 대로 Oracle Java SE 제품에 대한 Oracle Premier Support를 고객에게 제공한다. Java SE 8 이후의 제품 릴리스의 경우 Oracle은 특정 릴리스만 LTS(Long-Term-Support) 릴리스로 지정한다. Java SE 7, 8, 11 및 17은 LTS 릴리스이다. 오라클은 향후 2년마다 LTS를 출시할 계획이며, 이는 다음 LTS 출시가 2023년 9월에 Java 21이라는 것을 의미한다. Oracle Premier Support를 위해, non-LTS 릴리스는 최신 LTS 릴리스의 구현 개선 사항의 누적 집합으로 간주된다. 새로운 기능 릴리스가 제공되면, 이전의 non-LTS 릴리스는 대체된 것으로 간주된다. 예를 들어 Java SE 9는 non-LTS 릴리스였으며 즉시 Java SE 10(non-LTS)으로 대체되었다. Java SE 10은 즉시 Java SE 11로 대체되었다. Java SE 11은 LTS 릴리스이므로 Oracle 고객은 Java SE 12가 릴리스되었더라도 Oracle Premier Support 및 정기 업데이트를 받게 된다. 출처 : https://www.oracle.com/java/technologies/java-se-support-roadmap.html 이제 자바의 버전별 특징을 간략하게 살펴보자. 아직 공식적으로 지원하는 버전인 7부터 정리해보았다.Java 7String in switch statement before java 7 public void testStringInSwitch(String param){ final String JAVA5 = \"Java 5\"; final String JAVA6 = \"Java 6\"; final String JAVA7 = \"Java 7\"; if (param.equals(JAVA5)){ System.out.println(JAVA5); } else if (param.equals(JAVA6)){ System.out.println(JAVA6); } else if (param.equals(JAVA7)){ System.out.println(JAVA7); } } from java 7 public void testStringInSwitch(String param){ final String JAVA5 = \"Java 5\"; final String JAVA6 = \"Java 6\"; final String JAVA7 = \"Java 7\"; switch (param) { case JAVA5: System.out.println(JAVA5); break; case JAVA6: System.out.println(JAVA6); break; case JAVA7: System.out.println(JAVA7); break; } } Binary Literals before java 7 public void testBinaryIntegralLiterals(){ int binary = 8; if (binary == 8){ System.out.println(true); } else{ System.out.println(false); }} from java 7 public void testBinaryIntegralLiterals(){ int binary = 0b1000; //2^3 = 8 if (binary == 8){ System.out.println(true); } else{ System.out.println(false); }} The try-with-resources before java 7 public void testTryWithResourcesStatement() throws FileNotFoundException, IOException{ FileInputStream in = null; try { in = new FileInputStream(\"java7.txt\"); System.out.println(in.read()); } finally { if (in != null) { in.close(); } }} from java 7 public void testTryWithResourcesStatement() throws FileNotFoundException, IOException{ try (FileInputStream in = new FileInputStream(\"java7.txt\")) { System.out.println(in.read()); }} Multi-Catch Similar Exceptions before java 7 public void testMultiCatch(){ try { throw new FileNotFoundException(\"FileNotFoundException\"); } catch (FileNotFoundException fnfo) { fnfo.printStackTrace(); } catch (IOException ioe) { ioe.printStackTrace();} from java 7 public void testMultiCatch(){ try { throw new FileNotFoundException(\"FileNotFoundException\"); } catch (FileNotFoundException | IOException fnfo) { fnfo.printStackTrace(); }} Underscores in Numeric Literalspublic void testUnderscoresNumericLiterals() { int oneMillion_ = 1_000_000; //new int oneMillion = 1000000; if (oneMillion_ == oneMillion){ System.out.println(true); } else{ System.out.println(false); }}Java 8Interface Default and Static Methods Java 8 이전에는 인터페이스에 public 추상 메서드만 선언할 수 있었다. Java 8 부터는 static, default 메서드를 선언할 수 있다. static method 정적 메서드는 인터페이스 내부에서만 사용할 수 있으며,. 구현 클래스에서 재정의할 수 없다. public interface Vehicle { static String producer() { return \"N&amp;F Vehicles\"; }}String producer = Vehicle.producer(); default method default 메소드는 default 키워드를 사용하여 선언된다. 해당 메서드는 구현 클래스의 인스턴스를 통해 액세스할 수 있다. public interface Vehicle { default String getOverview() { return \"ATV made by \" + producer(); }}public class VehicleImpl implements Vehicle { ...}Vehicle vehicle = new VehicleImpl();String overview = vehicle.getOverview();Optional Java 8 이전에는 NPE(NullPointerException)가 발생할 가능성 때문에 개발자가 참조한 값의 유효성을 주의 깊게 확인해야 했다.유효성 검사를 위해 성가시고 오류가 발생하기 쉬운 boilerplate code가 필요했다.Optional&lt;T&gt; 클래스는 T 타입의 객체에 대한 컨테이너로 작동한다. 이 값이 null이 아닌 경우 이 객체의 값을 반환할 수 있다.이 컨테이너 내부의 값이 null이면 NPE를 던지는 대신 미리 정의된 일부 작업을 수행할 수 있다. Without OptionalList&lt;String&gt; list = getList();List&lt;String&gt; listOpt = list != null ? list : new ArrayList&lt;&gt;();User user = getUser();if (user != null) { Address address = user.getAddress(); if (address != null) { String street = address.getStreet(); if (street != null) { return street; } }}return \"not specified\"; With OptionalList&lt;String&gt; listOpt = getList().orElseGet(() -&gt; new ArrayList&lt;&gt;());Optional&lt;User&gt; user = Optional.ofNullable(getUser());String result = user .map(User::getAddress) .map(Address::getStreet) .orElse(\"not specified\");Optional&lt;OptionalUser&gt; optionalUser = Optional.ofNullable(getOptionalUser());String result = optionalUser .flatMap(OptionalUser::getAddress) .flatMap(OptionalAddress::getStreet) .orElse(\"not specified\");Lambda 표현식// 익명 내부 클래스 사용Runnable runnable = new Runnable(){ @Override public void run(){ System.out.println(\"Hello world !\"); } };// 람다 사용Runnable runnable = () -&gt; System.out.println(\"Hello world two!\");Collections &amp; Streams Stream API 활용하여 함수형 프로그래밍 스타일로 코딩 가능List&lt;String&gt; list = Arrays.asList(\"franz\", \"ferdinand\", \"fiel\", \"vom\", \"pferd\");list.stream() .filter(name -&gt; name.startsWith(\"f\")) .map(String::toUpperCase) .sorted() .forEach(System.out::println);Java 9Collections List, Set, Map을 쉽게 생성할 수 있는 헬퍼 메서드가 생겼다.List&lt;String&gt; list = List.of(\"one\", \"two\", \"three\");Set&lt;String&gt; set = Set.of(\"one\", \"two\", \"three\");Map&lt;String, String&gt; map = Map.of(\"foo\", \"one\", \"bar\", \"two\");Streams takeWhile, dropWhile, iterate 메서드가 추가되었다.Stream&lt;String&gt; stream = Stream.iterate(\"\", s -&gt; s + \"s\") .takeWhile(s -&gt; s.length() &lt; 10);Optionals ifPresentOrElse 메서드가 추가되었다.user.ifPresentOrElse(this::displayAccount, this::displayLogin);Interfaces 인터페이스 내에서 private 메서드를 선언할 수 있다.public interface MyInterface { private static void myPrivateMethod(){ System.out.println(\"Yay, I am private!\"); }}HttpClient (Preview) Java 9는 자체적인 최신 Http 클라이언트인 HttpClient Preview 버전을 도입했다. 이 전까지의 Java의 내장 Http 지원은 다소 낮은 수준이었고 Apache HttpClient 또는 OkHttp와 같은 타사 라이브러리를 사용해야 했다.Java 10var 키워드 지역 변수(메서드 내 변수)에 대해 타입 추론이 가능하다.public void foo() { var name = \"Lee\";}Java 11 Java 10은 라이선스 없이 상업적으로 사용할 수 있는 마지막 무료 Oracle JDK 릴리스였다.즉, Java 11부터는 LTS를 사용하려면 비용을 지불해야 한다.Strings &amp; Files String, Files 클래스에 새로운 메서드가 추가됐다.\"Marco\".isBlank();\"Mar\\nco\".lines();\"Marco \".strip();Path path = Files.writeString(Files.createTempFile(\"helloworld\", \".txt\"), \"Hi, my name is!\");String s = Files.readString(path);Running Java Files 자바 파일을 실행하기 위해 javac로 컴파일 하지 않아도 된다. before java 11 $ javac HelloWorld.java$ java HelloWorldHello Java 8! from java 11 $ java HelloWorld.javaHello Java 11! 람다식에 var 키워드 사용(var firstName, var lastName) -&gt; firstName + lastNameNot Predicate Method Predicate 인터페이스에 정적 not() 메소드가 추가되었다.List&lt;String&gt; sampleList = Arrays.asList(\"Java\", \"\\n \\n\", \"Kotlin\", \" \");List withoutBlanks = sampleList.stream() .filter(Predicate.not(String::isBlank)) .collect(Collectors.toList());assertThat(withoutBlanks).containsExactly(\"Java\", \"Kotlin\");HttpClient (Standard) 새로운 HTTP API는 전반적인 성능을 향상시키고 HTTP/1.1 및 HTTP/2를 모두 지원한다.HttpClient httpClient = HttpClient.newBuilder() .version(HttpClient.Version.HTTP_2) .connectTimeout(Duration.ofSeconds(20)) .build();HttpRequest httpRequest = HttpRequest.newBuilder() .GET() .uri(URI.create(\"http://localhost:\" + port)) .build();HttpResponse httpResponse = httpClient.send(httpRequest, HttpResponse.BodyHandlers.ofString());assertThat(httpResponse.body()).isEqualTo(\"Hello from the server!\");Collections java.util.Collection 인터페이스에는 IntFunction 인수를 사용하는 새로운 기본 toArray() 메소드가 포함된다.이렇게 하면 컬렉션에서 배열을 더 쉽게 생성할 수 있다.List sampleList = Arrays.asList(\"Java\", \"Kotlin\");String[] sampleArray = sampleList.toArray(String[]::new);assertThat(sampleArray).containsExactly(\"Java\", \"Kotlin\");Java 12Switch Expression(Preview)boolean result = switch (status) { case SUBSCRIBER -&gt; true; case FREE_TRIAL -&gt; false; default -&gt; throw new IllegalArgumentException(\"something is murky!\");};Java 13Multiline Strings (Preview)String htmlWithJava13 = \"\"\" &lt;html&gt; &lt;body&gt; &lt;p&gt;Hello, world&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; \"\"\";Java 14Switch Expression (Standard)int numLetters = switch (day) { case MONDAY, FRIDAY, SUNDAY -&gt; 6; case TUESDAY -&gt; 7; default -&gt; { String s = day.toString(); int result = s.length(); yield result; }};Records (Preview) boilerplate code를 작성하는 고통을 완화하는 데 도움이 되는 Record 클래스가 추가되었다.해당 클래스는 데이터,(잠재적으로) getter/setter, equals/hashcode, toString만 포함된다. before final class Point { public final int x; public final int y; public Point(int x, int y) { this.x = x; this.y = y; }} // state-based implementations of equals, hashCode, toString // nothing else record 사용public record Point(int x, int y) { }var point = new Point(1, 2);point.x(); // returns 1point.y(); // returns 2Helpful NullPointerException 마침내 NullPointerException은 정확히 어떤 변수가 null인지 설명한다.author.age = 35;---Exception in thread \"main\" java.lang.NullPointerException: Cannot assign field \"age\" because \"author\" is nullPattern Matching For InstanceOf (Preview) instanceof로 타입 검사 후 형변환을 할 필요가 없어졌다.if (obj instanceof String s) { System.out.println(s.contains(\"hello\"));}Packaging Tool (Incubator) 필요한 모든 종속성을 포함하여 Java 애플리케이션을 플랫폼별 패키지로 패키징할 수 있는 인큐베이팅 패키지 도구가 도입됐다. Linux: deb and rpm macOS: pkg and dmg Windows: msi and exeGarbage Collectors Concurrent Mark Sweep(CMS) Garbage Collector가 제거되고 실험용 Z Garbage Collector가 추가되었다.Java 15Text-Blocks / Multiline Strings Java 13의 실험 기능으로 도입된 Multiline Strings은 이제 production-ready 단계가 되었다.String text = \"\"\" Lorem ipsum dolor sit amet, consectetur adipiscing \\ elit, sed do eiusmod tempor incididunt ut labore \\ et dolore magna aliqua.\\ \"\"\";Sealed Classes (Preview) 이것은 클래스가 public인 동안 Shape을 서브클래스로 허용하는 유일한 클래스는 Circle, Rectangle 및 Square임을 의미한다.public abstract sealed class Shape permits Circle, Rectangle, Square {...}ZGC: Production Ready Z Garbage Collector는 더 이상 실험용이 아닌 production-ready 단계가 되었다.Java 16Unix-Domain Socket Channels 이제 Unix 도메인 소켓에 연결할 수 있습니다(macOS 및 Windows(10+)에서도 지원됨). socket.connect(UnixDomainSocketAddress.of( \"/var/run/postgresql/.s.PGSQL.5432\"));Foreign Linker API (Preview) JNI(Java Native Interface)에 대한 계획된 교체로, 기본 라이브러리에 바인딩할 수 있다.Records &amp; Pattern Matching Records 클래스 및 Pattern Matching 모두 production-ready 단계가 되었다.Java 17Sealed Class (Standard)public abstract sealed class Shape permits Circle, Rectangle {...}public class Circle extends Shape {...} // OKpublic class Rectangle extends Shape {...} // OKpublic class Triangle extends Shape {...} // Compile error// No need for default case if all permitted types are covereddouble area = switch (shape) { case Circle c -&gt; Math.pow(c.radius(), 2) * Math.PI case Rectangle r -&gt; r.a() * r.b()};Pattern Matching For switch (Preview)String formatted = switch (o) { case Integer i &amp;&amp; i &gt; 10 -&gt; String.format(\"a large Integer %d\", i); case Integer i -&gt; String.format(\"a small Integer %d\", i); case Long l -&gt; String.format(\"a Long %d\", l); default -&gt; o.toString();};참고 자료 https://en.wikipedia.org/wiki/Java_version_history https://www.marcobehler.com/guides/a-guide-to-java-versions-and-features https://advancedweb.hu/a-categorized-list-of-all-java-and-jvm-features-since-jdk-8-to-17/ https://www.baeldung.com/java-11-new-features https://blog.ippon.tech/comparing-java-lts-releases/ https://www.oracle.com/java/technologies/java-se-support-roadmap.html https://dzone.com/articles/new-java-7-language-features https://www.baeldung.com/java-8-new-features" }, { "title": "Java - 왜 equals()와 hashCode()는 함께 오버라이딩 해야할까 ?", "url": "/posts/equals-and-hashcode/", "categories": "지식 더하기, 이론", "tags": "Java", "date": "2021-09-24 22:25:00 +0900", "snippet": "들어가기 전실무에서는 equals(), hashCode() 메서드를 오버라이드 할 일이 별로 없었던 것 같다. 하지만 몇 달 전 간단한 웹 서버를 만들어보는 토이 프로젝트를 진행하던 중, 클라이언트로부터 받은 요청을 유틸 클래스의 메서드를 사용해서 HttpRequest 객체를 만드는 부분에 대한 테스트 코드를 작성했는데 원하는 결과가 나오지 않았다.객...", "content": "들어가기 전실무에서는 equals(), hashCode() 메서드를 오버라이드 할 일이 별로 없었던 것 같다. 하지만 몇 달 전 간단한 웹 서버를 만들어보는 토이 프로젝트를 진행하던 중, 클라이언트로부터 받은 요청을 유틸 클래스의 메서드를 사용해서 HttpRequest 객체를 만드는 부분에 대한 테스트 코드를 작성했는데 원하는 결과가 나오지 않았다.객체간의 비교를 수행했는데 equals()가 오버라이드 되지 않아 발생했던 문제였다. 그런데 equals()를 오버라이드하면 hashCode()도 반드시 함께 오버라이드 해야한다고 한다. 왜 그래야하는 것인지 살펴보자.※ 작성했던 테스트 코드@Testpublic void getHttpRequest() throws Exception { Map&lt;String, HttpRequest&gt; requests = new HashMap&lt;&gt;(); String request1 = \"GET /index.html HTTP/1.1\"; String request2 = \"POST /user/create HTTP/1.1\"; requests.put(request1, new HttpRequest(GET, \"/index.html\")); requests.put(request2, new HttpRequest(POST, \"/user/create\")); for (String req : requests.keySet()) { HttpRequest answer = requests.get(req); HttpRequest getObj = HttpRequestUtils.getHttpRequest(new ByteArrayInputStream(req.getBytes())); assertThat(getObj, is(answer)); // equals 오버라이드 전 원하는 결과가 나오지 않았음 }}equals() Object 클래스에서의 equals() 메서드는 기본적으로 ‘==’ 연산자를 통해 메모리 주소 비교(동일성 비교)를 한다.public boolean equals(Object obj) { return (this == obj);} equals 메소드는 null이 아닌 객체에 대해 아래의 등가 관계가 성립되어야 한다. reflexive : x.equals(x)는 항상 성립한다. symmetric : x.equals(y)와 y.equals(x)는 동일한 결과를 반환한다. transitive : x.equals(y)가 성립하면 y.equals(z)도 성립한다. consistent : equals()의 값은 equals()에 포함된 속성이 변경되는 경우에만 변경되어야 한다. 위 규칙을 위반하는 예시(x.equals(y)!=y.equals(x) 인 경우)class Money { int amount; String currencyCode; @Override public boolean equals(Object o) { if (o == this) return true; if (!(o instanceof Money)) return false; Money other = (Money)o; boolean currencyCodeEquals = (this.currencyCode == null &amp;&amp; other.currencyCode == null) || (this.currencyCode != null &amp;&amp; this.currencyCode.equals(other.currencyCode)); return this.amount == other.amount &amp;&amp; currencyCodeEquals; }}class WrongVoucher extends Money { private String store; @Override public boolean equals(Object o) { if (o == this) return true; if (!(o instanceof WrongVoucher)) return false; WrongVoucher other = (WrongVoucher)o; boolean currencyCodeEquals = (this.currencyCode == null &amp;&amp; other.currencyCode == null) || (this.currencyCode != null &amp;&amp; this.currencyCode.equals(other.currencyCode)); boolean storeEquals = (this.store == null &amp;&amp; other.store == null) || (this.store != null &amp;&amp; this.store.equals(other.store)); return this.amount == other.amount &amp;&amp; currencyCodeEquals &amp;&amp; storeEquals; }}Money cash = new Money(42, \"USD\");WrongVoucher voucher = new WrongVoucher(42, \"USD\", \"Amazon\");voucher.equals(cash) =&gt; false // As expected.cash.equals(voucher) =&gt; true // That's wrong.동등성(equality)과 동일성(identity) 실생활의 예를 생각해본다면, 같은 날 입대한 두 군인의 경우 ‘동등한’ 훈련병의 입장이라고 생각할 수 있다. 하지만, 두 훈련병은 각각 다른 사람이기에 ‘동일’하진 않다. 동등성(equality) 두 객체가 동일한 상태(속성)를 포함하는지 여부 동일성(identity) 두 객체가 동일한 메모리 주소를 공유하는지 여부 두 객체가 동일하면 동등하지만, 동등하다고 해서 동일하지는 않다.hashCode() 객체에 대한 해시 코드 값을 반환한다. 이 값은 해시 테이블을 사용할 때 주어진 키에 대한 해시 값으로 사용된다.이를 활용해 데이터에 효율적으로 접근하기 위해, 해당 값을 사용하여 데이터를 저장한다. hashCode() 메서드는 가상 머신에 의해 native operation으로 구현된다. hashCode() 값은 (32비트 아키텍처에서) 메모리 참조 또는 (64비트 아키텍처에서) 메모리 참조에 대한 modulo 32 표현을 반환하는 것으로 구현되는 경우가 많다.public native int hashCode(); hashCode()가 준수해야 할 사항은 다음과 같다. internal consistency : hashCode()의 값은 equals()에서 비교하는 속성이 변경되는 경우에만 변경될 수 있다. equals consistency : x.equals(y)이면 x,y의 hashCode()는 동일한 값을 반환해야 한다. collisions : x.equals(y)가 성립하지 않더라도 x,y의 hashCode()는 동일한 값을 반환할 수도 있다. equals()와 hashCode()를 함께 오버라이드 해야하는 이유 hashCode()가 준수해야 할 사항의 두 번째 항목을 주목해보자. 만약, 어떤 클래스에서 equals() 메서드만 오버라이드 되었다면 x.equals(y)가 성립하더라도 x,y의 hashCode() 값은 다를 것이다.이런 상황으로 인해 발생되는 문제는 ? → 해시맵을 사용할 때 의도치 않은 결과를 얻게된다.class Team { String city; String department; public Team(String city, String department) { this.city = city; this.department = department; } @Override public final boolean equals(Object o) { if (o == this) return true; if (!(o instanceof Team)) return false; Team other = (Team) o; boolean isSameCity = (this.city == null &amp;&amp; other.city == null) || (this.city != null &amp;&amp; this.city.equals(other.city)); boolean isSameDept = (this.department == null &amp;&amp; other.department == null) || (this.department != null &amp;&amp; this.department.equals(other.department)); return isSameCity &amp;&amp; isSameDept; }}Map&lt;Team,String&gt; leaders = new HashMap&lt;&gt;();leaders.put(new Team(\"New York\", \"development\"), \"Anne\");leaders.put(new Team(\"Boston\", \"development\"), \"Brian\");leaders.put(new Team(\"Boston\", \"marketing\"), \"Charlie\");Team myTeam = new Team(\"New York\", \"development\");String myTeamLeader = leaders.get(myTeam); // \"Anne\"를 기대하지만 결과는 null이 나온다.equals(), hashCode() 작성하기 일반적으로 직접 일일이 작성하진 않고, IDE의 자동완성이나 Lombok 또는 Java7 부터 도입된 java.util 패키지의 Objects 클래스의 메서드를 활용한다.IntelliJ Defaultclass Team { String city; String department; public Team(String city, String department) { this.city = city; this.department = department; } @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Team team = (Team) o; if (city != null ? !city.equals(team.city) : team.city != null) return false; return department != null ? department.equals(team.department) : team.department == null; } @Override public int hashCode() { int result = city != null ? city.hashCode() : 0; result = 31 * result + (department != null ? department.hashCode() : 0); return result; }}Objects 유틸 활용import java.util.Objects;class Team { String city; String department; public Team(String city, String department) { this.city = city; this.department = department; } @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Team team = (Team) o; return Objects.equals(city, team.city) &amp;&amp; Objects.equals(department, team.department); } @Override public int hashCode() { return Objects.hash(city, department); }}Lombok 활용import lombok.EqualsAndHashCode;@EqualsAndHashCodeclass Team { String city; String department; public Team(String city, String department) { this.city = city; this.department = department; }}참고 자료 https://docs.oracle.com/javase/7/docs/api/java/lang/Object.html#hashCode() https://stackoverflow.com/questions/1692863/what-is-the-difference-between-identity-and-equality-in-oop https://www.baeldung.com/java-equals-hashcode-contracts https://www.javacodegeeks.com/2016/03/equality-vs-identity.html https://www.baeldung.com/java-hashcode" }, { "title": "OS - OS 캐시", "url": "/posts/os-caching/", "categories": "지식 더하기, 이론", "tags": "OS", "date": "2021-09-16 22:25:00 +0900", "snippet": "OS 캐시 OS에는 디스크 내의 데이터에 빠르게 액세스할 수 있도록 하는 구조가 갖춰져 있다. OS는 메모리를 이용해서 디스크 액세스를 줄인다. 애플리케이션에서 이를 활용한다면 OS에 상당부분을 맡길 수 있다. 이를 가능하게 하는 것이 OS캐시이다. 리눅스의 경우에는 페이지 캐시(page cache), 버퍼 캐시(...", "content": "OS 캐시 OS에는 디스크 내의 데이터에 빠르게 액세스할 수 있도록 하는 구조가 갖춰져 있다. OS는 메모리를 이용해서 디스크 액세스를 줄인다. 애플리케이션에서 이를 활용한다면 OS에 상당부분을 맡길 수 있다. 이를 가능하게 하는 것이 OS캐시이다. 리눅스의 경우에는 페이지 캐시(page cache), 버퍼 캐시(buffer cache)라고 하는 캐시 구조를 갖추고 있다. 가상 메모리 프로그램이 메모리를 사용할 때 물리적인 메모리를 직접 다루지 않고, 추상화한 소프웨어적인 메모리를 다루는 구조이다. 하드웨어에서 제공하는 페이징(paging)이라고 하는 가상 메모리 구조를 사용해 실현한다. 즉, 가상 메모리란 OS가 커널 내에서 물리 메모리를 추상화 한 것이다. 프로세스에서 메모리를 다루기 쉽게 하는 이점을 제공한다. 가상 메모리의 관리는 OS가 담당한다.※ 참고. 스왑(Swap) 스왑은 가상 메모리를 응용한 기술 중 하나이다. 물리 메모리가 부족할 때 2차 기억창치(주로 디스크)를 메모리로 간주해서 외형상의 메모리 부족을 해소한다. 물리 메모리가 부족한 경우 장시간 사용되지 않은 영역의 가상 메모리와 물리 메모리 영역 매핑을 해제한다. 해제된 데이터는 디스크 등에 저장해두고 다시 필요해지면 원래로 되돌린다. 프로세스에서 메모리에 접근하려면 ? 프로세스에서 OS로 메모리를 요청한다. OS가 메모리에서 비어있는 (물리)메모리 주소 찾는다. OS는 프로세스로 (가상)메모리 주소를 반환한다. OS는 메모리 4KB 정도를 블록으로 확보해서 관리한다. 하나의 블록 단위를 ‘페이지’라고 한다. 즉, 페이지란 OS가 물리 메모리를 확보/관리하는 단위이다. OS는 프로세스에서 메모리 요청을 받으면 필요한 만큼의 페이지를 확보해서 프로세스에 반환한다. 프로세스는 가상 메모리에 접근한다. (물리 메모리로 직접 접근할 수 없음) 커널이 프로세스에 반환하는 가상 메모리 영역은 아직 실제로 물리 메모리와 연결되어 있지 않다. 즉, 실체가 없는 메모리 영역이다. 프로세스가 가상 영역에 대해 쓰기 작업을 수행하는 시점에 물리 메모리 영역과 연결관계를 맺는다. 가상 메모리를 사용해야 하는 이유 가상 메모리를 통해 얻을 수 있는 이점은 매우 크며, 이는 멀티 태스킹 OS를 지탱하는 중요한 역할을 담당한다. 물리 메모리로 탑재되어 있는 용량 이상의 메모리를 다룰 수 있을 것처럼 프로세스에게 보일 수 있다. 물리 메모리상에서는 흩어져 있는 영역을 연속된 하나의 메모리 영역으로 프로세스에게 보일 수 있다. Swap을 사용할 수 있다. 서로 다른 두 개의 프로세스가 참조하는 가상 메모리 영역을 동일한 물리 메모리 영역에 대응 시킬 수 있다. 이를 통해 두 개의 프로세스가 메모리 내용을 공유할 수 있다. IPC(Inter Process Communication) 공유 메모리 등은 이 방법으로 구현된다. 리눅스의 페이지 캐시 원리 OS는 확보한 페이지를 메모리상에 계속 확보해두는 기능을 갖고 있다. 이를 ‘페이지 캐시’라고 한다. 즉, 커널이 한 번 할당한 메모리를 해제하지 않고 계속 남겨두는 것이 페이지 캐시의 기본이다.페이지 캐싱 과정 OS는 프로세스에게 요청받은 데이터를 디스크로부터 블록의 크기(ex : 4KB)만큼 읽는다. 읽은 데이터를 (물리)메모리상에 위치시킨다. 물리 메모리 주소를 가상 메모리 주소로 변환하여 프로세스에게 알려준다. 프로세스는 (가상)메모리에 접근해서 데이터를 읽는다. 프로세스가 데이터를 읽고 나서도 OS에서는 해당 데이터를 메모리에서 해제하지 않고 남겨둔다. 이를 통해 다른 프로세스에서 동일한 데이터가 필요한 경우 OS가 다시 디스크에 접근하지 않아도 된다. ※ 캐시 이외의 메모리가 필요하게 되면 오래된 캐시가 파기된다. 즉, 메모리가 1GB 인데 그 중 900MB가 캐시로 사용되고 있다고 해서 메모리가 부족하다고 생각하지 않아도 된다.4GB의 파일을 1.5GB의 여유가 있는 메모리에 캐싱할 수 있을까? 정답은 ‘가능하다’이다. 어떻게 가능한 것인지 원리를 살펴보자. 캐싱하고자 하는 파일에 대해 i노드 번호와 오프셋 두 가지 값을 키로 캐싱한다. i노드 번호 - 리눅스가 파일을 식별하는 번호 오프셋 - 파일의 어느 위치부터 시작할지를 나타내는 값 따라서, 파일 전체가 아닌 일부를 캐싱할 수 있다. ‘파일의 크기 &gt; 메모리 공간’ 이기 때문에, 파일의 일부를 캐싱하더라도 결국 메모리 공간이 꽉 차게 된다. LRU(Least Recently Used) - 새로운 데이터를 캐싱하기 위한 메모리 공간을 확보하기 위해 가장 오래된 데이터를 파기 파일이 아무리 크더라도 캐싱된 키로부터 해당 페이지를 찾을 때의 데이터 구조는 최적화 되어있다. OS(커널) 내부에서 사용되고 있는 자료 구조는 Radix Tree이다. 캐싱된 파일이 커지더라도 캐시 탐색속도가 떨어지지 않도록 개발된 자료 구조이다. 참고 자료 이토 나오야 외 1명, 『웹 개발자를 위한 대규모 서비스를 지탱하는 기술』, 제이펍(2011), 8장 이토 나오야 외 5명, 『24시간 365일 서버/인프라를 지탱하는 기술』, 제이펍(2009), 4장" }, { "title": "추상화를 통해 엑셀 생성 로직 리팩토링하기", "url": "/posts/applying-absraction-for-excel-logic-refactoring/", "categories": "경험하기, 작업 노트", "tags": "", "date": "2021-09-13 22:25:00 +0900", "snippet": "상황현재 운영하고 있는 시스템에는, 사용자가 조회한 결과에 대해 엑셀 파일로 다운받을 수 있는 기능을 몇몇 화면에서 제공한다.엑셀을 만드는 부분은 스프링부트 기반의 백엔드 서버에서 담당한다. 문제는 해당 로직에서 모든 셀에 대한 영역 지정을 하드코딩으로 해주고 있다는 것이다. 예를 들어, 아래와 같은 경우 Title1의 병합 영역을 ‘2번 행 + B,...", "content": "상황현재 운영하고 있는 시스템에는, 사용자가 조회한 결과에 대해 엑셀 파일로 다운받을 수 있는 기능을 몇몇 화면에서 제공한다.엑셀을 만드는 부분은 스프링부트 기반의 백엔드 서버에서 담당한다. 문제는 해당 로직에서 모든 셀에 대한 영역 지정을 하드코딩으로 해주고 있다는 것이다. 예를 들어, 아래와 같은 경우 Title1의 병합 영역을 ‘2번 행 + B,C 열’ 이런식으로 명시해주고 있다.결과적으로 중간에 새로운 Title을 추가하게되면 (삭제가 되는 경우도 마찬가지로) 그 이후의 영역에 대한 하드코딩이 다 변경되어야 한다는 것이다. 아래의 예시는 영역이 몇 개 없지만, 실제 코드에는 매우 많은 영역이 존재한다. 따라서, 사용자로부터 중간에 새로운 컬럼이 추가되어야 한다는 요구사항이 들어온다면매우 비효율적인 노가다 작업을 해야한다.해결 과정 절차지향스럽게 산재되어 있는 코드 대신, 역할에 따라 각각의 클래스로 추상화해보자.1. 셀 추상화 하나의 셀 뿐만 아니라, 병합된 셀, 하위 셀(위 예시에 subtitle 부분)을 포함하는 셀 등을 나타낼 수 있는 클래스가 필요하다고 생각했다.또한 기존처럼 행과 열의 특정 위치가 아닌 ‘몇 개의 행을 병합할 것인지’, ‘몇 개의 열을 병합할 것인지’에 대한 정보가 필요하다고 생각했다.public class CustomCell { private final String cellValue; // 셀에 세팅될 값 private final int rowSpan; // 행 병합 수 private final int colSpan; // 열 병합 수 private String dataKey; // Map에 담긴 조회결과에서, 해당 컬럼에 세팅되어야할 데이터를 가져오기 위한 key값 private List&lt;CustomCell&gt; childs; // 하위 셀 ...}2. 테이블 추상화 테이블은 데이터가 무엇인지 나타내는 헤더와 데이터 영역인 body를 포함해야한다.또한, 위 예시에는 없지만 테이블의 좌측에 붙어있는 수직 헤더가 포함되는 경우도 있다.public class CustomTable { private int horizontalHeaderStartRow; // 수평 헤더 시작 행 private String horizontalHeaderStartCol; // 수평 헤더 시작 열 private int verticalHeaderStartRow; // 수직 헤더 시작 행 private String verticalHeaderStartCol; // 수직 헤더 시작 열 private List&lt;CustomCell&gt; horizontalHeaders; // 수평 헤더 private List&lt;CustomCell&gt; verticalHeaders; // 수직 헤더 private List&lt;Map&lt;String,Object&gt;&gt; body; // 세팅할 데이터}3. 엑셀 만드는 부분 추상화 세팅된 CustomTable을 활용하여 엑셀 파일을 만든다. 이 과정에서 셀에 스타일 등을 적용할 수 있다.public class CustomExcelCreator { private List&lt;CustomTable&gt; tables; ... public void createExcel() { ... } public void createCell(...) { ... } public void createTable(...) { ... }}결과 새로운 헤더가 추가되어야 하면 해당 영역을 CustomCell로 만든 뒤, 헤더 리스트에 추가한다. 기존에는 추가된 컬럼 이후의 모든 부분에 대해 코드를 재작성해야 했다면, 앞으로는 헤더 리스트에 추가하는 부분만 고치면 된다.느낀점 쓸데없는 곳에 시간낭비 하지 않도록 항상 확장성을 생각해서 코드를 작성하자. 확장성이 좋다는 것은 기능, 요구사항 등이 추가/변경되는 경우 최소한의 코드 수정만 하게 만드는 것이라고 생각한다. 객체지향의 기본이 되는 캡슐화, 추상화, 다형성 등의 개념을 잘 이해하고 적용하는게 중요한 것 같다." }, { "title": "WEB - CDN(Content Delivery Network)", "url": "/posts/content-delivery-network/", "categories": "지식 더하기, 이론", "tags": "WEB", "date": "2021-09-11 00:25:00 +0900", "snippet": "들어가기 전얼마전 면접에서 ‘화면의 초기 로딩 속도가 느리다면 어떻게 해결할 수 있을까요?’ 라는 질문에 제대로 대답하지 못했던 기억이 있다.뒤늦게 생각났는데 CDN을 활용하면 어느정도 개선되지 않을까 하는 생각이 들었다. 하지만, CDN에 대해 어렴풋이 알고있어 얘기를 꺼내지 못했다.이번 기회에 CDN에 대해 잘 정리해보자.CDN이란 ? Conte...", "content": "들어가기 전얼마전 면접에서 ‘화면의 초기 로딩 속도가 느리다면 어떻게 해결할 수 있을까요?’ 라는 질문에 제대로 대답하지 못했던 기억이 있다.뒤늦게 생각났는데 CDN을 활용하면 어느정도 개선되지 않을까 하는 생각이 들었다. 하지만, CDN에 대해 어렴풋이 알고있어 얘기를 꺼내지 못했다.이번 기회에 CDN에 대해 잘 정리해보자.CDN이란 ? Content Delivery Network : 컨텐츠 전송 네트워크 정적 컨텐츠를 전송하는데 쓰이는, 지리적으로 분산된 서버의 네트워크이다. html, 이미지, js, 비디오 등과 같은 정적 컨텐츠를 캐싱한다. 어떤 사용자가 웹사이트를 방문하면, 그 사용자에게 가장 가까운 CDN 서버가 정적 컨텐츠를 전달한다. 다음은 CDN을 활용하는 예를 보여준다. 사용자 A가 이미지 URL을 이용해 image.png에 접근한다. URL의 도메인은 CDN 서비스 사업자가 제공한 것이다. 예를 들어 다음 두 URL은 Cloudfront와 Akamai CDN이 제공하는 URL의 예이다. https://mysite.cloudfront.net/logo.png https://mysite.akamai.com/image-manager/img/logo.png CDN 서버의 캐시에 해당 이미지가 없는 경우, 서버는 원본(origin) 서버에 요청하여 파일을 가져온다. 원본 서버는 웹 서버일 수도 있고 Amazon S3와 같은 온라인 저장소일 수도 있다. 원본 서버가 파일을 CDN 서버에 반환한다. 응답의 HTTP 헤더에는 해당 파일이 얼마나 오래 캐시될 수 있는지를 설명하는 TTL(Time-To-Live)값이 들어있다. CDN 서버는 파일을 캐시하고 사용자 A에게 반환한다. 이미지는 TTL에 명시된 시간이 끝날 때까지 캐시된다. 사용자가 B가 같은 이미지에 대한 요청을 CDN 서버에 전송한다. 만료되지 않은 이미지에 대한 요청은 캐시를 통해 처리한다. 출처 : https://mygumi.tistory.com/67 CDN 사용 시 고려해야 할 사항1. 비용CDN은 보통 제 3 사업자(thrid-party providers)에 의해 운영되며, CDN으로 들어가고 나가는 데이터 전송 양에 따라 요금을 내게 된다.따라서, 자주 사용되지 않는 컨텐츠를 캐싱하는 것은 득이 될 것이 없으므로 CDN에서 빼는 것을 고려한다.2. 적절한 만료 시한 설정시의성이 중요한(time-sensitive) 컨텐츠의 경우, 만료 시점을 잘 정해야 한다.너무 길지도 않고 짧지도 않아야 하는데, 너무 길면 컨텐츠의 신선도는 떨어질 것이고, 너무 짧으면 원본 서버에 빈번히 접속하게 되어 좋지 않다.3. CDN 장애에 대한 대처 방안CDN 자체가 죽었을 경우 웹사이트/애플리케이션이 어떻게 동작해야 하는지 고려해야 한다.일시적으로 CDN이 응답하지 않을 경우, 해당 문제를 감지하여 원본 서버로부터 직접 컨텐츠를 가져오도록 클라이언트를 구성하는 것이 필요할 수도 있다.4. 컨텐츠 무효화(invalidation) 방법아직 만료되지 않은 컨텐츠라 하더라도 아래 방법 가운데 하나를 쓰면 CDN에서 제거할 수 있다. CDN 서비스 사업자가 제공하는 API를 이용하여 컨텐츠 무효화 컨텐츠의 다른 버전을 서비스하도록 오브젝트 버저닝(object versioning)을 이용 컨텐츠의 새로운 버전을 지정하기 위해서는 URL 마지막에 버전 번호를 인자로 주면 된다. 예를 들어, image.png?v=2와 같은 식이다. CDN의 필요기술▶ 1. Load Balancing 사용자에게 콘텐츠 전송 요청(Delivery Request)을 받았을 때, 최적의 네트워크 환경을 찾아 연결하는 기술, GSLB(Global Server Load Balancing)이라고도 한다. 물리적으로 가장 가깝거나 여유 트래픽이 남아 있는 곳으로 접속을 유도하는 기술이다.※ GSLB(Global server Load Balancing)DNS(도메인 이름을 IP주소로 변환하는 서비스) 서비스의 발전된 형태라고 할 수 있다.※ DNS와 GSLB 차이점 health checkDNS : 서버의 상태를 알 수 없어서 서비스를 실패하는 유저도 생길 수 있다.GSLB : 서버의 상태를 모니터링(주기적으로 health check를 수행) 하고 실패한 서버의 IP는 응답에서 제외 하므로, DNS보다 훨씬 강력한 기능을 제공한다. 로드밸런싱DNS : Round Robin 방식을 사용, 정교한 로드 밸런싱이 힘들다.GSLB : 서버의 로드를 모니터링 하기 때문에 로드가 적은 서버의 IP를 반환하는 식으로 정교한 로드밸런싱을 할 수 있다. 레이턴시 기반 서비스DNS : Round Robin 방식을 사용하여 유저는 네트워크상에서 멀리 떨어진 위치의 서버로 연결 할 수도 있다.GSLB : 각 지역별로 서버에 대한 레이턴시(latency) 정보를 가지고 있기 때문에 유저가 접근을 하면, 유저의 지역으로 부터 가까운(더 작은 레이턴시를 가지는) 서버로 연결을 한다. 위치기반 서비스DNS : 유저는 Round Robin하게 서버로 연결된다.GSLB : 유저의 지역정보를 기반으로, 해당 지역을 서비스하는 서버로 연결 할 수 있다. ▶ 2. 컨텐츠를 배포하는 기술 컨텐츠의 삭제나 수정이 일어났을 때 이를 관리할 수 있는 기술이 필요하다.▶ 3. CDN의 트래픽을 감지하는 기술 통계자료를 고객에게 제공하기 위해 필요하다. 트래픽을 분산하기 위해서 필요하다CDN의 캐싱 방식 Static Caching– Origin Server에 있는 Content를 운영자가 미리 Cache Server에 복사 미리 복사해 두기 때문에 사용자가 Cache Server에 Content를 요청시 무조건 Cache Server에 있다.– 대부분의 국내 CDN에서 이 방식을 사용( ex. NCSOFT 게임파일 다운로드 등) Dynamic Caching– Origin Server에 있는 Content를 운영자가 미리 Cache Server에 복사하지 않음– 사용자가 Content를 요청시 해당 Content가 없는 경우 Origin Server로 부터 다운로드 받아 전달한다. (Content가 있는 경우는 캐싱된 Content 사용자에게 전달.)– 각각의 Content는 일정 시간이후 Cache Server에서 삭제될 수도 있다. (계속 가지고 있을 수도 있음) CDN 서비스 이용방법 CDN이용시 소스코드상에서 이미지 링크나 리다이렉트등 CDN을 서비스를 이용할 도메인을 호출 하는 경우 도메인의 주소를 CDN 업체장비의 주소로 이미지를 호출하는 경로로 변경 한다.ex) 기존의 =&gt; 서비스 신청 대상 도메인이 서버(Origin Server)를 바라보게끔 CDN장비주소로 연결 해주는 작업을 해야한다.(CDN 서비스 신청시 CDN 서비스를 이용할 도메인의 네임서버레코드를 CDN서비스 업체에서 제공하는 도메인주소 또는 IP주소로 연결을 해야 한다.) ex) http://www.goddaehee.com 이 CDN 서비스를 신청하고 싶은 도메인이라면 네임서버 A레코드를 CDN업체도메인.co.kr 으로 변경작업을 한다.브라우저 주소창에 http://www.goddaehee.com를 입력했을때 발생되는 요청을 CDN업체 서버로 가게 해주는 작업이다.기존 http://www.goddaehee.com 에 연결된 A레코드 IP값과 도메인 정보를 보내주면 CDN업체측에서 변경에 필요한 CNAME을 준다.이에 따라 DNS상에서 CNAME을 각각의 도메인에 적용하여 수정해줘야 한다. https://goddaehee.tistory.com/173 [갓대희의 작은공간]CDN vs Squid-cachehttp 프로토콜에 대해 문서 저장하는데 유용한 squid-cache 라는 것도 있는데, 용도는 비슷한것 같다.둘을 비교해보자https://www.g2.com/compare/cloudflare-inc-cloudflare-cdn-vs-squid-cachehttps://blog.matthewskelton.net/2011/12/02/improving-page-speed-cdn-vs-squid-varnish-nginx/참고 자료 알렉스 쉬, 『가상 면접 사례로 배우는 대규모 시스템 설계 기초』, 인사이트(2021), 1장" }, { "title": "Java - 자바에서 파일 읽어오기", "url": "/posts/java-reading-file/", "categories": "지식 더하기, 이론", "tags": "Java", "date": "2021-09-08 00:29:00 +0900", "snippet": "들어가기 전사실 실무에서 자바 코드로 파일을 읽어와서 데이터를 처리하거나 할 일이 없어서, 파일을 다루는 부분에 대한 코드를 작성할 일이 생기면,그 때 마다 구글링해서 사용했던 것 같다. 최근에 .json 파일에 있는 데이터를 읽어와서 처리해야할 일이 있었는데,매번 구글링 하는 것 보다 머릿속에 한 번 확실히 정리해야 할 것 같다는 생각이 들어 정리해...", "content": "들어가기 전사실 실무에서 자바 코드로 파일을 읽어와서 데이터를 처리하거나 할 일이 없어서, 파일을 다루는 부분에 대한 코드를 작성할 일이 생기면,그 때 마다 구글링해서 사용했던 것 같다. 최근에 .json 파일에 있는 데이터를 읽어와서 처리해야할 일이 있었는데,매번 구글링 하는 것 보다 머릿속에 한 번 확실히 정리해야 할 것 같다는 생각이 들어 정리해보기로 했다.※ 파일 이외에도 네트워크 등 자바에서의 전반적인 I/O(Input/Output)에 대해 알고싶으면 이 글 을 먼저 읽고 오시는 것을 추천드립니다.파일과 경로 파일을 다루는 것과 관련하여 뗼 수 없는 것이 바로 ‘경로’라고 생각한다. 따라서, 자바에서는 경로를 어떻게 다루는지 먼저 살펴보자.getPath() 간단히 말해서, getPath()는 파일의 추상 경로 이름(abstract pathname)의 문자열 표현을 반환한다. 이것은 본질적으로 파일 생성자에 전달되는 경로 이름이다. 따라서 상대 경로를 사용하여 File 객체를 생성한 경우 getPath() 메서드에서 반환된 값도 상대 경로가 된다. {user.home} / baeldung 디렉토리에서 다음 코드를 호출한다고 가정하면 결과는 다음과 같이 될 것이다.File file = new File(\"foo/foo-one.txt\");String path = file.getPath();foo/foo-one.txt // on Unix systemsfoo\\foo-one.txt // on Windows systems※ 추상 경로 이름(abstract pathname) abstract pathname은 java.io.File 객체이고 경로명 문자열은 java.lang.String 객체이다. 둘 다 디스크의 동일한 파일을 참조한다. Javadoc 에서는 java.io.File에 대해 다음과 같이 설명한다. 파일 및 디렉토리 경로 이름의 추상 표현. UI 및 운영 체제는 시스템 종속 경로 이름 문자열을 사용하여 파일 및 디렉토리의 이름을 지정한다. 따라서, File 클래스는 계층적 경로 이름에 대한 추상적이고 시스템 독립적인 뷰를 제공한다. absolute path getAbsolutePath() 메서드는 현재 사용자 디렉터리의 경로를 확인한 후 파일의 경로 이름을 반환한다. 이를 ‘절대 경로명’이라고 한다.File file = new File(\"foo/foo-one.txt\");String path = file.getAbsolutePath();/home/username/baeldung/foo/foo-one.txt // on Unix systemsC:\\Users\\username\\baeldung\\foo\\foo-one.txt // on Windows systems 상대 경로에 대한 현재 디렉터리만 확인한다. 약식 표현(예: “.” 및 “..”)은 더 이상 해결되지 않는다.File file = new File(\"bar/baz/../bar-one.txt\");String path = file.getAbsolutePath();/home/username/baeldung/bar/baz/../bar-one.txt // on Unix systemsC:\\Users\\username\\baeldung\\bar\\baz\\..\\bar-one.txt // on Windows systemscanonical path Canonical : 절대적인, 유일한 getCanonicalPath() 메서드는 한 단계 더 나아가 절대 경로 이름뿐만 아니라 “”, “…“와 같은 약어 또는 중복 이름을 확인한다. 또한 Unix 시스템에서 심볼릭 링크를 확인하고, Windows 시스템에서는 드라이브 문자를 표준 케이스로 변환한다.File file = new File(\"bar/baz/../bar-one.txt\");String path = file.getCanonicalPath();/home/username/baeldung/bar/bar-one.txt // on Unix systemsC:\\Users\\username\\baeldung\\bar\\bar-one.txt // on Windows systems 만약 현재 디렉토리가 ${user.home}/baeldung이고 new File(\"bar/baz/.baz-one.txt\")이 생성된다면 getCanonicalPath()의 출력은 다음과 같다./home/username/baeldung/bar/baz/baz-one.txt // on Unix systemsC:\\Users\\username\\baeldung\\bar\\baz\\baz-one.txt // on Windows Systems 약어 표현을 사용할 수 있는 방법은 무궁무진하기 때문에 파일 시스템의 단일 파일은 무한히 많은 절대 경로를 가질 수 있다. 그러나 canonical path는 항상 고유하다. getCanonicalPath()는 파일 시스템 쿼리가 필요하기 때문에 IOException을 throw할 수 있다. 예를 들어 Windows 시스템에서 잘못된 문자 중 하나를 사용하여 File 객체를 생성하는 경우 표준 경로를 확인하면 IOException이 발생한다. 사용 예시 File 객체를 매개변수로 받아 fully qualified name을 데이터베이스에 저장하는 메소드를 작성한다고 가정해보자. 경로가 상대적인지 또는 약어가 포함되어 있는지 알 수 없는 경우, getCanonicalPath()를 사용할 수 있다. 하지만, getCanonicalPath()는 파일 시스템을 읽기 때문에 성능이 저하된다. 따라서, 만약 중복된 이름이나 심볼릭 링크가 없고 드라이브 문자 대소문자가 표준화된 경우(Windows OS를 사용하는 경우) getAbsoultePath()를 사용하는 것이 좋다.java.io.File File 객체가 제공하는 기능은 다음과 같다. File 객체가 가리키는 것이 파일인 경우 파일이 존재하는지 확인 파일인지 경로인지 확인 읽거나 쓰거나 실행할 수 있는지 확인 언제 수정되었는지 확인 파일 생성, 삭제, 이름 변경 전체 경로 확인 File 객체가 가리키는 것이 경로인 경우 파일 목록 가져오기 경로 생성, 삭제 File 클래스의 다양한 생성자 child, pathname으로 되어 있는 값은 경로가 될 수도, 파일이 될 수도 있다. 생성자 설명 File(File parent, String child) 이미 생성되어 있는 File 객체(parent)와 그 경로의 하위 경로 이름으로 새로운 File 객체 생성 File(String pathname) 지정한 경로 이름으로 File 객체를 생성 File(String parent, String child) 상위 경로(parent)와 하위 경로(child)로 File 객체를 생성 File(URI uri) URI에 따른 File 객체를 생성 File.separator 윈도우에서는 경로를 구할 때 역슬래시(또는 ₩ 기호를 사용한다), 유닉스 계열에서는 슬래시(/)를 사용한다. 따라서 OS에 독립적인 코드를 짜기위해서는 해당 OS의 경로 구분 문자를 가져오는 File.sperator를 사용해야한다. java.nio.file(Path, Paths, Files) java.nio.file 패키지는 JVM이 파일, 파일 속성 및 파일 시스템에 접근하기 위한 인터페이스 및 클래스를 정의한다. 이 API는 java.io.File 패키지의 한계를 극복하기 위해 사용될 수 있다. 핵심은 Path이고 Paths, Files에서는 다양한 메서드를 제공하는 것 같다. Path 파일 시스템에서 파일을 찾는 데 사용할 수 있는 개체입니다. 일반적으로 시스템 종속 파일 경로를 나타냅니다. 경로는 계층적 경로를 나타내며 특수 구분 기호 또는 구분 기호로 구분된 일련의 디렉토리 및 파일 이름 요소로 구성됩니다. 파일 작업에 대한 보다 효율적이고 광범위한 접근을 위해 Files 클래스와 함께 사용할 수 있다. public interface Path extends Comparable&lt;Path&gt;, Iterable&lt;Path&gt;, Watchable Paths 이 클래스는 경로 문자열 또는 URI를 변환하여 Path를 반환하는 정적 메서드로만 구성된다. public final class Paths { private Paths() { } ...} Files 이 클래스는 파일, 디렉토리 또는 기타 유형의 파일에서 작동하는 정적 메소드로만 구성된다. 대부분의 경우 여기에 정의된 메서드는 FileSystemProvider에게 파일 작업을 수행하도록 위임한다. public final class Files { private Files() { } ...} java.nio.file.Path 자바7 부터 제공되는 NIO2의 일부이다. I/O 작업을 위한 완전히 새로운 API를 제공한다. java.io.File 클래스와 마찬가지로 Path는 파일 시스템에서 파일을 찾는 데 사용할 수 있는 객체도 생성한다. 마찬가지로 파일 클래스로 수행할 수 있는 모든 작업을 수행할 수 있습니다. Path path = Paths.get(\"baeldung/tutorial.txt\"); 기존의 File 클래스처럼 생성자를 사용하는 대신 정적 java.nio.file.Paths.get()을 사용하여 Path 인스턴스를 만든다. ‘그럼 그냥 기존의 File을 사용하면 되는것 아닌가?’ 라는 의문이 들 수 있다. 하지만 기존 File 클래스는 몇 가지 단점이 있다.java.io.File의 단점1. 에러 처리 File 클래스의 많은 메서드는 원하는 결과를 얻지 못하더라도 예외를 발생시키지 않는다. 따라서, 오류 메시지를 얻기가 어려웠다. 예를 들어, 파일을 삭제하는 다음과 같은 코드가 있다고 가정해보자. 이 코드는 오류 없이 성공적으로 컴파일 및 실행된다. 물론 false 값을 포함하는 결과 플래그가 있지만 실패의 근본 원인을 알 수는 없다. File file = new File(\"baeldung/tutorial.txt\");boolean result = file.delete(); 새로운 NIO2 API를 사용하여 동일한 기능을 다음과 같이 작성할 수 있다. 이제 컴파일러는 IOException을 처리하도록 요구한다. 또한, 예외는 실패에 대한 세부 정보를 가지고 있다. Path path = Paths.get(\"baeldung/tutorial.txt\");Files.delete(path);2. 메타데이터 지원 java.io.File 클래스는 파일에 대한 메타 정보가 필요한 I/O 작업에 대해 여러 플랫폼에서 일관되게 작동하지 않을 수 있다. 메타데이터에는 권한, 파일 소유자 및 보안 속성 등이 포함될 수 있다. 이로 인해 File 클래스는 심볼릭 링크를 전혀 지원하지 않으며, rename() 메서드는 여러 플랫폼에서 일관되게 작동하지 않는다. ※ 심볼릭 링크 : 절대 경로 또는 상대 경로의 형태로 된 다른 파일이나 경로에 대한 참조를 포함하고 있는 특별한 종류의 파일3. 메서드 확장과 성능 Method Scaling and Performance 파일 클래스의 메소드는 확장되지 않는다. 따라서, 대규모 디렉토리 목록 요청으로 인해 서버가 중단될 수 있다. 또한 DoS(Denial of Service)로 이어질 수 있는 메모리 리소스 문제를 일으킬 수도 있다. File 클래스는 Path 클래스보다 객체 지향적이다. 따라서, Path API 기반 I/O 스트림은 GC 관점에서 File 클래스보다 저렴하다. Mapping FunctionalityNIO2 패키지는 위에서 살펴본 File 클래스의 단점에 대한 개선 사항을 포함하여 모든 레거시 기능을 제공한다. 주로 java.nio.file.Files 클래스의 정적 메소드를 활용한다.1. 파일과 경로 인스턴스 생성// java.io.FileFile file = new File(\"baeldung\", \"tutorial.txt\");// java.nio.filePath path = Paths.get(\"baeldung\", \"tutorial.txt\");Path path2 = Paths.get(\"baeldung\").resolve(\"tutorial.txt\");Path pathFromFile = file.toPath();File fileFromPath = path.toFile();2. 파일, 경로 맵핑 파일을 생성하려면 createNewFile() 및 Files.createFile() 메서드를 사용한다. boolean result = file.createNewFile();Path newPath = Files.createFile(path); 디렉토리를 생성하려면 mkdir() 또는 Files.createDirectory()를 사용한다. boolean result = file.mkdir();File newPath = Files.createDirectory(path); mkdirs() 및 Files.createDirectories() 메서드를 통해 존재하지 않는 모든 하위 디렉터리를 포함할 수 있다. boolean result = file.mkdirs();File newPath = Files.createDirectories(path); 파일의 이름을 바꾸거나 이동하려면 다른 인스턴스 개체를 만들고 renameTo() 또는 Files.move()를 사용한다. boolean result = file.renameTo(new File(\"baeldung/tutorial2.txt\"));Path newPath = Files.move(path, Paths.get(\"baeldung/tutorial2.txt\")); 삭제 작업을 수행하려면 delete() 또는 Files.delete()를 사용한다. boolean result = file.delete();Files.delete(Paths.get(path)); 3. 메타데이터 읽기 (Reading Metadata) 권한이나 유형과 같은 파일에 대한 기본 정보도 얻을 수 있다.// java.io APIboolean fileExists = file.exists();boolean fileIsFile = file.isFile();boolean fileIsDir = file.isDirectory();boolean fileReadable = file.canRead();boolean fileWritable = file.canWrite();boolean fileExecutable = file.canExecute();boolean fileHidden = file.isHidden();// java.nio APIboolean pathExists = Files.exists(path);boolean pathIsFile = Files.isRegularFile(path);boolean pathIsDir = Files.isDirectory(path);boolean pathReadable = Files.isReadable(path);boolean pathWritable = Files.isWritable(path);boolean pathExecutable = Files.isExecutable(path);boolean pathHidden = Files.isHidden(path);4. 경로 관련 메서드 (Pathname Methods)// java.io APIString absolutePathStr = file.getAbsolutePath();String canonicalPathStr = file.getCanonicalPath();// java.nio APIPath absolutePath = path.toAbsolutePath();Path canonicalPath = path.toRealPath().normalize();URI fileUri = file.toURI();URI pathUri = path.toUri();// java.io APIString[] list = file.list();File[] files = file.listFiles();// java.nio APIDirectoryStream&lt;Path&gt; paths = Files.newDirectoryStream(path);참고 자료 이상민, 『자바의 신 2』, 로드북(2017), 26장 https://www.javatpoint.com/java-path-vs-file https://www.baeldung.com/java-path-vs-file https://docs.oracle.com/javase/7/docs/api/java/nio/file/Path.html https://stackoverflow.com/questions/24611148/what-does-abstract-path-means-in-java-io" }, { "title": "Java - 자바 IO, NIO", "url": "/posts/java-io-nio/", "categories": "지식 더하기, 이론", "tags": "Java", "date": "2021-09-07 00:29:00 +0900", "snippet": "Java I/O Input, Output을 나타낸다. In, Out의 기준은 JVM이다. 즉, JVM으로 들어오는 데이터를 다루는 경우엔 Input 데이터를 내보내는 경우엔 Output이다. Stream 기반이다. 한 번에 1byte 또는 그 이상의 byte를 읽는다. 데이터가 캐시되지 않는다. 따라서 데이터의 앞뒤로 이동...", "content": "Java I/O Input, Output을 나타낸다. In, Out의 기준은 JVM이다. 즉, JVM으로 들어오는 데이터를 다루는 경우엔 Input 데이터를 내보내는 경우엔 Output이다. Stream 기반이다. 한 번에 1byte 또는 그 이상의 byte를 읽는다. 데이터가 캐시되지 않는다. 따라서 데이터의 앞뒤로 이동할 수 없다. 만약, 데이터에서 앞뒤로 이동해야 하는 경우 먼저 버퍼에 캐시해야 한다. blocking 방식이다. 즉, 스레드가 read() 또는 write()를 호출하면 읽을 데이터가 있거나 데이터가 완전히 쓰여질 때까지 해당 스레드는 다른 작업을 수행할 수 없다. 크게 InputStream/OutputStream, Reader/Writer로 구분된다.java.io.InputStream 바이트 기반 입력 스트림의 최상위 클래스이며 추상클래스이다. 추상 클래스이다. Closeable 인터페이스를 구현한다. 즉, java.io 패키지에 있는 클래스를 사용할 때에는 하던 작업이 종료되면 해당 리소스를 close() 메서드로 항상 닫아주어야 한다. 여기서 리소스는 파일, 네트워크 연결 등이 될 수 있다. 주요 하위 클래스로는 FileInputStream, DataInputStream, ObjectInputStream, BufferedInputStream 등이 있다.java.io.OutputStream 바이트 기반 출력 스트림의 최상위 클래스이며 추상클래스이다. Closeable, Flushable 인터페이스를 구현한다. Flushable에는 flush() 메서드만 정의되어 있다. 일반적으로 어떤 리소스에 데이터를 쓸 때, 쓰기 작업을 요청할 때마다 저장하는 방식으로 하게되면 효율이 떨어진다. 따라서, 버퍼(buffer)를 갖고 데이터를 쌓아두었다가 어느정도 차게 되면 한번에 쓰는 것이 좋다. flush() 메서드는 버퍼에 있는 데이터를 기다리지 말고 무조건 저장하게 만드는 기능을 수행한다. 주요 하위 클래스로는 FileOutputStream, DataOutputStream, ObjectOutputStream, BufferedOutputStream 등이 있다.java.io.Reader 문자 기반 입력 스트림의 최상위 클래스이며 추상클래스이다. public abstract class Reader implements Readable, Closeable 주요 하위 클래스로는 FileReader, InputStreamReader, BufferedReader 등이 있다.java.io.Writer 문자 기반 출력 스트림의 최상위 클래스이며 추상클래스이다. public abstract class Writer implements Appendable, Closeable, Flushable 주요 하위 클래스로는 FileWriter, OutputStreamWriter, PrinterWriter, BufferedWriter 등이 있다.Blocking 방식 살펴보기Name: AnnaAge: 25Email: anna@mailserver.comPhone: 1234567890 위 텍스트 파일을 읽는 프로그램을 작성해보자.InputStream input = ... ; // get the InputStream from the client socketBufferedReader reader = new BufferedReader(new InputStreamReader(input));String nameLine = reader.readLine();String ageLine = reader.readLine();String emailLine = reader.readLine();String phoneLine = reader.readLine(); 출처 : http://tutorials.jenkov.com/java-nio/nio-vs-io.html#main-differences-between-java-nio-and-io New IO java.nio 패키지는 Java 1.4에서 처음 도입되었으며 향상된 파일 작업 및 ASynchronousSocketChannel는 Java 1.7(NIO.2)에서 업데이트되었다. 스트림 기반 → 버퍼(Buffer)와 채널(Channel) 기반으로 데이터 처리 NIO에서 데이터를 주고 받을 때는 버퍼를 통해서 처리한다. 버퍼 기반 데이터 처리 데이터는 버퍼로 읽혀지고 나중에 처리된다. 필요에 따라 버퍼에서 앞뒤로 이동할 수 있습니다. 이를 통해 처리 중에 유연성을 높일 수 있다. 버퍼를 완전히 처리하려면 필요한 모든 데이터가 버퍼에 포함되어 있는지 확인해야 한다. 또한 버퍼로 더 많은 데이터를 읽을 때 아직 처리하지 않은 버퍼의 데이터를 덮어쓰지 않도록 해야한다. Non-Blocking 스레드는 채널에 데이터 읽기를 요청할 수 있으며 현재 사용 가능한 데이터만 가져오거나 현재 사용 가능한 데이터가 없는 경우 아무 것도 가져오지 않는다. 데이터를 읽을 수 있을 때까지 blocking 상태를 유지하는 대신 스레드는 다른 작업을 계속할 수 있다. 스레드는 일부 데이터가 채널에 기록되도록 요청할 수 있지만, 데이터가 완전히 기록되기를 기다리지는 않는다. 그런 다음 스레드는 계속해서 중간에 다른 작업을 수행할 수 있다. IO 호출에서 차단되지 않을 때 스레드가 유휴 시간을 보내는 것은 일반적으로 그 동안 다른 채널에서 IO를 수행하는 것이다. 즉, 단일 스레드가 여러 입력 및 출력 채널을 관리할 수 있다. bufferFull() 메서드는 버퍼에 읽어들인 데이터의 양을 추적하고 버퍼가 가득 찼는지 여부에 따라 true 또는 false를 반환해야한다. 즉, 버퍼가 처리할 준비가 되면 가득 찬 것으로 간주된다. bufferFull() 메서드는 버퍼를 스캔하지만 메서드가 호출되기 전과 동일한 상태로 버퍼를 유지해야 한다. 그렇지 않으면 버퍼로 읽은 다음 데이터가 올바른 위치에서 읽히지 않을 수 있다.ByteBuffer buffer = ByteBuffer.allocate(48);int bytesRead = inChannel.read(buffer);while(! bufferFull(bytesRead) ) { bytesRead = inChannel.read(buffer);} 위 코드를 다음과 같은 다이어그램으로 나타낼 수 있다. 출처 : http://tutorials.jenkov.com/java-nio/nio-vs-io.html#main-differences-between-java-nio-and-io Java NIO 주요 컴포넌트 Java NIO는 Buffer, Channel, Selector를 기반으로 새로운 I/O 모델을 제공한다.1. Buffer NIO에서 제공하는 Buffer는 java.nio.Buffer 클래스를 확장하여 사용한다. 기본 데이터 유형에 대해 버퍼를 사용할 수 있습니다. Java NIO는 버퍼 지향 패키지이다. 즉, 채널을 사용하여 추가 처리된 버퍼에 데이터를 쓰거나 읽을 수 있다. ByteBuffer, CharBuffer, DoubleBuffer 등 다양한 타입의 버퍼가 존재한다. 버퍼는 기본 데이터 유형을 보유하고 다른 NIO 패키지에 대한 개요를 제공하므로 데이터의 컨테이너 역할을 한다. 버퍼는 채우기, 비우기, 뒤집기, 되감기 등을 할 수 있다. 위치와 관련된 메서드 position() : 현재의 위치를 나타냄 limit() : 읽거나 쓸 수 없는 위치를 나타냄 capacity() : 버퍼의 크기를 나타냄 0 &lt;= position &lt;= limit &lt;= capacity 예제 코드import java.io.FileInputStream;import java.io.FileOutputStream;import java.io.IOException;import java.nio.ByteBuffer;import java.nio.channels.FileChannel;public class NioSample { public static void main(String[] args) { NioSample sample = new NioSample(); sample.basicWriteAndRead(); } public void basicWriteAndRead() { String fileName = \"nio.txt\"; try { writeFile(fileName, \"My first NIO sample\"); readFile(fileName); } catch (IOException e) { e.printStackTrace(); } } private void writeFile(String fileName, String data) throws IOException { FileChannel channel = new FileOutputStream(fileName).getChannel(); // 파일을 쓰기 위한 채널 얻어오기 byte[] byteData = data.getBytes(); ByteBuffer buffer = ByteBuffer.wrap(byteData); // ByteBuffer 객체 생성 channel.write(buffer); // 버퍼를 이용해서 파일 생성 channel.close(); } private void readFile(String fileName) throws IOException { FileChannel channel = new FileInputStream(fileName).getChannel(); // 파일을 읽기 위한 채널 얻어오기 ByteBuffer buffer = ByteBuffer.allocate(1024); channel.read(buffer); // 버퍼를 넘겨줌으로써 데이터를 해당 버퍼에 담게된다. buffer.flip(); // 버퍼에 담겨있는 데이터의 가장 앞으로 이동 while (buffer.hasRemaining()) { System.out.print((char) buffer.get()); } channel.close(); }}2. Channel 채널은 외부 세계와 통신하는 데 사용되는 스트림과 같다. 채널에서 버퍼로 데이터를 읽거나 버퍼에서 채널로 데이터를 쓸 수 있다. Java NIO는 non-blocking I/O 작업을 수행하며 이러한 I/O 작업에 채널을 사용할 수 있다. 서로 다른 엔티티에 대한 연결은 논블로킹 I/O 동작을 수행할 수 있는 다양한 채널로 표현된다. 채널은 중간 매체 또는 게이트웨이로 작동한다. 출처 : https://www.geeksforgeeks.org/introduction-to-java-nio-with-examples/ 3. Selector Selector를 통해 non-blocking I/O가 가능하다. Selector는 이벤트에 대해 여러 채널을 모니터링한다. 하나의 Selector를 사용해서 다수의 channels를 등록할 수 있다. 하나의 스레드를 사용해서 input을 처리할 수 있는 channel, writing을 위해 준비된 channel을 선택할 수 있다. 즉, Selector는 I/O 작업을 위해 준비된 채널을 선택하는 데 사용된다. 결과적으로, 하나의 스레드 여러 개의 channel을 쉽게 관리할 수 있게된다. 다수의 스레드로 IO를 관리하는 방식에 비해 스레드 간의 context switching을 줄여준다. 출처 : https://www.geeksforgeeks.org/introduction-to-java-nio-with-examples/ Java I/O는 내부적으로 어떻게 동작할까 ?Buffer Handling and Kernel vs User Space 출처 : https://howtodoinjava.com/java/io/how-java-io-works-internally/ 위의 이미지는 블록 데이터가 하드 디스크와 같은 외부 소스에서 실행 중인 프로세스 내부의 메모리 영역(예: RAM)으로 이동하는 방식에 대한 단순화된 ‘논리적’ 다이어그램을 나타낸다. 프로세스는 read() 시스템 호출을 통해 버퍼를 채우도록 요청한다. 사용자 프로세스는 User Space에서 동작하기 때문에 하드웨어에 직접적으로 접근할 수 없다. 따라서, OS에서 제공하는 시스템콜을 통해 I/O를 수행해야 한다. read() 시스템콜을 통해 커널에게 I/O 수행을 요청한다. 이 과정에서 유저 모드(User mode)에서 커널 모드(Kernal mode)로 스위칭이 발생한다. 제어권을 넘겨받은 커널은 우선 프로세스가 요청한 데이터가 이미 커널 영역 캐시 메모리에 존재하는지 확인한다. 만일 데이터가 캐시에 존재한다면 해당 데이터를 read() 함수 호출 시 전달받은 메모리 영역에 복사한 뒤 제어권을 다시 사용자 프로세스에게 넘긴다.(커널 모드 -&gt; 유저 모드로 스위칭) 데이터가 캐시에 존재하지 않는다면 디스크로부터 데이터를 가져오는 과정을 수행한다. 읽기 호출은 커널이 디스크 컨트롤러 하드웨어에 명령을 실행하여 디스크에서 데이터를 가져오도록 한다. 이는 CPU가 디스크보다 수백배는 빠르기 때문에, 디스크의 처리 시간을 기다리는 것이 낭비이기 때문이다. 디스크 컨트롤러는 CPU의 추가 지원 없이 DMA를 통해 데이터를 커널 메모리 버퍼에 직접 쓴다. 디스크 컨트롤러가 버퍼 채우기를 마치면 커널은 커널 공간의 임시 버퍼에서 프로세스가 지정한 버퍼로 데이터를 복사한다. read() 과정이 종료되면서, 사용자 프로세스는 Block 되어 있던 메서드가 완료되며 요청한 데이터를 사용할 수 있게 된다.Arguments 위에서 살펴봤듯이, Java I/O의 경우 커널 메모리를 직접 접근하는 것이 아닌 JVM에 데이터를 copy하는 작업 로직이 포함되어 있기 때문에 비효율적이다는 의견이 있다. CPU가 개입하여 커널 영역 메모리의 데이터를 사용자 영역으로 옮기며 오버헤드가 발생한다. 이 과정에서 생성된 Java의 객체들은 GC 대상이 된다. 일반적으로 DMA를 하게되면 CPU 자원사용 없이 직접적인 메모리 접근을 하기 때문에 CPU 오버헤드가 없으며 CPU 자원 점유가 없는 non-blocking 수행이 가능하다는 이점이 있다. java.nio 에서는 사용자 영역상에 Buffer를 만들어 사용하는것이 아닌 커널영역에 Buffer를 만들어 직접 DMA를 할 수 있도록 제공하고 있다. 따라서, java.nio를 사용하는것이 java.io를 사용하는 것 보다 일반적으로 성능적으로 뛰어나다고 알려져있다. 하지만, 항상 그런 것만은 아니다. 관련 글 cf) DMA(Direct memory access) DMA는 특정 하드웨어 하위 시스템이 CPU와 독립적으로 RAM에 액세스할 수 있도록 하는 기능이다. DMA가 없으면 CPU가 프로그래밍된 입출력을 사용할 때, 일반적으로 읽기 또는 쓰기 작업의 전체 시간 동안 완전히 사용되므로 다른 작업을 수행할 수 없다. 따라서, CPU는 먼저 DMA 전송을 시작하고, 전송이 진행되는 동안 다른 작업을 수행한다. 작업이 완료되면 DMA 컨트롤러로부터 인터럽트를 수신한다. 이 기능은 CPU가 데이터 전송 속도를 따라가지 못하거나 CPU가 상대적으로 느린 I/O 데이터 전송을 기다리는 동안 작업을 수행해야 할 때 유용하다.참고 자료 이상민, 『자바의 신 2』, 로드북(2017), 26,27장 https://www.baeldung.com/java-io-vs-nio http://tutorials.jenkov.com/java-nio/nio-vs-io.html#main-differences-between-java-nio-and-io https://www.geeksforgeeks.org/introduction-to-java-nio-with-examples/ https://howtodoinjava.com/java/io/how-java-io-works-internally/ https://taes-k.github.io/2021/01/06/java-nio/ https://leeyh0216.github.io/posts/java_nio_why_java_io_slow/ https://en.wikipedia.org/wiki/Direct_memory_access" }, { "title": "try-with-resource 사용시 Socket closed 예외 발생", "url": "/posts/try-with-resource-caution/", "categories": "경험하기, 이슈 노트", "tags": "Java, Socket", "date": "2021-09-05 14:00:00 +0900", "snippet": "상황자바로 만든 간단한 웹 서버에서 404, 500과 같은 에러 페이지 처리를 위해 catch 절 내부에 에러 페이지를 응답하는 로직을 작성했다.하지만 에러 발생시 에러 페이지가 응답되지 않고 java.net.SocketException: Socket closed가 발생했다.public class HttpServer { ... public ...", "content": "상황자바로 만든 간단한 웹 서버에서 404, 500과 같은 에러 페이지 처리를 위해 catch 절 내부에 에러 페이지를 응답하는 로직을 작성했다.하지만 에러 발생시 에러 페이지가 응답되지 않고 java.net.SocketException: Socket closed가 발생했다.public class HttpServer { ... public void start() throws IOException { ExecutorService pool = Executors.newFixedThreadPool(NUM_THREADS); try (ServerSocket server = new ServerSocket(port)) { Socket connection; while ((connection = server.accept()) != null) { Runnable r = new HttpPageProcessor(connection); pool.submit(r); } } }}public class HttpPageProcessor implements Runnable { @Override public void run() { try (InputStream inputStream = connection.getInputStream(); OutputStream outputStream = connection.getOutputStream()){ httpReq = HttpUtils.getHttpRequest(inputStream); httpResp = HttpUtils.getHttpResponse(outputStream); ... 요청에 따라 알맞은 html 페이지 응답 } catch (Exception e) { ... 에러 페이지 응답 } } ...}원인 원인은 try-with-resource 사용으로 인해 Socket이 자동으로 닫히기 떄문이다. 즉, catch절 내에서 에러 페이지 응답 로직을 수행하는데 필요한 소켓 커넥션이 이미 닫힌 상태이다. 근데 여기서 한가지 의문이 들었다. try() 내부에서 할당한 것은 InputStream과 OutputStream인데 왜 소켓도 닫히는거지 ? 해답은 각각의 구현체인 java.net.SocketInputStream, java.net.SocketOutputStream에 있는 close() 메서드를 보면 알 수 있다. close() 메서드 내부에서 소켓이 닫혔는지 여부를 확인하고 socket.close()를 호출하는 로직이 있다. try 내부에서 OutputStream이 더 늦게 할당되므로 SocketOutputStream.close() 메서드가 먼저 호출된다. private boolean closing = false; public void close() throws IOException { // Prevent recursion. See BugId 4484411 if (closing) return; closing = true; if (socket != null) { if (!socket.isClosed()) socket.close(); } else impl.close(); closing = false; } 해결 과정 try-catch-finally 사용 이렇게 해결하는게 맞는지는 모르겠으나, 일단 생각나는게 이것밖에 없어서 이렇게 해결해보았다. 추후에 더 나은 방법을 알게되면 업데이트 해야겠다.public class HttpPageProcessor implements Runnable { ... private void init() throws IOException { InputStream inputStream = connection.getInputStream(); OutputStream outputStream = connection.getOutputStream()); ... 객체 초기화 } @Override public void run() { try { init(); ... 요청에 따라 알맞은 html 페이지 응답 } catch (Exception e) { ... 에러 페이지 응답 } finally { try { connection.close(); } catch (IOException e) { logger.error(e.getMessage(), e); } } } ...}참고 자료 https://stackoverflow.com/questions/1388602/do-i-need-to-close-both-filereader-and-bufferedreader" }, { "title": "OOP - 객체 지향 언어의 특징(기능과 책임 분리)", "url": "/posts/function-and-responsibility/", "categories": "지식 더하기, 이론", "tags": "OOP", "date": "2021-09-04 00:29:00 +0900", "snippet": "해당 글은 최범균 님의 객체 지향 프로그래밍 입문 강의를 듣고 정리한 내용입니다.기능 기능은 하위 기능으로 분해 가능하다 각각의 기능을 누가 제공할 것인지 결정하는 것이 객체 지향 설계의 기본이다. 기능은 곧 책임이다. 따라서, 분리한 기능을 알맞게 분배해야한다.(책임 분리) 분리한 하위 기능...", "content": "해당 글은 최범균 님의 객체 지향 프로그래밍 입문 강의를 듣고 정리한 내용입니다.기능 기능은 하위 기능으로 분해 가능하다 각각의 기능을 누가 제공할 것인지 결정하는 것이 객체 지향 설계의 기본이다. 기능은 곧 책임이다. 따라서, 분리한 기능을 알맞게 분배해야한다.(책임 분리) 분리한 하위 기능을 활용해서 전체 기능을 완성한다. public class ChangePasswordService { public Result changePassword(...) { Member findMem = memberRepository.findById(id); if(findMem==null) ... }} 기능을 분리하지 않으면 ? 클래스나 메서드가 커지면 절차 지향의 문제가 발생한다. 큰 클래스의 경우 많은 필드를 많은 메서드가 공유하게 된다. 큰 메서드의 경우 많은 변수를 많은 코드가 공유하게 된다. 이런 경우, 여러 기능이 한 클래스/메서드에 섞여 있을 가능성이 높다. 따라서, 책임에 따라 코드를 적절하게 분리해야 한다.책임 분배/분리 방법 살펴보기 크게 ‘패턴 적용’, ‘계산 기능 분리’, ‘외부 연동 분리’, ‘조건별 분기 추상화’ 관점에서 분리해 볼 수 있다.1. 패턴 적용(전형적인 역할 분리) 간단한 웹 - 컨트롤러, 서비스, DAO 복잡한 도메인 - 엔티티, VO, 리파지토리, 도메인 서비스 AOP - Aspect(공통 기능) 디자인 패턴 - 팩토리, 빌더, 전략, 템플릿 메서드 등2. 계산 기능 분리BeforeMember mem = memberRepository.findOne(id);Product prod = productRepository.findOne(prodId);int payAmount = prod.getPrice() * orderReq.getAmount();double pointRate = 0.01;if(mem.getMembership() == GOLD) { 계산 ...} else if(mem.getMembership() == SILVER) { 계산 ...}...After 포인트 계산에 대한 책임은 PointCalculator에게 맡긴다. 이를 통해, 포인트 계산에 대한 부분만 따로 테스트 할 수 있게된다.즉, 역할 분리가 잘 되면 테스트가 용이해지는 장점도 얻을 수 있다.Member mem = memberRepository.findOne(id);Product prod = productRepository.findOne(prodId);int payAmount = prod.getPrice() * orderReq.getAmount();PointCalculator cal = new PointCalculator(...);int point = cal.calculate();...public class PointCalculator { ... public int calculate() { if(membership == GOLD) { ... } ... }}3. 외부 연동 분리 네트워크, 메시징, 파일 등에 대한 연동 코드를 분리한다.BeforeProduct prod = findOne(id);RestTemplate rest = new RestTemplate();List&lt;RecommendItem&gt; recoItems = rest.get(\"http:// ~~?prodId=\"+ prod.getId());After 의도가 잘 드러나는 이름을 사용하도록 신경쓰자. 위와 같은 상황에서는 HttpDataService 보다 RecommendService가 더 직관적일 것이다.Product prod = findOne(id);RecommendService recoService = new RecommendService();List&lt;RecommendItem&gt; recoItems = recoService.getRecommendItems(prod);4. 조건별 분기 추상화 조건별 분기가 산재해 있는 경우, 추상화 할 수 있는(공통적으로 묶을 수 있는) 부분이 있는지 살펴본다.BeforeString fileUrl = \"\";if(fildId.startWith(...)) { fileUrl = ...;} else if(fileId.startWith(...)) { fileUrl = ...;} else if(...) { fileUrl = ...;}After 공통적으로 계속 나타나는 url을 제공하는 부분을 묶는다.String fileUrl = FileInfo.getUrl(fileId);public class FileInfo { public static String getUrl(fileId) { if(fileId.startWith(...)) { ... } ... }}" }, { "title": "OOP - 객체 지향 언어의 특징(다형성과 추상화)", "url": "/posts/abstraction/", "categories": "지식 더하기, 이론", "tags": "OOP", "date": "2021-09-01 00:29:00 +0900", "snippet": "해당 글은 최범균 님의 객체 지향 프로그래밍 입문 강의를 듣고 정리한 내용입니다.다형성과 추상화다형성 (polymorphism) 여러(poly) 모습(morph)을 갖는 것. 즉, 한 객체가 여러 타입을 갖는 것을 의미한다. 즉, 한 객체가 여러 타입의 기능을 제공 타입 상속으로 다형성 구현 하위 타입은 상위 타입도 됨 ...", "content": "해당 글은 최범균 님의 객체 지향 프로그래밍 입문 강의를 듣고 정리한 내용입니다.다형성과 추상화다형성 (polymorphism) 여러(poly) 모습(morph)을 갖는 것. 즉, 한 객체가 여러 타입을 갖는 것을 의미한다. 즉, 한 객체가 여러 타입의 기능을 제공 타입 상속으로 다형성 구현 하위 타입은 상위 타입도 됨 아래와 같은 관계가 있다고 가정했을 때, IotTimer는 Timer, Rechargeable 타입 모두로 선언될 수 있다. 즉, 다양한 타입을 갖는다. public class IotTimer extends Timer implements Rechargeable { ...}// 다형성IotTimer it = new IotTimer();Timer t = it;Rechargeable r = it;추상화(Abstraction) 데이터나 프로세스 등을 의미가 비슷한 개념이나 의미 있는 표현으로 정의하는 과정 두 가지 방식의 추상화 특정한 성질 공통 성질(일반화) 추상화의 간단한 예 아이디, 이름, 메일 → DB의 USER 테이블 통화, 금액 → Money HP MXX, 삼성 SL-M2XX → 프린터 지포스 GX - XX, 라데온 RD- XX → GPU SCP로 파일 업로드, HTTP로 데이터 전송, DB 테이블에 삽입 → 푸시 발송 요청 공통 성질을 뽑아내는 추상화를 통해 다형성이 실현된다.타입 추상화 여러 구현 클래스를 대표하는 상위 타입 도출 흔히 인터페이스 타입으로 추상화 추상화 타입과 구현은 타입 상속으로 연결 상위타입(인터페이스)인 Notifier는 기능에 대한 의미를 제공한다. 즉, 구현은 제공하지 않고 하위 클래스(concrete 클래스)에게 맡긴다. 추상 타입 추상 타입은 구현을 감춘다. 즉, 기능의 구현이 아닌 의도를 더 잘 드러낸다. 추상 타입을 사용하지 않는 경우 요구사항의 변경으로 인해 그와 관련없는 코드가 변경될 수 있다. 예를 들어, 아래와 같은 경우 주문 취소 자체와는 크게 상관 없는 요구 사항 변경(취소시 이메일 전송, sms전송 등)으로 인해 본질적인 취소 메서드의 코드가 변경된다. 최초 요구사항 (주문 취소시 sms 발송)private SmsSender smsSender;public void cancel(String orderNo) { ... 주문 취소 처리 smsSender.sendSms(...);} 요구사항 변경 (카카오 알림, 메일 알림 추가)private SmsSender smsSender;private KakaoPush kakaoPush;private MailService mailSvc;public void cancel(String orderNo) { ... 주문 취소 처리 if(pushEnabled) { kakaoPush.push(...); } else { smsSender.sendSms(...); } mailSvc.sendMail(...);} 이런 경우 추상 타입을 활용해 유연함을 제공할 수 있다. 아래 예시는 Notifier의 콘크리트 클래스를 생성하는 부분까지 NotifierFactory라는 인터페이스를 사용해 추상화 시켰다. public void cancel(String orderNo) { ... 주문 취소 처리 Notifier notifier = NotifierFactory.instance().getNotifier(...); notifier.notify(...);}public interface NotifierFactory { Notifier getNotifier(...); static NotifierFactory instance() { return new DefaultNotifierFactory(); }}public class DefaultNotifierFactory implements NotifierFactory { public Notifier getNotifier(...) { if(pushEnabled) return new KakaoNotifier(); else return new SmsNotifier(); }}추상화의 시점 아직 존재하지 않는 기능에 대한 이른 추상화는 주의해야 한다. 추상화 → 추상 타입 증가 → 복잡도 증가 따라서, 실제 변경 및 확장이 발생할 떄 추상화를 시도하는게 좋다.※ 추상화 팁 : 구현을 한 이유가 무엇 때문인지 잘 생각해보고 상위 타입을 도출한다.느낀점 캡슐화나 추상화의 본질은 결국 ‘구체적인 구현을 클라이언트(호출하는 부분)에게 숨긴다’인 것 같다. 캡슐화 : 객체가 제공하는 기능의 내부 구현을 숨긴다. 클라이언트는 자신이 사용하는 특정 객체의 기능에 대한 내부 구현은 알지못하고 해당 기능을 사용하기만 한다. 따라서, 해당 객체의 기능의 내부 구현이 변경되더라도 클라이언트 코드는 변경되지 않는다. 추상화 : 클라이언트가 사용하게될 구체 클래스(콘크리트 클래스)를 숨긴다. 클라이언트가 인터페이스에 의존하며 구체 클래스는 외부에서 가져옴으로써 런타임시 사용하게될 객체에 대해 알 수 없다. 따라서, 구체 클래스가 변경되더라도 클라이언트 코드는 변경되지 않는다. 실제로 적용해보기 추상화를 통해 엑셀 생성 로직 리팩토링하기연관 포스팅-객체 지향 언어의 특징 - 캡슐화" }, { "title": "OOP - 객체 지향 언어의 특징(캡슐화)", "url": "/posts/encapsulation/", "categories": "지식 더하기, 이론", "tags": "OOP", "date": "2021-08-31 00:29:00 +0900", "snippet": "해당 글은 최범균 님의 객체 지향 프로그래밍 입문 강의를 듣고 정리한 내용입니다.캡슐화 데이터 + 관련된 기능을 묶는 것 객체가 기능을 어떻게 구현했는지 외부에 감추는 것 구현에 사용된 데이터의 상세 내용을 외부에 감춤 정보 은닉의미 포함 캡슐화를 하지 않는 경우 요구사항의 변경이 데이터 구조/사용에 변화를 발생...", "content": "해당 글은 최범균 님의 객체 지향 프로그래밍 입문 강의를 듣고 정리한 내용입니다.캡슐화 데이터 + 관련된 기능을 묶는 것 객체가 기능을 어떻게 구현했는지 외부에 감추는 것 구현에 사용된 데이터의 상세 내용을 외부에 감춤 정보 은닉의미 포함 캡슐화를 하지 않는 경우 요구사항의 변경이 데이터 구조/사용에 변화를 발생시킴 예를 들어, 특정 회원이 정회원인지 체크해서 특정 기능을 제공하는 다음과 같은 코드가 있다고 가정해보자.if(account.getMembership() == REGULAR &amp;&amp; account.getExpDate().isAfter(now())) { ... 정회원 기능} 만약 해당 기능을 5년 이상 사용자에게도 제공해달라는 요구사항이 추가된다면 조건문에 또 다른 조건이 추가되어야한다. if(account.getMembership() == REGULAR &amp;&amp; ( account.getServiceDate.isAfter(fiveYearsAgo) &amp;&amp; account.getExpDate().isAfter(now()))) { ... 정회원 기능} 결과적으로, account에 대한 요구사항의 변화로 인해 기존에 위와 같이 선언되어 있던 모든 부분에서 코드 수정이 발생한다.캡슐화를 하게되면 ? 내부 구현을 감춤으로써 외부의 영향(변경)을 최소화 하면서 객체 내부 구현 변경이 가능하다. 즉, 이를 통해 OCP를 지킨 설계가 가능하다. 다시 위 예제로 예를 들면 아래와 같이 변경할 수 있다. 즉, 기능을 제공하고 구현 상세를 감춘다. if(account.hasRegularPermission()) { ... 정회원 기능} 만약 정회원 관련 요구사항이 변경되더라도 Account의 hasRegularPermission() 메서드 내부만 변경하면 된다. hasRegularPermission() 메서드를 호출하는 부분에서는 변경될게 없다. (OCP) public class Account { private Membership membership; private Date expDate; public boolean hasRegularPermission() { return account.getMembership() == REGULAR &amp;&amp; expDate.isAfter(now()); }} 또한 캡슐화를 통해 기능에 대한 의도를 명확하게 나타낼 수 있다. 즉, account.getMembership() == REGULAR &amp;&amp; account.getExpDate().isAfter(now()) 이런식으로 단순히 조건만 나열하는 것 보다는 hasRegularPermission() 메서드를 통해 ‘정회원 권한이 있는지 확인한다’라는 의도를 훨씬 명확하게 드러낼 수 있다. 캡슐화를 위한 규칙 “Tell, Don’t Ask” : 데이터를 달라 하지 말고 해달라고 하기 즉, 사용하는 입장에서 데이터를 가져와서 무언가 하려하지 말고, 데이터를 갖고있는 객체에게 해당 데이터로 무언가 해달라고 요청해라. account의 membership 데이터를 가져와서 내가 검증한다. (X)if (account.getMembership() == REGULAR) { ... 정회원 기능} account 객체에게 맡긴다. (O)if (account.hasRegularPermission()) { ... 정회원 기능} Demeter’s Law 메서드에서 생성한 객체의 메서드만 호출 파라미터로 받은 객체의 메서드만 호출 필드로 참조하는 객체의 메서드만 호출연관 포스팅-객체 지향 언어의 특징 - 다형성과 추상화" }, { "title": "MySQL - 데이터 모델링", "url": "/posts/data-modeling/", "categories": "지식 더하기, 이론", "tags": "MySQL", "date": "2021-08-25 23:00:00 +0900", "snippet": "들어가기 전애플리케이션을 설계할 때 ‘DB 테이블은 어떻게 구성해야하지?’ 에 대한 막막함이 있다면 데이터 모델링이 문제 해결의 출발점이라고 생각한다. 공부를 하기에 앞서 나의 언어로 정리해본다면 내가 생각하기에 모델링이라는 것은 먼저 문자 그대로 ‘모델화’ 시킨다는 것인데, 모델이란 ‘어떤 대상을 필요한 특징만 추려서 나타낸 것’ 이라는 생각이 든다...", "content": "들어가기 전애플리케이션을 설계할 때 ‘DB 테이블은 어떻게 구성해야하지?’ 에 대한 막막함이 있다면 데이터 모델링이 문제 해결의 출발점이라고 생각한다. 공부를 하기에 앞서 나의 언어로 정리해본다면 내가 생각하기에 모델링이라는 것은 먼저 문자 그대로 ‘모델화’ 시킨다는 것인데, 모델이란 ‘어떤 대상을 필요한 특징만 추려서 나타낸 것’ 이라는 생각이 든다. 예를 들어, ‘아파트 단지 모형’ 이라고 한다면 아파트, 부대시설 등을 나타낼 수 있을 것이며 이 때, 아파트 창문의 개수, 주차장의 주차 가능 대수, 내부 엘리베이터 등은 불필요한 정보일 것이다.내가 생각하는 모델링의 장점으로는 현실의 복잡성을 추상화를 통해 단순화 할 수 있다는 것이다. 단순화하는게 어떤 장점이 있을지 데이터베이스 관점에서 생각해보면 최소한의 자원을 활용하여 대상을 저장할 수 있다는 점이 있을 것 같다.데이터 모델링이란 ?데이터 모델링(data modeling)이란 주어진 개념으로부터 논리적인 데이터 모델을 구성하는 작업을 말하며, 일반적으로 이를 물리적인 데이터베이스 모델로 환원하여 고객의 요구에 따라 특정 정보 시스템의 데이터베이스에 반영하는 작업을 포함한다. 후자의 의미로 흔히 ‘데이터베이스 모델링’으로 불리기도 한다. 출처 : https://www.trifacta.com/blog/what-is-data-modeling/ 데이터 모델링의 필요성 이 글 에 나와있는 내용 중 일부를 정리해보았다. ‘아 이래서 모델링을 해야하는구나’ 정도의 느낌만 가져가면 될 것 같다. 애플리케이션의 더 나은 품질 평균적으로 소프트웨어 개발 작업의 약 70%가 실패하며, 주요 실패 원인은 섣부르게 코딩을 시작하는 것 때문이다. 데이터 모델은 문제를 정의하는 데 도움이 되므로, 실제 코드를 작성하기 전에 이를 통해 여러 가지 문제 접근 방법을 고려해볼 수 있다. 비용 절감(오류 조기 포착) 데이터 모델링은 오류와 감시를 수정하기 쉬운 시기에 조기에 포착한다. 이는 애플리케이션이 만들어진 후 오류를 수정하는 것 보다 비용이 적게 든다. 소통의 도구 데이터 모델을 통해 개발자 뿐 아니라 기획자 등 다양한 이해관계자들 간에 원만한 소통이 가능해진다. 이를 통해, 애플리케이션에 포함된 것과 생략된 것이 무엇인지 대해 서로 간에 동의하는 데 도움을 준다. 더 나은 성능 최적의 성능을 얻으려면 데이터 모델의 개념이 명확하고 일관성이 있어야 한다. 또한, 모델을 데이터베이스 설계로 변환하는 데 적절한 규칙을 사용해야 한다. 모델링은 빠른 성능을 위해 데이터베이스를 조정할 수 있도록 데이터베이스를 이해하는 수단을 제공한다. 리스크 관리 데이터 모델의 크기와 테이블 간의 연결 강도 등을 바탕으로, 소프트웨어의 복잡성을 추정하고 개발 작업 및 프로젝트 위험 수준에 대한 통찰력을 얻을 수 있다. 데이터 모델링을 위한 용어 모델링을 학습하기에 앞서 필요한 용어에 대해 공부해보자Entity(엔터티) 일반적으로 ‘테이블’이라고 칭하는 개체이다. 하지만 항상 테이블과 1:1로 맵핑되는 것은 아니며, 2개 이상의 엔터티가 물리 모델링 단계에서 통합되기도 하고, 하나의 엔터티가 여러 개의 물리적 테이블로 구현되기도 한다. 엔터티를 도출할 때 가장 중요한 것은, 해당 용어가 의미하는 범위가 어디까지인지를 명확히 하고 그에 걸맞는 이름을 부여하는 것이다. 이를 기반으로 속성, 식별자, 엔터티 간의 관계가 명확해진다. 엔터티의 작명 만약 엔터티 이름에 수식어가 있다면 검토해보고 필요하다면 통합하는 것이 좋다. 예를 들어, ‘상품’을 ‘직원용상품’, ‘고객용상품’ 등으로 범위를 제한해서 여러 개의 엔터티를 정의하는 것은 좋지 않다. ‘리스트, ‘목록’과 같은 복수형 표현을 지양한다. 이미 엔터티 자체가 레코드의 목록을 저장하는 개체이기 때문이다. ‘사원정보’와 같은 모호한 단어보다는 ‘사원’이라는 좀 더 명확하고 간결한 범위를 한정하는 것이 좋다.Attribute(속성) 더는 분리될 수 없는 최소의 데이터 보관 단위.(테이블의 컬럼과 맵핑된다.) 즉, 하나의 엔터티 내에서 다른 어트리뷰트와 비교했을 때, 독자적인 성질을 가지는 것이어야 한다. 가공하지 않은 그대로의 값이라는 의미도 내포하고 있다. 가공하지 않은 것의 반대 의미로 ‘추출 칼럼’이 있다. 추출 칼럼은 하나의 엔터티나 다른 엔터티의 어트리뷰트로부터 계산된 값이다. 예를 들어 게시물의 코멘트 개수, 게시판에 등록된 게시물의 개수 등이 있다. Attribute의 원자성 어트리뷰트는 반드시 독자적인 성질을 가지는 하나의 값만을 저장해야 한다. 예를 들어 회원의 취미 정보를 하나의 어트리뷰트에 구분자를 사용해서 한꺼번에 저장할 때도 있다. 하지만 이러한 방법은 어트리뷰트의 기본 조건에 위배되는 모델링 방법이며, 추후 물리 모델링 단계나 인덱스 설계에 나쁜 영향을 미칠 수 있다. 물리 모델 단계에서는 성능을 위해 어느정도 위배해서 설계할 수도 있지만, 논리 모델에서는 원자성을 위배하는 어트리뷰트는 고려하지 않는게 좋다.식별자(Primary Key) 하나의 엔티티에서 개별 레코드를 식별할 수 있는 어트리뷰트의 조합 일반적으로 식별자로 어트리뷰트 하나를 가질 때가 많으며, 두 개 이상의 어트리뷰트가 조합되는 경우도 있다.인조 식별자(Surrogate Key) 예를 들어 고객, 상품 엔터티를 기반으로 주문 엔터티를 만든다고 가정해보자. 주문 엔터티의 식별자는 (고객ID, 상품코드, 주문일시)로 구성할 수 있을 것이다. 하지만 주문 엔터티는 주문 이력, 상태 변화 등과 같은 수많은 자식 엔터티를 만들어낼 가능성이 높다. 주문 엔터티의 식별자가 3개의 어트리뷰트로 구성되기 때문에 자식 엔터티는 그 이상의 어트리뷰트를 사용해야 할 수도 있다. 따라서 ‘주문번호’와 같은 인위적인 숫자 값을 식별자로 대체해서 사용할 때가 많다. 인조 식별자를 도입한다면 본질 식별자(위의 경우 고객ID, 상품코드, 주문일시)는 대체키(유니크 인덱스)로 생성한다.슈퍼 키(Super Key) 한 릴레이션 내에 있는 속성들의 집합으로 구성된 키. 릴레이션을 구성하는 모든 튜플 중 슈퍼키로 구성된 속성의 집합과 동일한 값은 나타내지 않는다. 릴레이션을 구성하는 모든 튜플에 대해 유일성(Unique)은 만족하지만, 최소성(Minimality)은 만족하지 못한다.후보 키(Candidate Key) 엔터티를 구성하는 어트리뷰트들 중에서 레코드를 유일하게 식별하기 위해 사용되는 어트리뷰트들의 부분 집합. 유일성과 최소성을 모두 만족. 유일성 : 하나의 키 값으로 하나의 튜플만을 유일하게 식별할 수 있어야한다. 최소성 : 키를 구성하는 속성 하나를 제거하면 유일하게 식별할 수 없도록 꼭 필요한 최소의 속성으로 구성되어야 한다. 2개 이상의 어트리뷰트를 조합하여 만든 후보 키를 복합 키(Composite Key)라고 한다.대체 키(Alternate Key) 후보 키 중에서 기본 키를 제외한 나머지를 의미한다.외래 키(Foreign Key) 다른 엔터티의 기본 키를 참조하는 속성 또는 속성들의 집합을 의미한다. 엔터티 간의 관계를 표현할 때 사용한다. 즉, 외래키는 두 엔터티를 서로 연결하는 데 사용되는 키이다. 외래키가 포함된 테이블을 자식 테이블이라고 하고 외래키 값을 제공하는 테이블을 부모 테이블이라고 한다. 부모 엔터티의 기본 키와 동일한 키 속성을 가진다. 부모 테이블의 기본키, 고유키가 여러개의 컬럼으로 이루어져 있다면 부모가 가진 기본키, 고유키 컬럼을 원하는 개수만큼 묶어서 외래키로 지정할 수 있다.참고. Primary Key vs Unique Key   Primary Key Unique Key 용도 테이블의 각 행에 대한 고유 식별자 역할 기본 키가 아닌 것 중 행을 유일하게 식별하는 역할 NULL 허용 여부 X O 개수 오직 한 개 한 개 이상 인덱스 clustered index non-clustered index Relation(관계) 엔터티 간의 상호작용을 표현한 것 관계는 다른 엔터티의 어트리뷰트로 참여하기도 하지만 관계 자체가 별도의 엔터티로 구현돼야 할 때도 많다.식별, 비식별 관계 식별 관계 부모의 식별자가 자식 엔터티의 레코드를 식별하는데 꼭 필요한 관계 게시판과 게시물의 관계를 보면, 게시물의 경우 부모 엔터티인 게시판의 식별자 게시판ID가 게시물의 식별자로 반드시 포함되어야 한다. 비식별 관계 부모 엔터티의 식별자가 없어도 자식 엔터티의 레코드가 생성 가능한 관계 회원과 게시물의 관계를 보면, 게시물의 경우 게시물ID만 있으면 레코드 생성이 가능하고 부모 엔터티의 식별자인 회원ID는 외래키로 구성할 수 있다. Cardinality(기수성) 부모 엔터티의 레코드 하나에 대해 자식 엔터티의 레코드가 얼마나 만들어질 수 있는지(발생 빈도)를 의미한다. 주로 0 또는 1, 1건 이상(N 또는 M)의 수준으로 구분해서 표시한다. 출처 : https://www.lucidchart.com/pages/ER-diagram-symbols-and-meaning 다대다(M:M) 관계 어트리뷰트는 하나 이상의 값을 가지지 못하므로 M:M 관계를 1:M과 같이 어트리뷰트로 표현할 수는 없다. 논리 모델에서는 M:M 관계를 표기하기도 하지만, 물리 모델에서는 M:M 관계를 위한 표기법이 존재하지 않는다. M:M 관계는 물리 모델로 넘어 오면서 다음과 같이 두 개의 1:M 관계로 풀어줘야한다. 이를 ‘M:M 관계 해소’라고 한다. 꼭 물리 모델이 아니고 논리 모델에서 선행될 수도 있다. ‘수강’은 어떠한 엔터티가 아니라 학생과 과목 간의 ‘관계’를 나타내는 것이다. 하지만 RDB의 구조적 한계로 이를 수강이라는 엔터티로 변환한 것이다. 이처럼 관계를 저장하는 엔터티를 관계 엔터티(테이블)라고도 한다. 데이터 모델링 방법 일반적으로 업무 요건 정의 → 개념 모델링 → 논리 모델링 → 물리 모델링 순서로 진행된다.개념 모델링 정의된 업무 요건을 기반으로 다음을 도출한다. Entity Attribute 식별자(Identifier), 지정 후보키(candidate key), 기본키(primary key), 대체키(alternate key), 중복키(composite key) 식별자가 될만한 어트리뷰트가 없다면 인조키 생성 Entity간의 관계 정의(PK와 FK 연결, Cardinality &amp; Optionality) 각 엔터티 간의 관계를 최대한 간결하게 표현해야 한다. ERD(E-R Diagram)의 생명은 가독성이다. 출처 : https://bitnine.tistory.com/446 논리 모델링 개념적 모델링 된 것을 바탕으로 관계형 데이터베이스 패러다임에 어울리게 데이터를 정리하는 것 ERD뿐만 아니라 시스템 구축을 위한 사항을 모두 정의한다. 또한, 정규화를 통해 논리 데이터 모델을 상세화하여 일관성을 확보하고 중복을 제거한다. 개념 모델 논리 모델 Entity Table Attribute Column Relation PK, FK 출처 : https://bitnine.tistory.com/446 물리 모델링 논리적 모델링에서 도출된 이상적인 표를 실제 RDBMS에 맞게 변환하는 작업을 수행한다. M:M 관계 같이 RDB에서 구현할 수 없는 구조를 해소하는 작업 프라이머리 키의 컬럼 순서 선정 컬럼의 이름 부여 컬럼의 데이터 타입 이 단계에서 중요한 것은 ‘성능’이다. 슬로우 쿼리 찾기, 인덱스, 반정규화 등 프라이머리 키 순서 물리 모델에서는 프라이머리 키를 구성하는 컬럼의 순서가 매우 중요하다. 프라이머리 키도 하나의 인덱스로써 사용되므로 반드시 SELECT의 조건 절에 자주 사용되는 컬럼 위주로 순서를 배치해야 한다.데이터 타입 선정 문자, 숫자, 날짜, 이진 데이터 등 여러 형태를 가질 수 있는 데이터의 타입은 어떻게 결정해야 할까? 대표적인 예로 IP 주소를 생각해볼 수 있다. IP 주소 정보는 컴퓨터 내부적으로는 숫자(부호 없는 4바이트 정수)로 처리하지만 일반적으로는 4개의 숫자 영역으로 구분된 문자열로 통용되고 있다. 255.255.255.255 ⟷ 0xffffffff ⟷ 11111111 11111111 11111111 11111111 따라서, IP 주소를 저장하기 위한 컬럼의 데이터 타입을 문자 타입으로 할지 숫자 타입으로 할지 고민하게 된다. 편의성과 성능, 레코드 건수 등을 따져서 적절한 방법을 선택해야 한다. 또한, 업무적인 용도를 분석하여 장단점을 조율한 후에 사용해야 한다. 만약 4바이트 정수로 IP 주소를 저장한다고 했을 때 다음과 같은 트레이드 오프를 고려해 볼 수 있다. 장점 컬럼의 길이가 15 글자에서 4바이트로 줄어듦 컬럼의 길이 축소로 성능 향상이 기대됨 IP 주소를 A,B,C 등과 같은 대역별로 검색 가능 단점 값을 저장하거나 조회할 때 INET_NTOA() 또는 INET_ATON() 함수의 도움이 필요 단순한 문자열 패턴 검색(LIKE)을 사용할 수 없음 문자집합(캐릭터 셋) 문자집합에 따라 저장 공간의 길이가 2~3배씩 늘어날 수도 있고, 정렬이나 검색 규칙도 바뀔 수 있다. 하나의 DB에서 문자집합을 혼용하지 않는 것이 좋다는 의견과 명확한 기준만 있다면 2개 정도는 혼용해도 무방하다는 의견도 있다. 제대로 문자집합 관리가 되지 않는다면 쿼리의 성능만 떨어뜨리게 될 수도 있다. MySQL에서는 정렬이나 그룹핑과 같은 임시 테이블 또는 버퍼 작업을 위해 별도의 메모리 할당이 필요하다. 이때 MySQL 서버는 데이터 타입에 명시된 길이를 기준으로 메모리 공간을 할당하고 사용한다. 만약 해당 메모리 공간이 일정 크기 이상을 초과하면 메모리가 아닌 디스크에서 처리하게 된다. 즉, 컬럼이 과도하게 크게 설정되면 메모리로 처리할 수 있는 작업이 디스크에서 처리될 가능성이 높아진다. NULL과 NOT NULL NULL과 NOT NULL의 선택은 옵티마이저가 얼마나 쿼리를 더 최적화할 수 있게 환경을 만들어줄 것이냐의 관점에서 고려해야 한다. 예를 들어, NULL이 저장될 수 있는 컬럼에 대해 IN 형태의 조건을 사용하면 MySQL은 상상하지 못했던 이상한 비교 작업을 내부적으로 하게 된다.정규화(Normalization) 정제되지 않은 데이터를 관계형 데이터베이스에 어울리는 데이터로 만드는 작업. 즉, 데이터의 중복을 최소화하여 효율적인 모델을 만든다.논리 모델링에서의 정규화는 데이터 저장 비용을 최소화하고, 물리 모델링에서의 반정규화는 데이터를 읽어 오는 비용을 최소화한다.일반적으로는 3NF(제 3정규화)까지 사용한다.제 1정규화(No Repeating Group) 모든 속성은 반드시 하나의 값을 가져야 한다.제 2정규화(Whole Key Dependent) 데이터의 중복 방지를 위해 부분적으로 종속되는 컬럼들을 모으고 전체적으로 종속되는 컬럼은 나눈다.제 3정규화(Non-Key Independent) 식별자 이외의 속성간에 종속 관계가 존재하면 안된다.반정규화(Denormalization) 모델을 정규화할수록 SELECT 쿼리에서 필요한 테이블의 수 뿐만 아니라 GROUP BY나 쿼리 자체의 개수도 증가한다.따라서, 필요한 경우 반정규화를 통해 데이터를 읽어 오는 비용을 최소화하는 것을 고려해봐야 한다.반정규화는 정규화처럼 엄격하게 정해진 규칙들이 있는 것은 아니며,반정규화 시 얻는 것(성능적 이점)과 잃는 것(데이터 중복 등)이 분명하므로 trade-off를 잘 따져야한다.1. JOIN 줄이기 이 과정을 거치면 결국 정규화 되기 전 상태(데이터 중복)가 되는 것이다. 즉, 성능적으로 개선할 수 있는 방법이 이것밖에 없을 때 사용하는 최후의 수단 2. 계산 작업 줄이기 아래 예의 경우 GROUP BY를 사용하지 않아도 저자가 몇 개의 topic을 작성했는지 알 수 있다.3. 컬럼을 기준으로 테이블 분리하기 아래 예의 경우 description의 크기가 매우 크고 descripton을 따로 조회하는 경우도 많다면 테이블을 분리하는 것을 고려해 볼 수 있다.4. 행을 기준으로 테이블 분리하기 각각의 행을 다른 DB 서버에 분산하여 저장할 수 있다. 전체적인 데이터의 관리가 매우 어려울 수 있다.참고 자료 https://www.linkedin.com/pulse/why-data-modelling-important-munish-goswami 생활코딩 - 관계형 데이터 모델링 https://bitnine.tistory.com/446 https://moonibot.tistory.com/61" }, { "title": "MySQL - 트랜잭션과 격리 수준", "url": "/posts/transaction-and-isolation-level/", "categories": "지식 더하기, 이론", "tags": "MySQL", "date": "2021-08-24 23:00:00 +0900", "snippet": "트랜잭션과 ACID 트랜잭션이란 ? 데이터베이스 상태를 변경시키는 일련의 연산들을 논리적으로 묶을 수 있는 하나의 작업 단위 ex) 주문 - 결제 - 결제 승인 - 주문 접수 트랜잭션에는 중요한 두 가지 목적이 있다. 장애로부터 올바르게 복...", "content": "트랜잭션과 ACID 트랜잭션이란 ? 데이터베이스 상태를 변경시키는 일련의 연산들을 논리적으로 묶을 수 있는 하나의 작업 단위 ex) 주문 - 결제 - 결제 승인 - 주문 접수 트랜잭션에는 중요한 두 가지 목적이 있다. 장애로부터 올바르게 복구하고 데이터베이스를 일관되게 유지할 수 있는 안정적인 작업 단위를 제공 동시에 데이터베이스에 액세스하는 프로그램 간의 격리 제공 ACID DBMS는 각각의 트랜잭션에 대해 다음 4가지 특징을 보장한다.트랜잭션의 이러한 특징은 데이터 정합성을 보장해준다. Atomicity(원자성) 트랜잭션이 한 번에 이루어지거나 전혀 발생하지 않는다. 따라서, 변경이 부분적으로 발생하지 않는다. 다음 두 가지 작업을 포함한다. 중단(abort): 트랜잭션이 중단되면 데이터베이스의 변경 내용이 반영되지 않는다. 커밋(commit): 트랜잭션이 커밋하면 변경 내용이 반영된다. 원자성은 ‘All or nothing rule’이라고도 한다. Consistency(일관성) 트랜잭션 전후에 데이터베이스가 일관되도록 무결성(데이터의 일관성, 유효성)이 유지되어야 한다. 예를 들어 A가 B에게 송금하는 상황이라면, 거래 전후의 두 사람이 갖고 있는 금액의 총합은 동일해야 한다. Isolation(고립성) 여러 트랜잭션은 서로 간의 간섭 없이 독립적으로 발생해야 한다. 즉, 특정 트랜잭션에서 발생하는 변경 사항은 해당 트랜잭션의 특정 변경 사항이 메모리에 기록되거나 커밋될 때까지 다른 트랜잭션에서 볼 수 없다. 이로 인해, 데이터베이스 상태의 불일치 없이 여러 트랜잭션이 동시에 발생할 수 있다. Durability(영구성) 트랜잭션 실행이 완료되면 데이터베이스에 대한 업데이트 및 수정 내용이 디스크에 저장되고 시스템에 오류가 발생하더라도 유지된다. 이러한 업데이트는 영구적이며 비휘발성 메모리에 저장된다. 트랜잭션 격리 수준(Isolation Level)이란 ? 동시에 여러 트랜잭션이 처리될 때, 특정 트랜잭션이 다른 트랜잭션에서 변경하거나 조회하는 데이터를 볼 수 있도록 허용할지 말지 결정하는 것. 격리 수준은 크게 4가지로 나뉜다 (밑으로 갈수록 격리 수준 높아짐) READ UNCOMMITTED(DIRTY READ) READ COMMITTED REPEATABLE READ SERIALIZABLE 격리 수준이 높아질수록 동시성이 떨어지는 것이 일반적이다. 하지만, SERIALIZABLE 격리 수준이 아니라면 크게 성능의 개선이나 저하는 발생하지 않는다. READ UNCOMMITTED 커밋되지 않은 변경에 대해 조회가 가능하다. 특정 트랜잭션에서 처리한 작업이 완료되지 않았는데 다른 트랜잭션에서 볼 수 있게 되는 현상을 ‘Dirty Read’라고 한다. 이로 인해 데이터가 나타났다 사라졌다 하는 현상이 발생할 수 있다. RDBMS 표준에서는 트랜잭션의 격리 수준으로 인정하지 않을 정도로 데이터 정합성에 문제가 많은 격리 수준이다. 출처 : https://lng1982.tistory.com/287 READ COMMITTED 특정 트랜잭션에서 커밋 완료된 데이터만 다른 트랜잭션에서 조회할 수 있다. Oracle DBMS에서 기본적으로 사용되는 격리 수준이며, 일반적으로 가장 많이 사용된다. 출처 : https://lng1982.tistory.com/287 Dirty Read는 발생하지 않지만, NON-REPEATABLE READ 문제가 발생할 수 있다. 즉, 하나의 트랜잭션 내에서 똑같은 SELECT 쿼리를 실행했는데 다른 결과가 나올 수 있게된다. (Bob이 post 테이블에서 id가 1인 것을 조회하는데 처음엔 Transactions, 그 다음엔 ACID가 조회된다.) 출처 : https://vladmihalcea.com/non-repeatable-read/ 이러한 부정합 현상은 일반적인 웹 프로그램에서는 크게 문제되지 않을 수 있지만, 하나의 트랜잭션에서 동일한 데이터를 여러 번 읽고 변경하는 작업이 금전적인 처리와 연결되면 문제가 될 수 있다. 예를 들어, 다른 트랜잭션에서 입출금 처리가 계속 진행되는데 또 다른 트랜잭션에서 오늘 입금된 금액의 총합을 조회하는 경우, 조회할 때마다 다른 결과가 나오게된다. REPEATABLE READ NON-REPEATABLE READ 부정합 문제가 발생하지 않는다. MySQL의 InnoDB 스토리지 엔진에서 기본적으로 사용되는 격리 수준이다. 트랜잭션이 롤백될 가능성에 대비해 변경 전 데이터를 언두(Undo) 공간에 백업해두고 실제 레코드 값을 변경한다. 이러한 변경 방식을 MVCC(Multi Version Concurrency Control)이라고 한다. READ COMMITTED도 MVCC를 이용해 COMMIT 되기 전의 데이터를 보여준다. 차이는 언두 영역에 백업된 레코드의 여러 버전 가운데 몇 번째 이전 버전까지 찾아 들어가는지에 있다. 모든 InnoDB의 트랜잭션은 고유한 트랜잭션 번호(순차적으로 증가하는 값)를 가진다. 언두 영역에 백업된 모든 레코드에는 변경을 발생시킨 트랜잭션 번호가 포함된다. 언두 영역의 백업된 데이터는 InnoDB 스토리지 엔진이 불필요하다고 판단하는 시점에 주기적으로 삭제한다. REPEATABLE READ 격리 수준에서는 실행 중인 트랜잭션 가운데 가장 오래된 트랜잭션 번호보다 앞선 트랜잭션 번호를 갖는 레코드를 언두 영역에서 삭제할 수 없다. 아래 예에서 T2의 트랜잭션 번호는 10이므로, 해당 트랜잭션 안에서 실행되는 모든 SELECT 쿼리는 트랜잭션 번호가 10보다 작은 트랜잭션 번호에서 변경한 것만 보게 된다. 만약 하나의 트랜잭션이 장시간 동안 지속되면 언두 영역이 백업된 데이터로 무한정 커질 수도 있다. 백업된 레코드가 많아지면 DB 서버의 처리 성능이 떨어질 수 있다. REPEATABLE READ에서는 PHANTOM READ(PHANTOM ROW) 부정합 문제가 나타날 수 있다. PHANTOM READ : 다른 트랜잭션에서 수행한 변경 작업에 의해 레코드가 보였다 안 보였다 하는 현상 SELECT ... FOR UPDATE 쿼리는 SELECT 하는 레코드에 쓰기 잠금을 걸어야 하는데, 언두 영역의 레코에는 잠금을 걸 수 없다. 따라서, 언두 영역이 아닌 실제로 변경된 테이블에서 값을 가져오게 된다.SERIALIZABLE 가장 단순하면서 엄격한 격리 수준이다. 동시 처리 성능도 다른 격리 수준에 비해 떨어진다. InnoDB 테이블에서 기본적으로 순수한 SELECT 작업은(INSERT ... SELECT ... 또는 CREATE TABLE ... AS SELECT ...가 아닌) 아무런 레코드 잠금을 설정하지 않고 실행된다. 하지만 SERIALIZABLE 격리 수준에서는 이러한 읽기 작업도 공유 잠금을 획득해야 한다. 따라서 다른 트랜잭션에서는 절대 접근할 수 없다. 참고. DB별 격리 수준(default)MS-SQL : READ COMMITTEDMySQL : REPEATABLE READORACLE : READ COMMITTEDH2 : READ COMMITTED격리 수준에 따른 부정합 문제 정리 위에서 살펴본 격리 수준별 부정합 문제를 다시 한 번 정리해보자. Dirty Read : 트랜잭션이 아직 커밋되지 않은 데이터를 읽는 것. Non Repeatable read : 동일한 트랜잭션 내에서 실행한 동일한 SELECT 쿼리가 다른 결과를 가져오는 것. Phantom Read : 다른 트랜잭션에서 수행한 변경 작업에 의해 레코드가 보였다 안 보였다 하는 것. 격리 수준 DIRTY READ NON-REPEATABLE READ PHANTOM READ READ UNCOMMITTED O O O READ COMMITTED X O O REPEATABLE READ X X O (InnoDB는 X) SERIALIZABLE X X X 참고 자료 이성욱, 『개발자와 DBA를 위한 Real MySQL』, 위키북스(2012), 4장 https://www.geeksforgeeks.org/acid-properties-in-dbms/ https://en.wikipedia.org/wiki/Database_transaction https://lng1982.tistory.com/287" }, { "title": "Java - 자바의 메모리 구조와 변수 종류 살펴보기", "url": "/posts/java-variable-type-and-jvm-memory/", "categories": "지식 더하기, 이론", "tags": "Java", "date": "2021-08-23 23:00:00 +0900", "snippet": "들어가기 전스프링 빈은 기본적으로 싱글톤으로 생성되기 때문에, 만약 @Service로 선언된 비즈니스 로직 클래스에 인스턴스 변수가 선언되어 있다면비즈니스 로직을 처리하는 모든 스레드는 해당 데이터를 공유하게 된다. 이것을 고려하지 않고 무작정 인스턴스 변수로 선언하면, 멀티 스레딩 환경에서 데이터가 꼬여서 시스템 장애를 초래할 수도 있다. 그렇다면 ...", "content": "들어가기 전스프링 빈은 기본적으로 싱글톤으로 생성되기 때문에, 만약 @Service로 선언된 비즈니스 로직 클래스에 인스턴스 변수가 선언되어 있다면비즈니스 로직을 처리하는 모든 스레드는 해당 데이터를 공유하게 된다. 이것을 고려하지 않고 무작정 인스턴스 변수로 선언하면, 멀티 스레딩 환경에서 데이터가 꼬여서 시스템 장애를 초래할 수도 있다. 그렇다면 왜 인스턴스 변수는 스레드 간에 공유되는 것일까? 이를 이해하기 위해 JVM이 관리하는 메모리 구조에 대해 알아보자.Run-Time Data Areas Java Virtual Machine(JVM)은 프로그램 실행 중에 사용되는 다양한 런타임 데이터 영역을 정의한다. 데이터 영역은 크게 JVM 단위의 영역과 스레드 단위의 영역으로 구분할 수 있다. JVM 단위 : JVM을 시작할 때 생성되며 JVM이 종료될 때 삭제되는 데이터 영역. Heap Method Area JVM 단위의 데이터 영역은 모든 스레드 간에 공유된다. 스레드 단위 : 스레드가 생성될 때 만들어지고 스레드가 종료될 때 삭제되는 데이터 영역. PC Register JVM Stack Native Method Stack 출처 : https://homoefficio.github.io/2019/01/31/Back-to-the-Essence-Java-%EC%BB%B4%ED%8C%8C%EC%9D%BC%EC%97%90%EC%84%9C-%EC%8B%A4%ED%96%89%EA%B9%8C%EC%A7%80-2/Heap 힙은 모든 클래스 인스턴스 및 배열이 할당되는 영역이다. 힙에 저장된 객체에 할당된 메모리는 명시적인 방법으로는 절대 회수되지 못하며, 오직 가비지 컬렉터(garbage collector)에 의해서만 회수될 수 있다. 힙은 고정된 크기이거나 계산에 따라 확장/축소될 수 있다. 연산에 필요한 힙의 크기가 부족할 경우 JVM이 OutOfMemoryError를 발생시킨다.Method Area 런타임 상수 풀, 필드와 메서드 데이터, 생성자 및 메서드의 코드와 같은 클래스 단위의 데이터를 저장한다. 메서드 영역은 논리적으로 힙의 일부이기 때문에 GC 대상이 되지만 단순 구현에서는 garbage collecting 또는 compacting(압축)하지 않도록 구현할 수 있다. 할당 요청을 위해 메서드 영역의 메모리를 사용할 수 없는 경우 JVM OutOfMemoryError를 발생시킨다.Run-Time Constant Pool 런타임 상수 풀에는 컴파일 타임에 이미 알 수 있는 숫자 리터럴 값부터 런타임에 해석되는 메서드와 필드의 참조까지를 포괄하는 여러 종류의 상수가 포함된다. 런타임 상수 풀은 다른 전통적인 언어에서 말하는 심볼 테이블과 비슷한 기능을 한다고 보면 된다.PC Register JVM은 한 번에 여러 스레드의 실행을 지원할 수 있다. 따라서 각 스레드에는 자체 PC(프로그램 카운터) 레지스터가 있다. 각 JVM 스레드는 단일 메서드의 코드, 즉 해당 스레드의 현재 메서드를 실행한다. 이 메서드가 native가 아닌 경우 pc 레지스터에는 현재 실행 중인 JVM 명령의 주소를 저장한다. 스레드에서 현재 실행 중인 메서드가 native이면 JVM의 pc 레지스터 값이 정의되지 않는다.Java Virtual Machine Stacks 각 스레드에는 스레드와 동시에 생성된 전용 JVM 스택이 있다. JVM 스택은 프레임(Frame)을 저장한다. JVM 스택은 로컬 변수와 부분 결과를 보유하고 있으며 메서드 호출 및 반환을 담당한다. 스레드의 계산에 허용된 크기보다 큰 JVM 스택이 필요한 경우 JVM이 StackOverflowError를 발생시킨다. JVM 스택을 동적으로 확장할 수 있지만 메모리가 부족하거나, 새 스레드에 대한 초기 JVM 스택을 만드는 데 메모리가 부족한 경우JVM이 OutOfMemoryError를 발생시킨다.참고. Stack Frame JVM 스택을 구성하는 프레임은 다음과 같은 용도로 사용된다. 데이터 및 부분 결과를 저장 동적 연결(dynamic linking) 수행 메서드에 대한 값을 반환 예외 전달 각 프레임의 구성 요소는 다음과 같다. 로컬 변수 배열(array of local variables) 피연산자 스택(operand stack) 현재 메서드 클래스의 런타임 상수 풀에 대한 참조 메서드가 호출될 때마다 새 프레임이 생성된다. 프레임은 메서드 호출이 완료되면 해당 완료가 정상인지 여부에 관계없이 삭제된다.Native Method Stack JVM 구현시 native 메서드(Java 프로그래밍 언어가 아닌 언어로 작성된 메서드)를 지원하기 위해 일반적으로 “C 스택”이라고 불리는 기존 스택을 사용할 수 있다. native 메서드를 로드할 수 없고 기존 스택에 의존하지 않는 JVM 구현에서는 Native Method Stack을 제공할 필요가 없다. 제공된 경우 Native Method Stack은 일반적으로 각 스레드가 생성될 때 스레드별로 할당된다. 이 스펙을 사용하면 Native Method Stack이 고정된 크기이거나 계산에 필요한 대로 동적으로 확장 및 축소될 수 있다. Native Method Stack의 크기가 고정된 경우 해당 스택을 만들 때 각각의 크기를 개별적으로 선택할 수 있다. 스레드의 계산에 허용된 것보다 큰 Native Method Stack이 필요한 경우 JVM이 StackOverflowError를 발생시킨다. Native Method Stack을 동적으로 확장하려고 했으나 사용 가능한 메모리가 부족하거나, 새 스레드에 대한 초기 Native Method Stack을 만드는 데 사용할 수 없는 경우 JVM에서 OutOfMemoryError를 발생시킨다.변수의 종류 자바의 변수에는 클래스 변수 / 인스턴스 변수 / 매개 변수 / 지역 변수가 있다.위에서 공부한 메모리 구조를 기반으로 각각의 변수가 어디에 저장될지 생각해보자.public class VariableType { static int 클래스_변수; int 인스턴스_변수; public void test(int 매개_변수) { int 지역_변수; }} 클래스 변수 클래스가 처음 호출될 때 생명이 시작되고, 자바 프로그램이 끝날 때 소멸된다. 저장되는 영역 : Method Area 주의 : JVM 단위의 영역이므로 모든 스레드 간에 데이터가 공유된다. 인스턴스 변수 객체가 생성될 때 생명이 시작되고, 그 객체를 참조하고 있는 다른 객체가 없으면 소멸된다. 저장되는 영역 : Heap 주의 : JVM 단위의 영역이므로 모든 스레드 간에 데이터가 공유된다. 매개 변수 메서드가 호출될 때 생명이 시작되고, 메소드가 끝나면 소멸된다. 저장되는 영역 : JVM Stack(Stack Frame) 지역 변수 지역 변수를 선언한 중괄호 내에서만 유효 저장되는 영역 : JVM Stack(Stack Frame) ※ 참고 : 메소드 밖에서 선언된 변수를 ‘멤버 변수’라고 하며 클래스 변수와 인스턴스 변수가 포함된다.참고자료 https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-2.html https://homoefficio.github.io/2019/01/31/Back-to-the-Essence-Java-%EC%BB%B4%ED%8C%8C%EC%9D%BC%EC%97%90%EC%84%9C-%EC%8B%A4%ED%96%89%EA%B9%8C%EC%A7%80-2/ 이상민, 『자바의 신 1』, 로드북(2017), 4장" }, { "title": "CompletableFuture, Parellel Stream을 통해 성능 개선해보기", "url": "/posts/applying-java-async-for-performance/", "categories": "경험하기, 작업 노트", "tags": "Java", "date": "2021-08-18 14:00:00 +0900", "snippet": "들어가기 전현재 운영하고 있는 시스템(자바 버전8)에서 조회 시간이 꽤 오래 소요되는 화면들이 있었다. 모든 화면이 그런것은 아니었지만 몇몇 화면의 백엔드 로직은 아래와 같이 되어있었다.public Map&lt;String, Object&gt; retrieve() { Object result1 = testProxy.findSomething(param...", "content": "들어가기 전현재 운영하고 있는 시스템(자바 버전8)에서 조회 시간이 꽤 오래 소요되는 화면들이 있었다. 모든 화면이 그런것은 아니었지만 몇몇 화면의 백엔드 로직은 아래와 같이 되어있었다.public Map&lt;String, Object&gt; retrieve() { Object result1 = testProxy.findSomething(param1); // blocking Object result2 = testProxy.findSomething(param2); // blocking Object result3 = testProxy.findSomething(param3); // blocking Map&lt;String, Object&gt; toClient = new HashMap&lt;&gt;(); toClient.put(\"result1\", result1); toClient.put(\"result2\", result2); toClient.put(\"result3\", result3); return toClient;}즉, 모든 결과를 하나의 Map에 담아서 클라이언트에게 리턴하는 형식이었다. 하지만, 조회 결과들은 서로 독립적이었기 때문에 굳이 sequential하게 처리할 필요가 없을 것 같다는 생각이 들었다.따라서 자바에서의 비동기 처리를 공부 하고 해당 로직을 개선해보았다.Before 로직은 위에서 살펴본 것과 같고, insomnia를 활용해 응답 시간을 측정해보았다. 개선 전(약 5.7초 소요) (핸드폰 카메라로 찍어 화질이 좋지 않은 점 양해부탁드립니다.)After. CompletableFuture 적용Case1. 성능 개선됨위에서 살펴본 로직을 다음과 같이 변경하여 약 2배 이상 빠르게 조회되는 것을 확인할 수 있었다.public Map&lt;String, Object&gt; retrieve() { CompletableFuture&lt;Object&gt; future1 = CompletableFuture.supplyAsync(() -&gt; testProxy.findSomething(param1)); CompletableFuture&lt;Object&gt; future2 = CompletableFuture.supplyAsync(() -&gt; testProxy.findSomething(param2)); CompletableFuture&lt;Object&gt; future3 = CompletableFuture.supplyAsync(() -&gt; testProxy.findSomething(param3)); Map&lt;String, Object&gt; toClient = new HashMap&lt;&gt;(); toClient.put(\"result1\", future1.join()); toClient.put(\"result2\", future2.join()); toClient.put(\"result3\", future3.join()); return toClient;} CompletableFuture 적용 후(약 2.2초 소요) (핸드폰 카메라로 찍어 화질이 좋지 않은 점 양해부탁드립니다.)get() vs join() Future의 결과를 가져오기 위한 메서드로는 get()과 join()이 있는데 get()의 경우 checked exception/unchecked exception, join()의 경우 unchecked exception을 발생시킨다. 좀 더 공부해봐야겠지만, 현재 드는 생각으로는 get()에서 체크하는 ExecutionException, InterruptedException, TimeoutException(timeout 속성 사용시)이 발생하더라도 호출하는 입장에서는 명확한 복구 대책이 없을 것 같다. 따라서, join()을 활용하고 혹여나 런타임시 예외가 발생하면 ExceptionHandler(@ControllerAdvice, @ExceptionHandler)에서 처리하여 결과적으로 클라이언트(브라우저)에 문제가 생겼다는 것을 전달한다. 사용자는 몇 번의 조회 시도를 더 해보고 안되면 관리자에게 연락할 수 있도록 한다.CompletableFuture 사용시 while문을 통해 결과 확인 ?CompletableFuture를 공부하면서 Future의 결과를 가져오기전에 작업이 완료됐는지 확인하는 아래와 같은 샘플코드를 접할 수 있었다.ExecutorService executorService = Executors.newSingleThreadExecutor();CompletableFuture&lt;String&gt; future = new CompletableFuture&lt;&gt;(); // creating an incomplete futureexecutorService.submit(() -&gt; { Thread.sleep(500); future.complete(\"value\"); // completing the incomplete future return null;});while (!future.isDone()) { // checking the future for completion Thread.sleep(1000);}String result = future.get(); // reading value of the completed futurelogger.info(\"result: {}\", result);executorService.shutdown();내가 알기로 get, join은 작업이 완료될때까지 blocking 하는 메서드인데 왜 굳이 while이 필요할까? 라는 생각이 들어 StackOverflow에 질문을 해봤고, 결과적으로 답변자들은 while문이 필요하지 않다는 답변을 남겼다. 이 사람들도 모르는 무언가가 있을지는 몰라도, 나 또한 굳이 while문이 필요하지 않은 것 같다는 생각이었기에 개선한 코드에서 알 수 있듯이 while문은 배제했다.참고. StackOverflow 답변“while문을 사용해야 한다면 제 코드의 경우 다음과 같이 작성해야하는게 좋을까요?” 에 대한 답변이다.CompletableFuture&lt;Void&gt; allFutures = CompletableFuture.allOf(future1, future2, future3); while(!allFutures.isDone()){} Map&lt;String, Object&gt; toClient = new HashMap&lt;&gt;(); toClient.put(\"result1\", future1.get()); toClient.put(\"result2\", future2.get()); toClient.put(\"result3\", future3.get()); 답변1. The actively blocking loop while(!allFutures.isDone()){} is not okay and will melt your CPU (100% CPU usage). If you want to wait until all futures are done, just do allFutures.join() or allFutures.get(). That will be much better. → while문이 CPU에 과부하를 줄 것이고 get 이나 join이면 충분하다. 답변2. The first “Code after refactoring” is fine, the subsequent stuff is horrible. Why do you think you have to compare your code to something you “found somewhere”? Either, it’s a reputable source you can cite (which also usually explains why it does something in a certain way), or it’s not worth discussing. → 끔찍한 코드이며, 다른 사람의 코드에 근거가 부족하다면 굳이 비교하지 마라. (혼났음..) Case2. 성능 개선 되지 않음비슷한 로직을 가진 다른 화면에도 동일하게 적용해봤지만 성능 개선이 되지 않는 경우도 있었다. 원인을 살펴보니, 여러 개의 조회 메서드 중, 특정 하나에서 시간이 오래걸리는 경우였다.그렇게 되면, 아래 코드 기준으로 retrieve()는 가장 오래 걸리는 작업(future2)이 끝날때까지 기다려야하므로, context switching과 같은 멀티 스레딩 오버헤드를 고려했을 때,sequential하게 처리하는게 더 나을수도 있다. Case1의 경우 측정해보니 세 개의 메서드가 거의 동일한 시간이 걸렸다.즉, 적용하고자 하는 로직이 어떤 특성을 갖는지 파악하고 적용해야 개선 효과를 얻을 수 있을 것 같다.public Map&lt;String, Object&gt; retrieve() { CompletableFuture&lt;Object&gt; future1 = CompletableFuture.supplyAsync(() -&gt; testProxy.findSomething(param1)); CompletableFuture&lt;Object&gt; future2 = CompletableFuture.supplyAsync(() -&gt; testProxy.findSomething(param2)); CompletableFuture&lt;Object&gt; future3 = CompletableFuture.supplyAsync(() -&gt; testProxy.findSomething(param3)); Map&lt;String, Object&gt; toClient = new HashMap&lt;&gt;(); toClient.put(\"result1\", future1.join()); // 10ms toClient.put(\"result2\", future2.join()); // 2000ms toClient.put(\"result3\", future3.join()); // 10ms return toClient;}After. Parellel Stream 적용위에서 살펴본 로직을 다음과 같이 변경하였고, 개선 전에 비해 빨라졌지만 CompletableFuture와 비교했을 때는 좀 더 느린 것을 확인할 수 있었다.둘 다 기본적으로 fork-join common pool을 사용하지만, Stream에서는 그룹화하는 부분(toMap()) 때문에 CompletableFuture에 비해 시간이 좀 더 걸리는 것 같다.public Map&lt;String, Object&gt; retrieve() { Map&lt;String, String&gt; queries = new HashMap&lt;&gt;(); queries.put(\"result1\", \"queryId\"); queries.put(\"result2\", \"queryId2\"); queries.put(\"result3\", \"queryId3\"); return queries.entrySet().parallelStream() .collect(Collectors.toMap( e -&gt; e.getKey(), e -&gt; testProxy.findSomething(e.getValue(), params)) );} CompletableFuture 적용 후(약 3.8초 소요) (핸드폰 카메라로 찍어 화질이 좋지 않은 점 양해부탁드립니다.)결론 사실 5초에서 2초로 개선은 되었지만, 2초도 절대 빠른속도는 아니라고 생각한다. 애플리케이션 로직뿐 아니라, 근본적인 쿼리를 수정해서 성능을 개선해보고 싶다.아직은 학습이 부족하기에 차근차근 공부해서 쿼리 튜닝도 할 수 있도록 해보자. supplyThen() 등 다양한 메서드를 활용하여 비동기적으로 파이프라인을 구축할 수 있다는게 CompletableFuture의 큰 장점인 것 같다. 아직은 맛보기에 불과하지만 앞으로 점차 다양한 메서드를 활용해서CompletableFuture를 잘 활용해보자. 더 공부해야할 부분 쿼리 성능 개선" }, { "title": "Java - Exception과 Error", "url": "/posts/java-exception-error/", "categories": "지식 더하기, 이론", "tags": "Java", "date": "2021-08-10 00:29:00 +0900", "snippet": "들어가기 전CompletableFuture를 사용하여 결과를 가져올 때, get()과 join()을 사용할 수 있는데, get()의 경우 checkedException join()의 경우 uncheckedException을 발생시킨다고 한다.이 둘의 차이는 무엇이고 그럼 어떤 메서드를 사용해야할까에 대한 판단을 내리기 위해 공부해보자.Error vs ...", "content": "들어가기 전CompletableFuture를 사용하여 결과를 가져올 때, get()과 join()을 사용할 수 있는데, get()의 경우 checkedException join()의 경우 uncheckedException을 발생시킨다고 한다.이 둘의 차이는 무엇이고 그럼 어떤 메서드를 사용해야할까에 대한 판단을 내리기 위해 공부해보자.Error vs ExceptionError와 Exception은 모두 java.lang.Throwable의 하위 클래스이다. 각각의 특징을 한 번 살펴보자. 출처 : https://facingissuesonit.com/java-exception-handling/Error 에러는 Unchecked Exception이며 개발자는 이 것에 대한 어떠한 처리를 하지 않아도된다.(사실 할 수 있는게 없다.) 예를 들어 OutOfMemoryError, StackOverflowError 등이 있다. 즉, 애플리케이션이 해당 에러를 복구할 방법이 없기 때문에, try-catch 절을 사용하지 않고 대부분의 경우 애플리케이션이 종료되도록 허용해야 한다. 애플리케이션 수준에서 복구할 방법이 없는 이유는, 대부분의 오류가 JVM에 의해(시스템 레벨) 발생하기 때문이다. Exception ‘Exception Event’의 줄임말로써, 프로그램의 정상적인 흐름을 중단(interrupt)하는 이벤트이다. 개발자가 작성한 코드에 의해 발생하며, try-catch를 통해 키워드를 throw 함으로써 복구할 수 있다. 두 가지 유형의 예외로 나눌 수 있다. Checked Exception : 컴파일 시점에 컴파일러에 의해 체크되는 예외 Unchecked Exception : 컴파일 시점에 컴파일러에 의해 체크되지 않는 예외 Exception과 Error 비교 요약   Exception Error 복구 여부 try-catch 사용하여 호출자에게 다시 예외 throw 불가능 유형 Checked type, Unchecked type Unchecked type 발생 원인 프로그램에 의해 발생 대부분 프로그램이 실행되는 환경에 의해 발생 발생 시점 런타임(Checked type의 경우 컴파일 시점에 컴파일러가 체크) 런타임 Checked Exception vs Unchecked Exception자바에서 예외는 명시적으로 반드시 처리해야 하는 Checked Exception(SQLException, IOException 등)과명시적으로 처리하지 않아도 되는 Unchecked Exception(NullPointerException, NumberFormatException)으로 나눠진다.Unchecked Exception은 RuntimeException을 상속받는다.예외 처리의 강제성에 대해자바 공식문서 에 보면 예외 처리와 관련하여 설명하고 있다. Checked Exception의 경우 예외 처리를 강제하는 이유 해당 예외를 발생시킬 수 있는 여지를 포함하고 있는 메서드를 호출하는 호출자가 예외에 대해 인지하고 적절하게 처리하도록 할 수 있게 하기 위해. public FileInputStream(File file) throws FileNotFoundException { String name = (file != null ? file.getPath() : null); SecurityManager security = System.getSecurityManager(); if (security != null) { security.checkRead(name); } if (name == null) { throw new NullPointerException(); } if (file.isInvalid()) { throw new FileNotFoundException(\"Invalid file path\"); } fd = new FileDescriptor(); fd.attach(this); path = name; open(name);}public FileInputStream(String name) throws FileNotFoundException { this(name != null ? new File(name) : null);}private static void checkedExceptionWithTryCatch() { File file = new File(\"not_existing_file.txt\"); try { FileInputStream stream = new FileInputStream(file); } catch (FileNotFoundException e) { e.printStackTrace(); }} 왜 Unchecked Excpetion의 경우는 예외 처리를 강제하지 않았을까? Runtime Exception은 프로그램의 어디에서나 발생할 수 있으며 매우 다양한 경우가 있다. 만약, 모든 메서드 선언에 이러한 예외를 추가해야 하면 프로그램의 명확성이 떨어진다. 예를 들어, 배열을 사용하는 메서드에서 ArrayIndexOutOfBoundException, 객체 참조하는 코드가 있는 곳에서 NullPointerException 일일이 처리해줘야한다면 코드가 매우 지저분해질 것이다. 결론적으로, 해당 글에서는 다음과 같이 마무리한다. 클라이언트(호출자)가 예외로부터 복구할 것으로 합리적으로 기대할 수 있는 경우 checked exception을 사용할 것. 클라이언트(호출자)가 예외에서 복구하기 위해 아무 것도 할 수 없으면 unchecked exception을 사용할 것. 실제로 적용해보기 completableFuture 사용시 get(), fork() 중 어떤 것을 사용해야할까?참고자료 https://stackoverflow.com/questions/5813614/what-is-difference-between-errors-and-exceptions https://madplay.github.io/post/java-checked-unchecked-exceptions https://cheese10yun.github.io/checked-exception/#unchecked-exception-1 https://www.geeksforgeeks.org/errors-v-s-exceptions-in-java/ https://docs.oracle.com/javase/tutorial/essential/exceptions/runtime.html https://www.baeldung.com/java-checked-unchecked-exceptions" }, { "title": "Java - 비동기 프로그래밍을 위한 자바 클래스 살펴보기", "url": "/posts/java-asyncronous-programming/", "categories": "지식 더하기, 이론", "tags": "Java", "date": "2021-08-09 00:29:00 +0900", "snippet": "들어가기 전회사에서 화면 조회 성능 개선을 위해 어떤 부분을 리팩토링 하면될까 고민하던 중, 몇몇 화면이 호출하는 api에서 수행하는 로직이 각각 독립적인 조회 결과들을 Map에 담아 화면에 리턴해주는 형태로 되어있는 것을 보았다.따라서, 이런 경우 비동기적으로 처리한다면 성능이 개선되지 않을까 ? 하는 생각이들어 어떤식으로 자바에서 비동기 프로그래밍...", "content": "들어가기 전회사에서 화면 조회 성능 개선을 위해 어떤 부분을 리팩토링 하면될까 고민하던 중, 몇몇 화면이 호출하는 api에서 수행하는 로직이 각각 독립적인 조회 결과들을 Map에 담아 화면에 리턴해주는 형태로 되어있는 것을 보았다.따라서, 이런 경우 비동기적으로 처리한다면 성능이 개선되지 않을까 ? 하는 생각이들어 어떤식으로 자바에서 비동기 프로그래밍을 하면되는지 공부해보았다.Thread 자바에서 비동기 프로그래밍을 위한 첫 번째 방법은 JDK 1.0에 있는 Runnable 인터페이스와 Thread 클래스를 사용하는 것이다. 클래스는 Runnable을 구현하고 run() 메서드를 재정의하거나, Thread를 상속받아 동일한 작업을 수행할 수 있다. 차이점은 실행 메서드가 Runnable에서 직접 호출될 때 새 스레드가 생성되지 않고 호출 중인 스레드에서 실행된다. thread.start()를 수행하면 새 스레드가 생성된다. JDK 1.5의 스레드 관리를 개선하기 위해 Executor 프레임워크가 등장했다. 따라서, 여러 스레드 풀을 사용할 수 있으며, 수동으로 스레드를 작성할 필요가 없다. 또한, 스레드 수를 지정할 수 있으며 스레드를 재사용할 수 있다. 이전에 공부했듯이, 멀티 스레딩을 위해서는 스레드 풀을 사용하는 것이 장점이 많기 때문에 일반적으로는 비동기 처리를 위해 굳이 스레드를 직접 생성하고 관리하진 않는 것 같다.Future Java 5 부터, Future 인터페이스는 FutureTask를 사용하여 비동기 작업을 수행할 수 있는 방법을 제공한다. ExecutorService의 sumbit() 메서드를 사용하여 비동기적으로 작업을 수행하고 FutureTask 객체를 반환한다. 이 객체는 작업이 끝나면 결과를 얻을 수 있다는 약속(promise)이며, get() 메서드를 사용해 결과를 얻는다. ExecutorService threadpool = Executors.newCachedThreadPool();Future&lt;Long&gt; futureTask = threadpool.submit(() -&gt; factorial(number));while (!futureTask.isDone()) { System.out.println(\"FutureTask is not finished yet...\");}long result = futureTask.get();threadpool.shutdown();Future의 한계 Future API는 몇 가지 중요하고 유용한 기능이 부족하다.1. 수동으로 완료될 수 없다. 외부 API를 호출하는 등의 과정을 거쳐 결과적으로 Future를 반환하는 메서드가 있다고 가정하자. 만약 API 서비스가 다운된 경우, 캐시된 가장 최근의 값 등을 Future에 세팅하여 수동으로 리턴한다면 에러가 발생하지 않을 것이다. 하지만, Future에 이러한 기능은 없다.2. 블로킹 없이 추가적인 작업을 수행할 수 없다. Future에 콜백 함수를 추가하고 Future의 결과를 사용할 수 있을 때, 자동으로 호출하도록 할 수 없다. 즉, 결과를 사용할 수 있을 때까지 블로킹하는 get() 메서드만 제공한다. 3. 여러개의 작업을 병합할 수 없다. 병렬로 실행하려는 10개의 작업이 있고 모든 작업이 완료된 후 일부 기능을 실행한다고 가정했을 때, Future에는 10개를 한꺼번에 실행할 수 있는 기능은 없다.4. 예외 처리를 제공하지 않는다. Future API에는 예외 처리를 위한 메서드 등이 없다.CompletableFuture 위에서 살펴봤듯이 Future에는 여러가지 한계점이 있었다. 따라서, 자바8에서는 Future를 업그레이드 시킨 CompletableFuture가 등장했다. CompletableFuture는 Future 뿐아니라 CompletionStage 인터페이스도 구현한다. CompletionStage는 다른 스레드에서 계산된 결과를 간단하게 사용할 수 있는 다양한 메서드를 제공한다. 즉, 중첩된 콜백(callback hell) 없이 단일 결과에 여러 비동기 연산을 연결, 결합하는 파이프라인을 지원한다. CompletableFuture&lt;Integer&gt; priceInEur = CompletableFuture.supplyAsync(this::getPriceInEur);CompletableFuture&lt;Integer&gt; exchangeRateEurToUsd = CompletableFuture.supplyAsync(this::getExchangeRateEurToUsd);CompletableFuture&lt;Integer&gt; netAmountInUsd = priceInEur .thenCombine(exchangeRateEurToUsd, (price, exchangeRate) -&gt; price * exchangeRate);logger.info(\"this task started\");netAmountInUsd .thenCompose(amount -&gt; CompletableFuture.supplyAsync(() -&gt; amount * (1 + getTax(amount)))) .whenComplete((grossAmountInUsd, throwable) -&gt; { if (throwable == null) { logger.info(\"this task finished: {}\", grossAmountInUsd); } else { logger.warn(\"this task failed: {}\", throwable.getMessage()); } }); // non-blockinglogger.info(\"another task started\"); 내부적으로 ForkJoinPool을 사용하여 작업을 비동기식으로 처리한다. 즉, 전역 ForkJoinPool.commonPool()메서드에서 얻은 스레드에서 작업을 실행한다. ForkJoinPool.commonPool() 메서드에 의해 반환된 스레드 풀은 모든 CompletableFutures 및 모든 병렬 스트림에 의해 JVM 전체에서 공유된다. 전역적으로 공유되기 때문에 common pool 사용시 주의해야 한다.(실제 장애 사례) private static final Executor ASYNC_POOL = USE_COMMON_POOL ? ForkJoinPool.commonPool() : new ThreadPerTaskExecutor(); 사용자가 스레드 풀을 명시적으로 생성하여 해당 스레드 풀에서 가져온 스레드로 작업을 처리할 수도 있다. 즉, CompletableFuture의 여러 메소드에는 두 가지 변형이 있다. 하나는 사용자가 생성한 스레드 풀을 사용하고, 다른 하나는 내부적으로 생성된 스레드 풀을 사용한다. public static &lt;U&gt; CompletableFuture&lt;U&gt; supplyAsync(Supplier&lt;U&gt; supplier) { return asyncSupplyStage(ASYNC_POOL, supplier);}public static &lt;U&gt; CompletableFuture&lt;U&gt; supplyAsync(Supplier&lt;U&gt; supplier, Executor executor) { return asyncSupplyStage(screenExecutor(executor), supplier);} CompletableFuture&lt;Long&gt; completableFuture = CompletableFuture.supplyAsync(() -&gt; factorial(number));long result = completableFuture.get(); CompletableFuture 메서드 살펴보기 CompletableFuture 메서드는 크게 5개 그룹으로 나뉠 수 있다. CompletableFuture 생성 작업 완료 상태 체크 작업 완료 시키기 처리 결과 읽기 여러개의 작업 처리(bulk futures) 출처 : https://www.linkedin.com/pulse/asynchronous-programming-java-completablefuture-aliaksandr-liakh 1. CompletableFuture 생성 메서드 일반적으로 한 스레드에서 완료되지 않은 Future가 생성되고 다른 스레드에서 완료된다. 그러나 경우에 따라 이미 완료된 Future를 만들 수도 있다. 파라미터가 없는 CompletableFuture 생성자는 완료되지 않은 Future를 생성한다. 출처 : https://www.linkedin.com/pulse/asynchronous-programming-java-completablefuture-aliaksandr-liakh 2. 완료 상태 체크 메서드 CompletableFuture 클래스에는 작업이 완료되지 않았는지, 정상적으로 완료되었는지, 예외적으로 완료되었는지, 취소되었는지 여부를 확인하기 위한 non-blocking 메서드가 있다. 이미 완료된 작업은 취소가 불가능하다. 출처 : https://www.linkedin.com/pulse/asynchronous-programming-java-completablefuture-aliaksandr-liakh 3. 작업 완료를 위한 메서드 CompletableFuture에는 아직 완료되지 않은 작업을 정상완료, 예외완료, 취소 상태를 갖는 완료된 작업으로 바꾸는 메서드가 있다. cancel 메서드가 호출되면 CancellationException과 함께 연산이 취소된다. 하지만, 해당 작업을 수행하는 스레드를 중단하기 위한 Thread.interrupt()는 호출되지 않는다. 출처 : https://www.linkedin.com/pulse/asynchronous-programming-java-completablefuture-aliaksandr-liakh 4. 처리 결과 읽기 메서드 CompletableFuture는 작업에 대한 결과를 읽는(아직 완료되지 않은 경우는 대기) 메서드를 제공한다. 이러한 메서드는 대부분의 경우, 계산 파이프라인의 마지막 단계로 사용해야 한다. get(), get(timeout, timeUnit) 메서드는 ‘checked exception’을 발생시킬 수 있다. ExecutionException : 작업이 예외적으로 완료된 경우 InterruptedException : 현재 스레드가 중단된 경우 TimeoutException : get(timeout, timeUnit) 메서드 사용시 타임아웃 발생하는 경우 join(), getNow(valueIfAbsent) 메서드는 ‘unchecked exception’을 발생시킬 수 있다. CompletionException : 작업이 예외적으로 완료되는 경우 작업이 취소되는 경우, 모든 메서드가 CancellationException(unchecked exception)을 발생시킬 수 있다. 출처 : https://www.linkedin.com/pulse/asynchronous-programming-java-completablefuture-aliaksandr-liakh 5. 여러개의 작업 처리를 위한 메서드 CompletableFuture에는 많은 작업이 완료될 때까지 대기하는 두 가지 정적 메서드가 있다. 각각 다른 타입의 CompletableFuture가 메서드 파라미터로 입력될 수 있다. 정의된 파라미터 : CompletableFuture&lt;?&gt;... cfs 출처 : https://www.linkedin.com/pulse/asynchronous-programming-java-completablefuture-aliaksandr-liakh CompletionStage CompletionStage 인터페이스는 여러 단계를 거치는 연산에서, fork, chain, join할 수 있는 각 단계를 나타낸다. 또한, future/promise 구현에 대한 파이프라이닝을 명시한다. 파이프라이닝 각 단계는 연산을 수행한다. 값을 계산하거나(결과 반환) 작업 수행만 할 수도 있다(결과 반환 안 함). 각 단계를 파이프라인으로 연결한다. 하나 또는 두 개의 이전 단계를 완료하여 현재 단계를 시작할 수 있다. 각 단계는 연산이 완료되면 종료된다. 각 단계는 동기식 또는 비동기식으로 실행될 수 있다. 처리될 데이터에 따라 적절한 방법을 선택해야 한다. CompletionStage 인터페이스의 메서드는 크 두 그룹으로 나눌 수 있다. 파이프라이닝 연산을 위한 메서드 예외 처리를 위한 메서드 출처 : https://www.linkedin.com/pulse/asynchronous-programming-java-completablefuture-aliaksandr-liakh 1. 파이프라이닝 연산을 위한 메서드 CompletionStage 인터페이스에는 43개의 public 메서드가 있으며, 대부분 세 가지의 이름 패턴을 갖는다.첫째, 새로운 단계가 시작되는 방법을 설명한다. 메서드 이름에 “then”이 있으면, 하나의 이전 단계가 완료된 후 새 단계가 시작된다. 메서드 이름에 “either”가 있으면, 이전 두 단계 중 첫 번째 단계가 완료된 후 새 단계가 시작된다. 메서드 이름에 “both”가 있으면, 이전 두 단계를 모두 완료한 후 새 단계가 시작된다.둘째, 새로운 단계가 수행하는 연산에 대해 설명한다. 메서드 이름에 “apply”가 있으면, 새로운 단계는 주어진 Function을 기준으로 인수를 변환한다. 메서드 이름에 “accept”가 있으면, 새로운 단계는 주어진 Consumer를 기준으로 인수를 처리한다. 메서드 이름에 “run”이 있으면, 새로운 단계는 주어진 Runnable를 기준으로 작업을 수행한다. 출처 : https://www.linkedin.com/pulse/asynchronous-programming-java-completablefuture-aliaksandr-liakh 셋째, 어떤 스레드가 새로운 단계를 실행하는지 설명한다. 메서드에 “something(…)”의 형태이면, 새 단계는 기본 스레드 풀에 의해 실행된다.(동기 또는 비동기) 메서드에 “somethingAsync()”의 형태이면, 새 단계는 기본 비동기 스레드 풀(ForkJoinPool)에 의해 실행된다. 메서드에 “somethingAsync(…, Executor))”의 형태이면,새 단계는지정된 Executor(사용자 정의 스레드 풀)에 의해 실행된다.CompletableFuture&lt;Double&gt; pi = CompletableFuture.supplyAsync(() -&gt; Math.PI);CompletableFuture&lt;Integer&gt; radius = CompletableFuture.supplyAsync(() -&gt; 1);// area of a circle = π * r^2CompletableFuture&lt;Void&gt; area = radius .thenApply(r -&gt; r * r) .thenCombine(pi, (multiplier1, multiplier2) -&gt; multiplier1 * multiplier2) .thenAccept(a -&gt; logger.info(\"area: {}\", a)) .thenRun(() -&gt; logger.info(\"operation completed\"));area.join();2. 예외 처리를 위한 메서드 각 단계별 연산은 정상적으로 또는 예외적으로 완료될 수 있다.또한, 비동기 연산에서는 예외가 발생한 곳과 예외 처리를 위한 메서드는 서로 다른 스레드에 있을 수 있다.따라서 이 경우 try-catch-finally 문을 사용하여 예외를 처리할 수 없기 때문에CompletionStage는 예외를 처리하기 위한 특별한 메서드를 제공한다. 이전 단계가 정상적으로 완료되면, 다음 단계가 정상적으로 실행되기 시작합니다. 이전 단계가 예외적으로 완료되면, 파이프라인에 예외 복구 단계가 없는 한 다음 단계는 예외적으로 완료된다. whenComplete 메서드를 사용하면 결과(없는 경우 null)와 예외(없는 경우 null)를 모두 읽을 수 있지만 결과를 변경할 수는 없습니다. 예외 발생시 복구해야 하는 경우 handle과 exceptionally 메서드를 사용한다. handle 메소드의 BiFunction 인수는 이전 단계가 정상적으로 또는 예외적으로 완료될 때 모두 호출된다. exceptionally 메서드의 Function 인수는 이전 단계가 예외적으로 완료될 때 호출된다. 두 경우 모두 예외가 다음 단계로 전파되지 않는다. 출처 : https://www.linkedin.com/pulse/asynchronous-programming-java-completablefuture-aliaksandr-liakh CompletableFuture.supplyAsync(() -&gt; 0) .thenApply(i -&gt; { logger.info(\"stage 1: {}\", i); return 1 / i; }) // executed and failed .thenApply(i -&gt; { logger.info(\"stage 2: {}\", i); return 1 / i; }) // skipped .whenComplete((value, t) -&gt; { if (t == null) { logger.info(\"success: {}\", value); } else { logger.warn(\"failure: {}\", t.getMessage()); // executed } }) .thenApply(i -&gt; { logger.info(\"stage 3: {}\", i); return 1 / i; }) // skipped .handle((value, t) -&gt; { if (t == null) { return value + 1; } else { return -1; // executed and recovered } }) .thenApply(i -&gt; { logger.info(\"stage 4: {}\", i); return 1 / i; }) // executed .join();CompletableFuture 사용시 주의사항 어떤 스레드가 어떤 단계를 실행하는지 알고, 되도록이면 우선 순위가 높은 스레드가 오래 실행되는 낮은 우선 순위의 작업을 처리하지 않게 한다. 파이프라인 내에서 블로킹 메서드 사용을 지양한다. 빈번한 context switch로 인해 상당한 오버헤드가 발생할 수 있으므로 짧은(수백 밀리초) 비동기식 계산을 지양한다. try-catch-finally 문과 다르게 작동하는 새로운 예외 처리 메커니즘에 유의할 것. 작업이 완료되는 것을 너무 오래 기다리지 않도록 타임아웃을 관리한다.Parallel Stream Fork-Join Framework를 이용하여 작업들을 분할하고, 병렬적으로 처리한다. Fork-Join Framework는 작업 데이터를 worker 스레드 간에 분할하고, 작업 완료 시 콜백 처리를 담당한다. 공통 풀의 스레드 수는 프로세서 코어 수(논리 코어 수, Runtime.getRuntime().availableProcessors())와 동일하다. 그러나 JVM 매개 변수를 전달하여 사용할 스레드 수를 지정할 수도 있다. -D java.util.concurrent.ForkJoinPool.common.parallelism=4 그러나 이 설정은 전역 설정이므로 모든 병렬 스트림과 공통 풀을 사용하는 fork-join 작업에 영향을 미치기 때문에, 합당한 이유가 아닌 이상 기본 설정을 사용하는 것을 권장한다. 전역적으로 공유되기 때문에 common pool 사용시 주의해야 한다.(실제 장애 사례) 병렬 처리의 이점을 완벽히 활용하려면, 다음과 같은 오버헤드를 고려해야한다.Splitting Costs(분할 비용) 데이터 소스를 고르게 분할하는 데 드는 비용이다. 즉, Parallel Stream은 작업을 분할하기 위해 Spliterator의 trySplit()을 사용하는데, 분할되는 작업의 단위가 균등하게 나누어져야 하며, 나누어지는 작업에 대한 비용이 높지 않아야 순차적 방식보다 효율적으로 이루어질 수 있다. 0 ~ 1,000,000까지 ArrayList와 LinkedList에 할당한 뒤, 일반 스트림과 병렬 스트림 사용하여 성능 비교 ArrayList는 위치 속성을 활용하여 저렴하고 고르게 분할할 수 있는 반면, LinkedList에는 이러한 속성이 없다. Benchmark Mode Cnt Score Error UnitsDifferentSourceSplitting.differentSourceArrayListParallel avgt 25 2004849,711 ± 5289,437 ns/opDifferentSourceSplitting.differentSourceArrayListSequential avgt 25 5437923,224 ± 37398,940 ns/opDifferentSourceSplitting.differentSourceLinkedListParallel avgt 25 13561609,611 ± 275658,633 ns/opDifferentSourceSplitting.differentSourceLinkedListSequential avgt 25 10664918,132 ± 254251,184 ns/op Merging Costs(병합 비용) 병렬 연산을 위해 분할한 데이터 소스를 처리하고 난 뒤에는 각각의 결과를 병합해야 한다. 다음은 0 ~ 1,000,000까지 ArrayList에 할당한 뒤 reduce()를 통해 병합하는 경우와, collect()를 통해 Set으로 그룹화하는 경우에 대한 성능비교이다. reduce 같은 연산의 경우 비용이 저렴한 반면, Set이나 Map에 그룹화하는 것과 같은 병합 작업은 상당한 비용이 들 수 있다. Benchmark Mode Cnt Score Error UnitsMergingCosts.mergingCostsGroupingParallel avgt 25 135093312,675 ± 4195024,803 ns/opMergingCosts.mergingCostsGroupingSequential avgt 25 70631711,489 ± 1517217,320 ns/opMergingCosts.mergingCostsSumParallel avgt 25 2074483,821 ± 7520,402 ns/opMergingCosts.mergingCostsSumSequential avgt 25 5509573,621 ± 60249,942 ns/op 독립적인 작업 distinct(), sorted()와 같은 중간 단계 연산들(intermediate operation) 중 일부 연산자들은 연산자 내부에 상태(State) 정보를 가지고 있다. 따라서, 모든 Worker들은 독립적으로 다른 Thread에 의해 실행되지만 처리 결과를 이런 상태 정보에 저장하고, distinct() 연산자는 이 정보를 이용하여 정해진 기능을 수행한다. 즉, 내부적으로 어떤 공용 변수를 만들어 놓고 각 worker들이 이 변수에 접근할 경우 동기화 작업(synchronized) 등을 통해 변수를 안전하게 유지하면서 처리하고 있다. 따라서, 잘못 사용할 경우 순차적 실행보다 더 느릴 수도 있다.NQ Model Oracle에서 제시한 간단한 모델로써, 병렬화가 성능 향상을 제공할 수 있는지 여부를 판단하는 데 도움이 될 수 있다. N : 소스 데이터 요소의 수 Q : 데이터 요소당 수행되는 계산의 양 N*Q 제품이 클수록 병렬화를 통해 성능이 향상될 가능성이 높다. 숫자 합계와 같이 Q가 아주 작은 문제의 경우, N은 10,000보다 커야 한다. 계산 수가 증가함에 따라 병렬 처리로 성능을 높이는 데 필요한 데이터 크기는 감소한다. 좀 더 상세한 내용은 이 글 을 참조하면 좋을 것 같다.CompletableFuture vs Parallel Stream CompletableFuture와 Parallel Stream이 동일한 fork join common pool을 사용하는 동안 성능은 비슷할 수 있다. CompletableFuture의 성능은 선택한 스레드 수로 사용자 정의 스레드 풀을 구성해야 하는 상황에서 더 좋을 수 있다. 또한, 다른 작업을 수행하는 동안 URL에서 다운로드하려는 경우와 같이 비동기식 방법을 찾고 있다면 CompletableFuture를 선택할 수 있다. Parallel Stream은 모든 작업이 일부 작업을 수행하기를 원하는 CPU 집약적 작업, 즉 모든 스레드가 다른 데이터로 동일한 작업을 수행하기를 원하는 경우 좋은 선택이 될 수 있다.실제로 적용하기 회사 코드 비동기적으로 리팩토링 해보고 성능 비교해보기더 공부해야할 부분 stream 기초, 내부적인 작동방식 등참고자료 https://www.baeldung.com/java-asynchronous-programming https://www.cognizantsoftvision.com/blog/async-in-java/ https://www.linkedin.com/pulse/java-8-future-vs-completablefuture-saral-saxena https://www.callicoder.com/java-8-completablefuture-tutorial/ https://www.linkedin.com/pulse/asynchronous-programming-java-completablefuture-aliaksandr-liakh https://www.baeldung.com/java-when-to-use-parallel-stream https://www.popit.kr/java8-stream%EC%9D%98-parallel-%EC%B2%98%EB%A6%AC/ https://m.blog.naver.com/tmondev/220945933678 https://roytuts.com/difference-between-parallel-stream-and-completablefuture-in-java/" }, { "title": "Java - Thread Pool을 위한 Java Executor Framework", "url": "/posts/java-threadpool-executor-framework/", "categories": "지식 더하기, 이론", "tags": "Java", "date": "2021-08-07 00:29:00 +0900", "snippet": "자바에서의 비동기 처리를 위해 CompletableFuture에 대해 공부하며 클래스 내부를 보다보니 Executor, ForkJoinPool 등이 눈에 띄었다.생소한 부분이라 공부하며 나름대로 정리해보았다.Thread Pool Executor, ForkJoinPool에 대해 알기 전에 먼저 스레드 풀의 개념에 대해 살펴보자. 스레드를 만들고 관리...", "content": "자바에서의 비동기 처리를 위해 CompletableFuture에 대해 공부하며 클래스 내부를 보다보니 Executor, ForkJoinPool 등이 눈에 띄었다.생소한 부분이라 공부하며 나름대로 정리해보았다.Thread Pool Executor, ForkJoinPool에 대해 알기 전에 먼저 스레드 풀의 개념에 대해 살펴보자. 스레드를 만들고 관리하는 데는 비용이 많이 들기 때문에, 스레드를 필요할 때 마다 생성하는게 아니라 미리 생성해놓고 필요할 때 마다 재사용한다. 이를 위해 스레드를 관리하기 위한 스레드 풀이라는 개념이 나오게 된다. 즉, 요청이 들어올 때마다 새 스레드를 만드는 대신 스레드 풀을 사용하여 task를 병렬로 실행할 수 있다. 스레드 풀 인스턴스는 이러한 작업을 실행하기 위해 재사용되는 여러 스레드를 제어한다. 스레드를 재사용함으로써, 멀티 스레드를 활용하는 애플리케이션의 리소스를 아낄 수 있다. 스레드 풀을 통해 생성하는 스레드 수와 스레드의 생명주기를 제어할 수 있다. 또한 작업(task)을 스케줄링하고 큐에 작업을 보관할 수 있다. 스레드 갯수를 정함으로써 동시성의 정도를 제한할 수 있다. 출처 : https://www.baeldung.com/thread-pool-java-and-guava 스레드 풀을 사용해야 하는 이유 자바에서 스레드는 운영 체제의 리소스인 시스템 수준 스레드(system-level thread)에 매핑되기 때문에, 스레드를 무분별하게 생성하면 리소스가 빠르게 소진될 수 있다. ex) java.lang.OutOfMemoryError: unable to create new native thread 운영 체제는 여러 task를 동시에 처리(실제로는 한 번에 하나의 task)하기 위해 스레드 간 context switching을 수행한다. 따라서, 스레드를 많이 생성할수록 각 스레드가 실제 작업을 수행하는 데 걸리는 시간이 줄어든다. reqeust 또는 task 처리 중에 스레드가 생성되지 않으므로 응답 시간이 단축된다. 필요에 따라 애플리케이션의 실행 정책을 유연하게 변경할 수 있다. 예를 들어, 자바의 ExecutorService 구현체를 교체하기만 하면 단일 스레드에서 멀티 스레드로 대체할 수 있다. 시스템 부하 및 사용 가능한 리소스에 기반하여 스레드 수를 결정하기 때문에, 시스템의 안정성을 높인다. 스레드 관리보다 비즈니스 로직에 집중할 수 있다.자바에서의 스레드 풀 관리 Java 1.5 이전까지는 스레드 풀을 만들고 관리하는 것이 개발자의 책임이었지만, JDK 5 부터는 Executor 프레임워크에서 Java에 내장된 다양한 스레드 풀(fixed thread pool, cached thread pool 등)을 제공한다.Executor, ExecutorService, Executors는 Executor 프레임워크의 핵심이다.Executor / ExecutorService / Executors 비교Executor Executor는 병렬 실행(parallel execution)을 위해 추상화된 핵심 인터페이스이다. public interface Executor { void execute(Runnable command);} Executor는 작업(task)과 실행(execution)을 결합한 Thread(new Thread(RunnableTask()).start())와는 다르게 작업과 실행을 구분한다. 따라서, Executor는 task를 처리하기 위한 스레드를 직접 호출하는 대신 다음과 같이 사용될 수 있다. static &lt;U&gt; CompletableFuture&lt;U&gt; asyncSupplyStage(Executor e, Supplier&lt;U&gt; f) { if (f == null) throw new NullPointerException(); CompletableFuture&lt;U&gt; d = new CompletableFuture&lt;U&gt;(); e.execute(new AsyncSupply&lt;U&gt;(d, f)); return d;} ExecutorService ExecutorService는 Executor 인터페이스의 확장으로, Future 개체를 반환하고, 스레드 풀을 종료하는 등의 다양한 기능을 제공한다.public interface ExecutorService extends Executor {...} shutdown()이 호출되면 스레드 풀은 새로운 task를 수락하지 않고 보류 중인 task를 완료한다. submit()을 통해 Future 객체를 리턴한다.&lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task);&lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result);Future&lt;?&gt; submit(Runnable task); Future 객체는 비동기 실행 기능을 제공한다. 즉, task에 대한 실행이 완료될 때까지 기다릴 필요없이, 추후에 Future 객체에 결과가 있는지 확인하고 실행이 완료되면 Future.get()을 사용하여 결과를 얻을 수 있다. get()은 blocking method이다. 즉, task의 실행이 완료될 때까지 기다리고 아직 완료되지 않은 경우 결과를 사용할 수 없다. cancel()을 통해 보류 중인 실행을 취소할 수 있다. 이외에도 invokeAny(), invokeAll() 등 다양한 메서드를 제공한다. (자세한 내용은 공식 문서 참조)Executors Executors는 Collections와 유사한 유틸리티 클래스로, fixed thread pool, cached thread pool과 같은 서로 다른 유형의 스레드 풀을 만드는 팩토리 메서드를 제공한다.public static ExecutorService newFixedThreadPool(int nThreads, ThreadFactory threadFactory) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(), threadFactory);}public static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));}Executor 프레임워크 사용시 주의할 점 fixed length thread pool 사용시 스레드 풀 용량 애플리케이션이 task를 효율적으로 실행하기 위해 필요한 스레드 수를 결정하는 것은 매우 중요하다. 너무 큰 스레드 풀은 대부분의 스레드가 대기 모드에 있게되고, 이러한 스레드를 만드는데 불필요한 오버헤드가 발생한다. 너무 적으면 큐에 있는 task는 대기하는 시간이 길어지기 때문에, 애플리케이션이 응답하지 않는 것처럼 보일 수 있다. 작업 취소 후 Future.get() 메서드 호출 이미 취소된 작업의 결과를 가져오려고 하면 CancellationException이 발생한다. Future.get() 메서드로 인해 예기치 않게 긴 blocking 이를 방지하기 위해 제한 시간을 사용하는 것이 좋다. ThreadPoolExecutor ThreadPoolExecutor는 미세 조정을 위한 많은 매개변수와 후크가 있는 확장 가능한 스레드 풀 구현체이다. ExecutorService를 스레드 풀을 관리하는 역할을 정의한 것이라고 한다면, ThreadPoolExecutor는 그 역할을 구현하는 구현체라고 생각하면 될 것 같다. 출처 : https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/ThreadPoolExecutor.html 주요 구성 매개변수는 corePoolSize, maximumPoolSize, keepAliveTime이다. corePoolSize 매개변수는 인스턴스화되어 풀에 보관될 코어 스레드의 수이다. 새 작업이 들어올 때 모든 코어 스레드가 사용 중이고 내부 큐가 가득 차면 풀이 maximumPoolSize까지 커질 수 있다. 풀은 항상 내부에 유지되는 고정된 수의 코어 스레드로 구성된다. 생성된 다음 더 이상 필요하지 않을 때 종료될 수 있는 excessive 스레드로 구성되기도 한다. public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue) { this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), defaultHandler);}public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory) { this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, threadFactory, defaultHandler);}public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, RejectedExecutionHandler handler) { this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), handler);}public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) { if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler;}FixedThreadPool corePoolSize와 maximumPoolSize가 같으며 keepAliveTime이 0인 ThreadPoolExecutor. 따라서, 이 스레드 풀의 스레드 수는 항상 동일하다.public static ExecutorService newFixedThreadPool(int nThreads, ThreadFactory threadFactory) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(), threadFactory);} 아래 예시의 경우, 동시에 실행되는 작업의 수가 항상 2개보다 작거나 같으면 즉시 실행된다. 즉, 처음 두 태스크는 한 번에 실행되고 세 번째 태스크는 대기열에서 대기해야 한다. ThreadPoolExecutor executor = (ThreadPoolExecutor) Executors.newFixedThreadPool(2);executor.submit(() -&gt; { Thread.sleep(1000); return null;});executor.submit(() -&gt; { Thread.sleep(1000); return null;});executor.submit(() -&gt; { Thread.sleep(1000); return null;});assertEquals(2, executor.getPoolSize());assertEquals(1, executor.getQueue().size());CachedThreadPool corePoolSize는 0, maximumPoolSize는 Integer.MAX_VALUE, keepAliveTime은 60초인 ThreadPoolExecutor. 즉, 스레드 풀이 모든 task를 수용할 수 있도록 제한 없이 커질 수 있음을 의미한다. 또한 스레드가 60초 동안 사용하지 않으면 폐기된다. CachedThreadPool은 애플리케이션이 주로 short-living task를 처리하는 경우 활용한다. 내부적으로 SynchronousQueue가 사용되므로 대기열 크기는 항상 0이다. SynchronousQueue에서는 삽입 및 제거 작업 쌍이 항상 동시에 수행되기 때문에, 실제로 아무것도 포함하지 않는다. public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());}public static ExecutorService newCachedThreadPool(ThreadFactory threadFactory) { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;(), threadFactory);}ThreadPoolExecutor executor = (ThreadPoolExecutor) Executors.newCachedThreadPool();executor.submit(() -&gt; { Thread.sleep(1000); return null;});executor.submit(() -&gt; { Thread.sleep(1000); return null;});executor.submit(() -&gt; { Thread.sleep(1000); return null;});assertEquals(3, executor.getPoolSize());assertEquals(0, executor.getQueue().size());SingleThreadExecutor Executors.newSingleThreadExecutor()는 단일 스레드를 포함하는 또 다른 일반적인 형태의 ThreadPoolExecutor를 만든다. SingleThreadExecutor는 이벤트 루프를 만드는 데 이상적이다. corePoolSize, maximumPoolSize는 1이고 keepAliveTime은 0이다.public static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));}public static ExecutorService newSingleThreadExecutor(ThreadFactory threadFactory) { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(), threadFactory));}AtomicInteger counter = new AtomicInteger();ExecutorService executor = Executors.newSingleThreadExecutor();executor.submit(() -&gt; { counter.set(1);});executor.submit(() -&gt; { counter.compareAndSet(1, 2);});ScheduledThreadPoolExecutor ScheduledThreadPoolExecutor는 ThreadPoolExecutor 클래스를 상속받고 ScheduledExecutorService 인터페이스도 구현하여 부가적인 기능을 제공한다. schedule() 메서드를 사용하면 지정된 지연 후 작업을 한 번 실행할 수 있다. scheduleAtFixedRate() 메서드를 사용하면 지정된 초기 지연 후에 작업을 실행한 다음 특정 기간 동안 반복 실행할 수 있다. 즉, task 수행 시작 시간은 (initialDelay + delay), (initialDelay + 2 * period), … 이런식으로 계산된다. scheduleWithFixedDelay() 메서드는 지정된 태스크를 반복적으로 실행한다는 점에서 scheduleAtFixedRate()와 유사하다. 하지만, delay는 이전 task의 종료와 다음 task의 시작 사이에서 측정된다. 즉, task 수행 시작 시간은 (이전 task의 끝나는 시점 + delay)가 된다. public class ScheduledThreadPoolExecutor extends ThreadPoolExecutor implements ScheduledExecutorService {...}public ScheduledThreadPoolExecutor(int corePoolSize) { super(corePoolSize, Integer.MAX_VALUE, DEFAULT_KEEPALIVE_MILLIS, MILLISECONDS, new DelayedWorkQueue());}public ScheduledThreadPoolExecutor(int corePoolSize, ThreadFactory threadFactory) { super(corePoolSize, Integer.MAX_VALUE, DEFAULT_KEEPALIVE_MILLIS, MILLISECONDS, new DelayedWorkQueue(), threadFactory);}public ScheduledThreadPoolExecutor(int corePoolSize, RejectedExecutionHandler handler) { super(corePoolSize, Integer.MAX_VALUE, DEFAULT_KEEPALIVE_MILLIS, MILLISECONDS, new DelayedWorkQueue(), handler);}public ScheduledThreadPoolExecutor(int corePoolSize, ThreadFactory threadFactory, RejectedExecutionHandler handler) { super(corePoolSize, Integer.MAX_VALUE, DEFAULT_KEEPALIVE_MILLIS, MILLISECONDS, new DelayedWorkQueue(), threadFactory, handler);}public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) { return new ScheduledThreadPoolExecutor(corePoolSize);}public static ScheduledExecutorService newScheduledThreadPool( int corePoolSize, ThreadFactory threadFactory) { return new ScheduledThreadPoolExecutor(corePoolSize, threadFactory);}ScheduledExecutorService executor = Executors.newScheduledThreadPool(5);executor.schedule(() -&gt; { System.out.println(\"Hello World\");}, 500, TimeUnit.MILLISECONDS); 다음 코드는 500밀리초 지연 후 작업을 실행한 후 100밀리초마다 반복하는 방법을 보여준다. 또한, 작업을 예약한 후 CountDownLatch lock을 사용하여 작업이 세 번 실행될 때까지 기다린 후, Future.cancel() 메서드를 사용하여 작업을 취소한다.CountDownLatch lock = new CountDownLatch(3);ScheduledExecutorService executor = Executors.newScheduledThreadPool(5);ScheduledFuture&lt;?&gt; future = executor.scheduleAtFixedRate(() -&gt; { System.out.println(\"Hello World\"); lock.countDown();}, 500, 100, TimeUnit.MILLISECONDS);lock.await(1000, TimeUnit.MILLISECONDS);future.cancel(true);더 공부할 부분 Fork/Join Framework참고자료 https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/Executor.html https://stackoverflow.com/questions/26938210/executorservice-vs-casual-thread-spawner https://stackoverflow.com/questions/3984076/what-are-the-advantages-of-using-an-executorservice https://www.baeldung.com/thread-pool-java-and-guava https://www.baeldung.com/java-executor-service-tutorial https://javarevisited.blogspot.com/2017/02/difference-between-executor-executorservice-and-executors-in-java.html#axzz72wOdvf6F" }, { "title": "Sync/Async, Blocking/Non-Blocking", "url": "/posts/sync-async-block-nonblock/", "categories": "", "tags": "", "date": "2021-08-07 00:29:00 +0900", "snippet": "들어가기 전항상 동기적으로만 프로그래밍을 해왔었지만, 비동기적으로 프로그래밍 하는 것도 반드시 알고있어야 한다고 생각한다. (reactive app ?? -&gt; Spring Webflux 등 ?)비동기면 non-blocking, 동기면 blocking 이라고 생각했는데, 잘못 생각하고있었다.제대로 알아보자.Synchronous vs Asynchro...", "content": "들어가기 전항상 동기적으로만 프로그래밍을 해왔었지만, 비동기적으로 프로그래밍 하는 것도 반드시 알고있어야 한다고 생각한다. (reactive app ?? -&gt; Spring Webflux 등 ?)비동기면 non-blocking, 동기면 blocking 이라고 생각했는데, 잘못 생각하고있었다.제대로 알아보자.Synchronous vs Asynchronous Synchronous : 동시 발생[존재]하는, “synchronize(일치시키다, 동시에 일어나다)”는 [syn 같이] + [chron 시간] + [ze 동사]로 만들어지는 단어예요.Asynchronous : 동시에 존재[발생]하지 않는–&gt; 영어 뜻만보면 sync가 여러개를 동시에 실행시키는 느낌쓰 ??–&gt; 동시에 발생한다는게 (끝남과) 동시에 발생 ??Synchronous, Asynchronous는 형용사. 그럼 컴퓨터 세계에선 뒤에 어떤 명사랑 자주 쓰일까 ?Synchronous Programming, Asynchronous Programming ?Synchronous Execution, Asynchronous Execution ?task를 처리하는 방식 ??clock을 알아야하는건가 ??–&gt; 스레드, 프로세스 입장에서 생각 ??https://www.makeuseof.com/synchronous-asynchronous-programming-different/Blocking vs Non-BlockingSynchronous / Asynchronous 와 Blocking / Non-Blocking 관계sync면 blocking, async면 non-blocking 아닌가 ??https://www.cs.unc.edu/~dewan/242/s07/notes/ipc/node9.html다양한 관점 관심사의 차이 Synchronous/Asynchronous는 호출되는 함수의 작업 완료 여부를 누가 신경쓰냐가 관심사다. 호출되는 함수에게 callback을 전달해서, 호출되는 함수의 작업이 완료되면 호출되는 함수가 전달받은 callback을 실행하고, 호출하는 함수는 작업 완료 여부를 신경쓰지 않으면 Asynchronous다. 호출하는 함수가 호출되는 함수의 작업 완료 후 리턴을 기다리거나, 또는 호출되는 함수로부터 바로 리턴 받더라도 작업 완료 여부를 호출하는 함수 스스로 계속 확인하며 신경쓰면 Synchronous다. Blocking/NonBlocking은 호출되는 함수가 바로 리턴하느냐 마느냐가 관심사다. 호출된 함수가 바로 리턴해서 호출한 함수에게 ‘제어권’을 넘겨주고, 호출한 함수가 다른 일을 할 수 있는 기회를 줄 수 있으면 NonBlocking이다. 그렇지 않고 호출된 함수가 자신의 작업을 모두 마칠 때까지 호출한 함수에게 제어권을 넘겨주지 않고 대기하게 만든다면 Blocking이다. 입장의 차이 Blocking/NonBlocking은 호출한 입장에서의 특징 Sync/Async는 처리되는 방식의 특징 동작관점의 차이 Non-Blocking은 제어문 수준에서 지체없이 반환하는 것 Asynchronous는 별도의 쓰레드로 빼서 실행하고, 완료되면 호출하는 측에 알려주는 것 4. Synchronous VS Asynchronous두 가지 이상의 대상(메서드, 작업, 처리 등)과 이를 처리하는 시간으로 구분한다.Synchronous: 호출된 함수의 리턴하는 시간과 결과를 반환하는 시간이 일치하는 경우Asynchronous: 호출된 함수의 리턴하는 시간과 결과를 반환하는 시간이 일치하지 않는 경우 Blocking VS Non-Blocking호출되는 대상이 직접 제어할 수 없는 경우 이를 구분할 수 있다.Blocking: 직접 제어할 수 없는 대상의 작업이 끝날 때까지 기다려야 하는 경우Non-Blocking: 직접 제어할 수 없는 대상의 작업이 완료되기 전에 제어권을 넘겨주는 경우 5. Blocking VS Non-Blocking블럭킹과 논블럭킹은 대기큐와 호출 결과 시점으로 구분할 수 있다. 설명하기 쉽게 프로그램 A와 B가 있다고 가정하겠다. 프로세스는 프로그램 A에서 B를 호출하는 순서로 진행한다.Blocking프로그램 A 에서는 프로그램 B 로직이 수행 완료될 때 까지 대기큐에 들어가 로직이 완료된 이후에나 대기큐에서 프로그램 A가 반환되어 이후 로직을 수행할 수 있다.프로그램 B는 호출 결과를 로직 수행 완료 이후에 프로그램 A에게 돌려주게 된다.Non Blocking프로그램 A는 프로그램 B를 호출한 이후에도 제어권을 가지고 있어 대기큐에 들어가지 않고 다른 로직을 수행할 수 있다.프로그램 B가 호출된 순간 호출되었다는 결과만 프로그램 A에게 돌려준다. Synchronous VS Asynchronous동기와 비동기는 호출한 결과의 완료 여부를 확인 하는가에 따라 구분할 수 있다.Sync프로그램 B가 완료할 때까지 프로그램 A는 기다리게 되므로 아무런 로직도 수행하지 못한다.ASync프로그램 B를 호출한 이후에 프로그램 A는 프로그램 B의 완료 여부를 기다리지 않고 다음 로직을 수행한다.sync vs async Synchronous/Asynchronous는 호출되는 함수의 작업 완료 여부를 누가 신경쓰냐가 관심사다. 호출되는 함수에게 callback을 전달해서, 호출되는 함수의 작업이 완료되면 호출되는 함수가 전달받은 callback을 실행하고, 호출하는 함수는 작업 완료 여부를 신경쓰지 않으면 Asynchronous다. 호출하는 함수가 호출되는 함수의 작업 완료 후 리턴을 기다리거나, 또는 호출되는 함수로부터 바로 리턴 받더라도 작업 완료 여부를 호출하는 함수 스스로 계속 확인하며 신경쓰면 Synchronous다. 동기는 두 가지 이상의 대상(함수, 애플리케이션 등)이 서로 시간을 맞춰 행동하는 것이다. 예를들어 호출한 함수가 호출된 함수의 작업이 끝나서 결과값을 반화하기를 기다리거나, 지속적으로 호출된 함수에게 확인 요청을하는 경우가 있다. A와 B가 시작 시간 또는 종료 시간이 일치하면 동기이다. 예를 들어 A, B 쓰레드가 동시에 작업을 시작하는 경우 (예를 들면 자바에서 CyclicBarrier) 메서드 리턴 시간(A)과 결과를 전달받는 시간(B)이 일치하는 경우 A가 끝나는 시간과 B가 시작하는 시간이 같으면 동기이다. 예를 들어 자바에서 synchronized와 BlockingQueue가 위와 같은 경우이다. 비동기는 동기와 반대로 대상이 서로 시간을 맞추지 않는 것을 말한다. 예를 들어 호출하는 함수가 호출되는 함수에게 작업을 맡겨놓고 신경을 쓰지 않는 것을 말한다. 비동기에 대한 자세한 예제는 아래에서 다룰 것이다.blocking vs non-blocking Blocking/NonBlocking은 호출되는 함수가 바로 리턴하느냐 마느냐가 관심사다. 호출된 함수가 바로 리턴해서 호출한 함수에게 ‘제어권’을 넘겨주고, 호출한 함수가 다른 일을 할 수 있는 기회를 줄 수 있으면 NonBlocking이다. 그렇지 않고 호출된 함수가 자신의 작업을 모두 마칠 때까지 호출한 함수에게 제어권을 넘겨주지 않고 대기하게 만든다면 Blocking이다. 블럭킹과 논블럭킹은 대기큐와 호출 결과 시점으로 구분할 수 있다. 블록킹/논블록킹을 동기/비동기와 같이 생각하는 경우가 많은데, 이는 서로 관점이 다르다. 블록킹/논블록킹은 직접 제어할 수 없는 대상을 처리하는 방법에 따라 나눈다. 직접 제어할 수 없는 대상은 대표적으로 IO, 멀티쓰레드 동기화가 있다.asynchronous programming vs multithreading programming IntroductionIn this tutorial, we’ll show a simple explanation for asynchronous programming and multithreading programming. Then, we’ll discuss the differences between them. What Is Asynchronous Programming?An asynchronous model allows multiple things to happen at the same time. When your program calls a long-running function, it doesn’t block the execution flow, and your program continues to run. When the function finishes, the program knows and gets access to the result (if there’s a need for that). Let’s take an example of a program that fetches two files over a network and combines them:In an asynchronous system, the solution is to start an additional thread of control. The first thread fetches the first file, and the second thread fetches the second file without waiting for the first thread to finish, and then both threads wait for their results to come back, after which they resynchronize to combine their results.Another example with a single-thread approach is a program that requests a file from the OS and needs to make a mathematical operation.In an asynchronous system, the program asks the OS for the file and returns the control to the mathematical operation to be executed on the CPU, while waiting for the file.One approach to asynchronous programming is to make functions that perform a slow action and take an extra argument, a callback function. The action is started, and when it finishes, the callback function is called with the result. What Is Multithreading Programming?Multithreading refers to the concurrent/parallel execution of more than one sequential set (thread) of instructions.On a single processor, multithreading gives the illusion of running in parallel. In reality, the processor is switching by using a scheduling algorithm. Or, it’s switching based on a combination of external inputs (interrupts) and how the threads have been prioritized.On multiple processor cores, threads are truly parallel. Individual microprocessors work together to achieve the result more efficiently. There are multiple parallel, concurrent tasks happening at once.A basic example of multithreading is downloading two files from two different tabs in a web browser. Each tab uses a new thread to download the requested file. No tab waits for the other one to finish, they are downloading concurrently.The following picture shows a simple explanation of concurrent execution of a multithreaded application: Asynchronous vs MultithreadingFrom the definitions we just provided, we can see that multithreading programming is all about concurrent execution of different functions. Async programming is about non-blocking execution between functions, and we can apply async with single-threaded or multithreaded programming.So, multithreading is one form of asynchronous programming.Let’s take a simple analogy; you have a friend, and you decided to make dinner together.Async is when you say to your friend, “You go to the store and buy pasta. Let me know when you get back, to make dinner together. Meanwhile, I’ll prepare sauce and drinks.”Threading is, “You boil the water. I’ll heat the tomato sauce. While the water is boiling, ask me and I’ll put the pasta in. When the sauce is hot, you can add cheese. When both are done, I’ll sit down and you serve dinner. Then we eat.”. In the threading analogy, we can see the sequence of “When, Do” events, which represent the sequential set of instructions per each person (thread).From that analogy, we can conclude that Multithreading is about workers, Asynchronous is about tasks. Which One To Use?Choosing between the two programming models depends mainly on performance.Given all possible combinations between sync/async and single/multi-threading, which model should perform better?In a nutshell, for large scale applications with a lot of I/O operations and different computations, using asynchronous multithreading programming flow, will utilize the computation resources, and take care of non-blocking functions. This is the programming model of any OS!With more power, comes more responsibility! So if we decided to implement this model, we have to take care of different issues like race condition, deadlocks, shared resources, and callbacks events.참고자료 https://www.baeldung.com/cs/async-vs-multi-threading http://homoefficio.github.io/2017/02/19/Blocking-NonBlocking-Synchronous-Asynchronous/ https://velog.io/@codemcd/Sync-VS-Async-Blocking-VS-Non-Blocking-sak6d01fhx https://incheol-jung.gitbook.io/docs/q-and-a/java/or-or-or" }, { "title": "(미해결) 카프카 프로듀서 재송신 테스트", "url": "/posts/kafka-producer-message-retry-test/", "categories": "", "tags": "", "date": "2021-07-31 14:00:00 +0900", "snippet": "상황카프카의 전달 보증에 대해 공부했고 at most once, at least once, exactly once 세 종류가 있다는 것을 알게되었다.각 전달 보증 수준에 대해 공부하면서, 브로커에서 응답하는 ack가 오지않는(유실되는) 경우 프로듀에서 메시지를 재전송 하는 부분에 대해다음과 같은 테스트를 해보고 싶었다. at most once ...", "content": "상황카프카의 전달 보증에 대해 공부했고 at most once, at least once, exactly once 세 종류가 있다는 것을 알게되었다.각 전달 보증 수준에 대해 공부하면서, 브로커에서 응답하는 ack가 오지않는(유실되는) 경우 프로듀에서 메시지를 재전송 하는 부분에 대해다음과 같은 테스트를 해보고 싶었다. at most once ack가 오지 않아도 메시지를 재전송하지 않는다. at least once ack가 오지 않으면 메시지를 재전송한다. 이미 브로커에서 저장된 메시지에 대해 재전송 하게되면, 메시지가 중복으로 저장될 수 있다. exactly once ack가 오지 않으면 메시지를 재전송한다. 이미 브로커에서 저장된 메시지에 대해 재전송 하더라도, 브로커에서는 중복 메시지 처리 알고리즘을 활용해 메시지를 중복 저장하지 않는다. 하지만, 생각처럼 상황을 시뮬레이션 할 수가 없었고 어떤 부분을 더 공부해야 원하는 테스트를 제대로 할 수 있는지 모르겠다…일단 시도해본 것과 실패에 대한 원인을 적어보고, 앞으로 더 공부하면서 어떤걸 몰랐고 잘못 생각하고 있던건지 깨닫게 될 때, 다시 제대로 테스트해서 이 글을 보완할 예정이다.시도해본 것들Case 1. at least once 수준에서의 재송신 환경 및 옵션 세팅 브로커 1대 / acks = 1(default) / request.timeout.ms = 1000 (1초) / retries = 10 토픽 생성 ./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic 토픽명 테스트 시나리오 브로커를 끈다. 프로듀서에서 메시지를 송신한다. request.timeout.ms에 정의된 시간 내에 ack 응답이 오지 않으면 10번 재송신된다. send 메서드 두 번째 파라미터인 Callback 인터페이스를 구현하여, 재송신된 10건의 메시지에 대한 exception이 10번 뜨는지 확인한다. 테스트 결과 exception : org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms. 프로듀서 코드import org.apache.kafka.clients.producer.*;import org.apache.kafka.common.serialization.StringSerializer;import java.util.Properties;import java.util.Scanner;public class SampleProducer { private static final String TOPIC_NAME = \"jaeyoon\"; private static final String FIN_MESSAGE = \"exit\"; public static void main(String[] args) { Properties properties = new Properties(); properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\"); properties.put(ProducerConfig.ACKS_CONFIG, \"1\"); properties.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, 1000); properties.put(ProducerConfig.RETRIES_CONFIG, 10); properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); KafkaProducer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(properties); while(true) { Scanner sc = new Scanner(System.in); System.out.print(\"Input &gt; \"); String message = sc.nextLine(); ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(TOPIC_NAME, message); try { producer.send(record, (metadata, exception) -&gt; { System.out.println(\"metadata : \"+metadata); if (exception != null) { // some exception System.out.println(\"exception : \"+exception); } }); } catch (Exception e) { // exception } finally { producer.flush(); } if(message.equals(FIN_MESSAGE)) { producer.close(); break; } } }}결과 분석 브로커가 꺼져있었기 때문에 metadata를 제한 시간내에 가져오지 못해서 발생한 에러이다. send 메서드는 실행 후 max.block.ms(default 60000)에 정의된 만큼 metadata fetch 및 버퍼 할당을 기다리는 총 시간을 제한한다. 해당 시간이 초과하면 TimeoutException이 발생한다. 메타데이터는 언제 가져오나 ? 프로듀서가 처음으로 메타데이터 요청을 하는 때는 클라이언트 구성에서 설정한 부트스트랩 서버(bootstrap.server)에 연결할 때다. 물론, 한 대 이상의 브로커일 수 있지만 반드시 클러스터에 있는 모든 브로커일 필요는 없다(따라서 메타데이터 요청은 각 브로커에 대한 것이 아님). 메타데이터를 통해, 프로듀서는 자신이 메시지를 보내고자 하는 토픽이 어디에 있는지 정보를 얻는다. 또는, 토픽의 리더 레플리카를 가진 브로커에 연결 오류가 발생하면 메타데이터 요청을 수행할 수 있다. 이 경우 브로커에 연결하기 위해(다른 토픽에 대해 아직 연결되지 않은 경우) 어떤 브로커가 새 리더인지 알아야 한다. Case 2. 1번 케이스에서 메타데이터 얻어온 후 브로커 종료 환경 및 옵션 세팅 (case1과 동일) 브로커 1대 / acks = 1(default) / request.timeout.ms = 1000 (1초) / retries = 10 테스트 시나리오 주키퍼와 브로커를 켜고 메시지를 한 번 송신한다.(메타데이터를 받아온다.) 브로커를 끈다. 메시지를 송신한다. request.timeout.ms에 정의된 시간 내에 ack 응답이 오지 않으면 10번 재송신된다. send 메서드 두 번째 파라미터인 Callback 인터페이스를 구현하여, 재송신된 10건의 메시지에 대한 exception이 10번 뜨는지 확인한다. 프로듀서 코드 (case1과 동일) 테스트 결과Input &gt; byeexception : org.apache.kafka.common.errors.TimeoutException: Expiring 1 record(s) for jaeyoon-0: 1018 ms has passed since batch creation plus linger time결과 분석 해당 에러는 버퍼에 쌓인 레코드가 request.timeout.ms(default 30000)에 세팅된 시간 + linger.ms (default 0)에 세팅된 시간을 초과할 때 까지 브로커에 송신되지 않으면 발생한다. 일반적으로 레코드가 버퍼에 쌓이는 속도 &gt;&gt;&gt; 레코드를 송신하는 속도인 경우 레코드가 버퍼에서 대기하는 시간이 길어져 이러한 에러가 발생할 수 있다. 위 테스트의 경우에는 프로듀서가 송신할 브로커를 찾지 못해 버퍼에 레코드가 쌓여있다가 request.timeout.ms에 세팅된 1초를 넘겨 에러가 발생했다. linger.ms 프로듀서는 해당 옵션에 세팅된 시간만큼(default 0) 지연 후 브로커로 메시지를 송신한다. 즉, 버퍼에 쌓인 레코드를 즉시 내보내는 것이 아니라, 다른 레코드가 함께 전송될 수 있도록(batch 처리) 기다린다. batch 처리 레코드는 브로커에게 전송하기 위한 묶음으로(batch) 그룹화되어 메시지당 전송 오버헤드를 줄이고 처리량을 증가시킨다. send 메서드를 호출하면 브로커에게 보낼 수 있도록 ProducerRecord가 내부 버퍼에 저장된다. send 메서드는 전송 여부와 관계없이 ProducerRecord가 버퍼링되면 즉시 return 한다. 배치가 제한 시간보다 오래 대기한 경우 예외가 발생하며, 해당 배치의 레코드는 전송 대기열에서 제거된다. (위 테스트의 경우) Case3. min.insync.replicas 활용하여 에러 발생시키기 환경 및 옵션 세팅 브로커 3대 / replication.factor = 3 / min.insync.replicas = 2 / acks = all / retries = 5 토픽 생성 ./bin/kafka-topics.sh --create --zookeeper localhost:2181 --config min.insync.replicas=2 --replication-factor 3 --partitions 1 --topic 토픽명 테스트 시나리오 주피커와 브로커를 3대를 켠다. 메시지를 한 번 송신하고 정상적으로 처리되는지 확인한다. 브로커 두 대를 다운시킨다. 브로커가 한 대만 남아있으므로 min.insync.replicas=2를 만족할 수 없기 때문에 에러가 발생할 것이고 메시지는 재전송 될 것이다. send 메서드 두 번째 파라미터인 Callback 인터페이스를 구현하여, 재송신된 10건의 메시지에 대한 exception이 10번 뜨는지 확인한다. 테스트 결과 프로듀서 측 Input &gt; down test!exception : org.apache.kafka.common.errors.NotEnoughReplicasException: Messages are rejected since there are fewer in-sync replicas than required. 브로커 측 (1대 남은 브로커) 첫 번째 에러 발생 후, 프로듀서 측에 세팅한 대로 5번 재시도 하는 것을 볼 수 있다. [2021-08-02 04:05:07,937] ERROR [ReplicaManager broker=0] Error processing append operation on partition dawn-0 (kafka.server.ReplicaManager)org.apache.kafka.common.errors.NotEnoughReplicasException: The size of the current ISR Set(0) is insufficient to satisfy the min.isr requirement of 2 for partition dawn-0[2021-08-02 04:05:08,043] ERROR [ReplicaManager broker=0] Error processing append operation on partition dawn-0 (kafka.server.ReplicaManager)org.apache.kafka.common.errors.NotEnoughReplicasException: The size of the current ISR Set(0) is insufficient to satisfy the min.isr requirement of 2 for partition dawn-0[2021-08-02 04:05:08,150] ERROR [ReplicaManager broker=0] Error processing append operation on partition dawn-0 (kafka.server.ReplicaManager)org.apache.kafka.common.errors.NotEnoughReplicasException: The size of the current ISR Set(0) is insufficient to satisfy the min.isr requirement of 2 for partition dawn-0[2021-08-02 04:05:08,258] ERROR [ReplicaManager broker=0] Error processing append operation on partition dawn-0 (kafka.server.ReplicaManager)org.apache.kafka.common.errors.NotEnoughReplicasException: The size of the current ISR Set(0) is insufficient to satisfy the min.isr requirement of 2 for partition dawn-0[2021-08-02 04:05:08,363] ERROR [ReplicaManager broker=0] Error processing append operation on partition dawn-0 (kafka.server.ReplicaManager)org.apache.kafka.common.errors.NotEnoughReplicasException: The size of the current ISR Set(0) is insufficient to satisfy the min.isr requirement of 2 for partition dawn-0[2021-08-02 04:05:08,465] ERROR [ReplicaManager broker=0] Error processing append operation on partition dawn-0 (kafka.server.ReplicaManager)org.apache.kafka.common.errors.NotEnoughReplicasException: The size of the current ISR Set(0) is insufficient to satisfy the min.isr requirement of 2 for partition dawn-0 프로듀서 코드import org.apache.kafka.clients.producer.*;import org.apache.kafka.common.serialization.StringSerializer;import java.util.Properties;import java.util.Scanner;public class SampleProducer { private static final String TOPIC_NAME = \"dawn\"; private static final String FIN_MESSAGE = \"exit\"; public static void main(String[] args) { Properties properties = new Properties(); properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092,localhost:9093,localhost:9094\"); properties.put(ProducerConfig.ACKS_CONFIG, \"all\"); properties.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, 1000); properties.put(ProducerConfig.RETRIES_CONFIG, 5); properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); KafkaProducer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(properties); while (true) { Scanner sc = new Scanner(System.in); System.out.print(\"Input &gt; \"); String message = sc.nextLine(); ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(TOPIC_NAME, message); try { producer.send(record, (metadata, exception) -&gt; { if (exception != null) { // some exception System.out.println(\"exception : \" + exception); } }); } catch (Exception e) { // exception } finally { producer.flush(); } if (message.equals(FIN_MESSAGE)) { producer.close(); break; } } }}결과 분석 브로커 측에서 프로듀서에 세팅한 재송신 횟수 만큼의 예외 메시지를 출력한다. 그렇다는건 초기에 세팅한 횟수만큼 메시지를 재전송 했다는 것이다. 재전송을 하긴 했는데, 프로듀서 측에서의 콜백은 한 번만 발생했다. 재전송이라는게 send 메서드가 여러번 호출되는 개념이 아니라, send를 한 번 호출하면 그 안에서 필요한 경우 자동으로 재전송이 되고 그에 대한 최종적인 결과를 콜백 메서드에 넘겨주는건가 ? 정확히 send 메서드 내부의 코드를 이해하지는 못했지만, 이에 대한 의문은 RetriableException에 대해 공부하면서 조금은 해소되었다. RetriableException 프로듀서의 에러 처리에는 두 가지가 있다. 프로듀서가 자동으로 처리하는 에러(=재시도 가능한 에러) 개발자가 프로듀서 라이브러리를 사용해서 처리해야하는 에러 예를 들어, LEADER_NOT_AVAILABE 같은 에러는 리더가 다시 선출되면 해결되는 문제이므로 재시도 가능한 에러(retriable error) 라고 한다. 반면, INVALD_CONFIG에러 (구성이 잘못되어 발생한 에러)의 경우는 아무리 메시지를 재전송해도 구성이 변경되지 않으므로 재시도 불가능한 에러이다. 재시도 가능한 에러는 카프카 프로듀서 객체가 알아서 해주므로 우리 코드에서 직접 처리할 필요가 없다. 위에서 발생한 NotEnoughReplicasException 또한 재시도 가능한 에러이다.ReplicaManager 테스트 결과의 브로커 측 예외 메시지를 보면 kafka.server.ReplicaManager에서 에러를 발생시키는 것 같아 찾아보니 다음과 같은 구성으로 되어있는 것 같다. “ReplicaManager manages log replicas using the LogManager.” 라고 설명 되어있는데, 카프카 서버측 로그를 담당하는 역할을 하는 것 같다. 출처 : https://jaceklaskowski.gitbooks.io/apache-kafka/content/kafka-server-ReplicaManager.html Case4. at most once vs at least once 메시지 유실 정도 비교 환경 및 옵션 세팅 브로커 3대 / replication.factor = 3 / min.insync.replicas = 2 / retries = Integer.MAX acks = 0 (at most once) acks = 1 (at least once) 토픽 생성 ./bin/kafka-topics.sh --create --zookeeper localhost:2181 --config min.insync.replicas=2 --replication-factor 3 --partitions 1 --topic 토픽명 테스트 시나리오 주키퍼와 브로커 3대를 켠다. ./bin/kafka-topics.sh --describe --topic 토픽명 --zookeeper localhost:2181 명령어를 통해 리더 레플리카를 파악한다. ./bin/kafka-consumer-groups.sh --bootstrap-server localhost:포트번호 --group dawn --describe 명령어를 통해 브로커 3대의 LOG-END-OFFSET을 파악한다. 프로듀서에서 10만건의 메시지를 송신하게 하고 도중에 리더 레플리카를 가진 브로커를 다운시킨다. 메시지 송신이 완료되고 난 뒤, 새로운 리더 레플리카를 파악하고 3번 명령어 통해 현재 LOG-END-OFFSET을 확인한다. 새로운 리더가 선출될 때 LeaderNotAvailableException가 발생할 것이고, 그로 인해 처리되지 못한 메시지가 얼마나 차이가 나는지 확인해본다. at most once인 경우에는 리더 선출로 인해 메시지 처리가 되지 않더라도 재송신 하지 않기 때문에 유실되는 메시지가 있을 것이다. at least once인 경우에 리더 선출로 인해 에러가 나는 경우 메시지를 재송할 것이므로 at most once 경우보다는 더 많은 메시지가 처리될 것이다. 테스트 결과 (at most once) before GROUP TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAGdawn dawn 0 5 5 0 after GROUP TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAGdawn dawn 0 100005 100005 0 테스트 결과 (at least once) before GROUP TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAGdawn dawn 0 100005 100005 0 after GROUP TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAGdawn dawn 0 200005 200005 0 프로듀서 코드package basic;import org.apache.kafka.clients.producer.*;import org.apache.kafka.common.serialization.StringSerializer;import java.util.Properties;import java.util.Scanner;public class SampleProducer { private static final String TOPIC_NAME = \"dawn\"; private static final String FIN_MESSAGE = \"exit\"; public static void main(String[] args) { Properties properties = new Properties(); properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092,localhost:9093,localhost:9094\"); properties.put(ProducerConfig.ACKS_CONFIG, \"1\"); // at most once : 0 , at least once : 1 properties.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE); properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); KafkaProducer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(properties); while (true) { Scanner sc = new Scanner(System.in); System.out.print(\"Input &gt; \"); String message = sc.nextLine(); ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(TOPIC_NAME, message); try { for(int i=0; i&lt;100000; i++) { producer.send(record, (metadata, exception) -&gt; { if (exception != null) { // some exception System.out.println(\"exception : \" + exception); } }); if(i%10000==0) { Thread.sleep(1000); } } } catch (Exception e) { // exception } finally { producer.flush(); } if (message.equals(FIN_MESSAGE)) { producer.close(); break; } } }}결과 분석 두 가지 전달 보증 수준 모두에서 메시지 유실은 없었다. 이렇게 테스트하는게 아닌가 … ?? 리더가 선출되는 과정과 그 때의 메시지 처리에 대해 좀 더 자세하게 공부해보자Case 별로 배운 부분추측이나 확실히 모르겠는건 추후 보완 예정 !Case1~2 브로커에서 ack 응답이 오지 않는 상황을 브로커를 다운시킴으로써 시뮬레이션 하려고 했으나, 브로커가 꺼져있으면 프로듀서에서 브로커로 송신 자체가 되지 않는다. ack를 유실하는 경우를 어떻게 시뮬레이션 할 수 있을까 ??Case3 재시도 가능한 에러는 프로듀서 내부에서 자동으로 재전송해준다. (해당 코드는 어디에 있을까 ?) 추측 : 재전송이 여러번 되더라도 send 메서드에 대한 콜백은 한 번만 발생하는 것 같다.참고자료 https://kafka.apache.org/documentation/#producerconfigs https://stackoverflow.com/questions/56794122/metadata-requests-in-kafka-producer https://stackoverflow.com/questions/46750420/kafka-producer-error-expiring-10-records-for-topicxxxxxx-6686-ms-has-passed https://bistros.tistory.com/152 네하 나크헤데 외 2인 『카프카 핵심 가이드』, 제이펍(2018), chapter6" }, { "title": "카프카 전달보증 구현하기 - Exactly Once", "url": "/posts/kafka-delivery-guarantees-exactly-once/", "categories": "", "tags": "", "date": "2021-07-29 14:00:00 +0900", "snippet": "Exactly Once 메시지 유실없이 정확히 한 번 전달한다 ?? –&gt; 전달보다는 ‘브로커에서’ 정확히 한번만 ‘처리된다’라는게 맞는거같다 -&gt; 근데 컨슈머측에서 처리하다 에러나면, 재송신하는거 아닌가 ? -&gt; 즉, at least once랑 같은데 메시지 중복처리만 안되게하는거 아닌가 ??v -&gt; 아닌거 같다, 만약 브로커에...", "content": "Exactly Once 메시지 유실없이 정확히 한 번 전달한다 ?? –&gt; 전달보다는 ‘브로커에서’ 정확히 한번만 ‘처리된다’라는게 맞는거같다 -&gt; 근데 컨슈머측에서 처리하다 에러나면, 재송신하는거 아닌가 ? -&gt; 즉, at least once랑 같은데 메시지 중복처리만 안되게하는거 아닌가 ??v -&gt; 아닌거 같다, 만약 브로커에서 프로듀서로부터 받은 메시지 저장했는데 ack를 다시 프로듀서로 못 보내서프로듀서에서 재송신오는 경우, transaction id 통해 메시지 중복처리 방지해줄거같다.-&gt; 따라서, 컨슈머측에서는 중복된 메시지를 consume 하는 일은 없는것 ?? 카프카는 초기에 at-most-once와 at-least-once 수준의 전달 보증만 지원했었다. 하지만, 카프카의 유용성이 높아지면서 Exactly Once 수준의 전달을 보증하고자 하는 요구가 높아졌다. 이를 위해 카프카에 트랜잭션 개념이 도입되었다.exactly-once 전달 보증은 0.11 버전부터(Released June 28, 2017, 참고 ) idempotent와 transactional producers 두 가지 옵션을 통해 지원한다. 결과적으로 메시지는 한 번만 전달되며 유실되지 않는다. exactly-once delivery는 프로듀서부터 컨슈머까지 연결되는 파이프라인의 처리를 뜻합니다. 기존 프로듀서의 경우 트랜잭션처리를 하지 않으면 카프카 클러스터에 두번이상 데이터가 저장될 수 있습니다.데이터가 클러스터에 저장되었으나 ack가 유실되어 프로듀서가 재처리하는 경우가 대표적입니다. 결과적으로 카프카 트랜잭션 처리를 하더라도 컨슈머가 중복해서 데이터 처리하는 것에 대해 보장하지 않으므로,컨슈머의 중복처리는 따로 로직을 작성해야합니다. enable.idempotence를 true로 하게 되면 카프카는 중복 메시지 제거 알고리즘에서 transactional.id를 사용하여 프로듀서가 보내는 모든 메시지의 멱등성(idempotence)을 보장한다. 간단히 말해, 프로듀서가 같은 메시지를 두 번 이상 송신하게 되면, Kafka는 이러한 설정을 통해 알아차릴 수 있다. 프로듀서마다 트랜잭션 ID는 구별되어야 한다. at-most-once와 at-least-once 수준의 전달 보증에 비해 처리량이 낮고(lower throughput) 대기 시간이 더 길 수 있다(higher latency).구현하기 To work with the transaction API, we’ll need Kafka’s Java client&lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt;&lt;/dependency&gt;1. 프로듀서-producerProps.put(\"enable.idempotence\", \"true\");producerProps.put(\"transactional.id\", \"prod-1\");producerProps.put(\"acks\", \"all\"); max.in.flight.requests.per.connection ???enable.idempotenceWhen set to ‘true’, the producer will ensure that exactly one copy of each message is written in the stream. If ‘false’, producer retries due to broker failures, etc., may write duplicates of the retried message in the stream. Note that enabling idempotence requires max.in.flight.requests.per.connection to be less than or equal to 5, retries to be greater than 0 and acks must be ‘all’.If these values are not explicitly set by the user, suitable values will be chosen. If incompatible values are set, a ConfigException will be thrown.-&gt; https://docs.confluent.io/platform/current/installation/configuration/producer-configs.html 프로듀서 준비 producer.initTransactions(); 트랜잭션을 사용하는 프로듀서로 브로커에 등록된다. 프로듀서는 transactional.id와 시퀀스 번호 또는 에포크(epoch)로 식별된다. 브로커는 이를 사용하여 트랜잭션 로그에 모든 작업을 미리 기록한다. 결과적으로 브로커는 동일한 트랜잭션 ID와 이전 에포크를 가진 프로듀서에 속하는 모든 작업을 해당 로그에서 제거하고, 이러한 작업이 존재하지 않는 트랜잭션에서 발생한 것으로 가정한다. (??) ??? producer.beginTransaction(); ??? 메시지는 파티셔닝 될 수 있기 때문에, 트랜잭션 메시지가 여러 파티션에 걸쳐 있고 각 파티션마다 별도의 컨슈머가 메시지를 읽는다. 따라서 Kafka는 트랜잭션에 대해 업데이트된 모든 파티션 목록을 저장한다. 트랜잭션 내에서 프로듀서는 멀티 스레드를 사용하여 레코드를 병렬로 전송할 수 있다. 2. 브로커 min.insync.replicas -&gt; When a producer sets acks to “all” (or “-1”), min.insync.replicas specifies the minimum number of replicas that must acknowledge a write for the write to be considered successful. If this minimum cannot be met, then the producer will raise an exception (either NotEnoughReplicas or NotEnoughReplicasAfterAppend). When used together, min.insync.replicas and acks allow you to enforce greater durability guarantees. A typical scenario would be to create a topic with a replication factor of 3, set min.insync.replicas to 2, and produce with acks of “all”. This will ensure that the producer raises an exception if a majority of replicas do not receive a write.3. Consumer isolation.level을 사용하여 연관된 트랜잭션이 커밋될 때까지 트랜잭션 메시지를 기다릴 수 있다. read_committed : 트랜잭션이 완료되기 전까지 트랜잭션 메시지를 읽지 않는다. (default 값) read_uncommitted : 아직 커밋되지 않은 메시지뿐 아니라 트랜잭션이 abort된 메시지도 읽는다. (즉 모든 메시지를 읽는다) 트랜잭션과 무관한 메시지는 isolation.level과 상관없이 모든 경우에 읽을 수 있다. consumerProps.put(\"enable.auto.commit\", \"false\");consumerProps.put(\"isolation.level\", \"read_committed\"); 마지막으로 방금 소비한 오프셋을 적용해야 합니다. 트랜잭션을 사용하면 오프셋을 정상적으로 읽은 입력 항목에 다시 적용합니다. 또한, 우리는 생산자의 거래에 그것들을 보냅니다.테스트 시나리오 enable.idempotence를 true로 하게 되면 카프카는 중복 메시지 제거 알고리즘에서 transactional.id를 사용하여 프로듀서가 보내는 모든 메시지의 멱등성(idempotence)을 보장한다.-&gt; 어케 테스트하지 ??? producer.sendOffsetsToTransaction(offsetsToCommit, CONSUMER_GROUP_ID); 없으면 오프셋 변동 없을지2.테스트 결과 정확히 한번만 전달되어야됨 메시지 중복 없어야됨전체 코드#트랜잭션 처리를 위한 STATE 같은게 있나 ??실제 운영환경 ? acks=1이라고 해서 메시지 손실률이 전혀 없는 것은 아닙니다. 하지만 방금 설명드린 현상이 빈번하게 일어나는 일은 아니고, 메시지를 보내는 속도 역시 고려사항중 하나이기 때문에 실제 운영환경에서는 acks=1로 가장 많이 사용하고 있습니다. 제가 운영하는 카프카에서도 프로듀서 옵션은 acks=1을 가장 많이 사용하고 있습니다. 이어서 마지막 옵션에 대해 살펴보겠습니다. 실제 운영환경에서 브로커 노드 2개가 동시에 다운되는 일은 거의 발생하지 않습니다. 그래서 Replication Factor를 3으로 운영하시고, 안정적인 구현을 위해서는 min.insync.replicas는 2로 설정하는 것이 가장 바람직하다고 생각됩니다. 제가 설명드린 여러 예제중 가장 안정적인 예제로 acks=all, Replication Factor=3과 min.insync.replicas=2로 설명드렸지만, 실제 운영환경에서 가장 많이 쓰이는 옵션은 아닙니다. 운영환경에서 가장 많이 쓰이는 옵션은 프로듀서의 acks=all보다는 acks=1를 가장 많이 사용하고 있습니다. 그래서 저는 운영하면서 브로커의 min.insync.replicas옵션에 대해 크게 신경쓰지 않았습니다. 하지만 앞으로 추가되는 메시지의 중요도에 따라 해당 옵션들을 변경하여 사용할 수 있기 때문에 해당 옵션들에 대해 완벽하게 이해하시고 본인의 운영환경을 파악한 후 그에 알맞은 설정을 하는 것이 가장 중요하다고 생각합니다. 우리회사는 ?? ???Invalid transition attempted from state READY to state ABORTING_TRANSACTIONconsumer.commitInSync랑 차이 ??TransactionalId prod-0: Invalid transition attempted from state READY to state ABORTING_TRANSACTION 전달보증은 메시지를 보내는 횟수와 관련이 있는거고, 메시지 유실과 관련된건 acks와 replication ?? 상호 보완적인것 ?? at least once를 구현하려면 acks가 필요하다 ?? ★ 프로듀서에서의 전달 보증은 acks를 통해 구현할 수 있다 !-&gt; acks가 0이면 at most once-&gt; acks가 1이면 at least once-&gt; acks가 all이면 exactly once ?? (all이라는건 min.insync.replica에 정의된 최소 ISR 수, min.insync.replicas 옵션은 브로커의 옵션이다 - config/server.properties) -&gt; all 이여도-&gt; acks가 all이어도 at least once 될 수 있지 않나 ??? 전달보증이라는건 브로커 -&gt; 컨슈머로의 전달을 의미 ??-&gt;참고자료 https://www.baeldung.com/kafka-exactly-once https://supergloo.com/kafka/kafka-architecture-delivery/ https://blog.voidmainvoid.net/354 https://dzone.com/articles/kafka-producer-delivery-semantics https://gunju-ko.github.io/kafka/2019/01/07/%EC%B9%B4%ED%94%84%EC%B9%B4%EC%BB%A8%EC%8A%88%EB%A8%B8.html https://www.popit.kr/kafka-%EC%9A%B4%EC%98%81%EC%9E%90%EA%B0%80-%EB%A7%90%ED%95%98%EB%8A%94-producer-acks/" }, { "title": "XSSF로 인한 OOM 에러 그리고 SXSSF와 메모리 사용량 비교해보기", "url": "/posts/xssf-oom-analyze/", "categories": "경험하기, 이슈 노트", "tags": "Java, Heap Memory, Eclipse Memory Analyzer, apache poi", "date": "2021-07-23 23:00:00 +0900", "snippet": "상황 이 글 에서 언급했듯이, poi 라이브러리를 활용해서 엑셀을 만들 때, XSSF 클래스를 사용하면데이터 건수(엑셀 row)가 많은 경우 OOM 에러가 발생한다. XSSF를 개선한 SXSSF를 사용하면 해당 문제를 해결할 수 있다고 하는데, 실제로 메모리 사용량을 비교해보고자 한다. 메모리 단면을 분석하는 방법은 이 글 을 참고...", "content": "상황 이 글 에서 언급했듯이, poi 라이브러리를 활용해서 엑셀을 만들 때, XSSF 클래스를 사용하면데이터 건수(엑셀 row)가 많은 경우 OOM 에러가 발생한다. XSSF를 개선한 SXSSF를 사용하면 해당 문제를 해결할 수 있다고 하는데, 실제로 메모리 사용량을 비교해보고자 한다. 메모리 단면을 분석하는 방법은 이 글 을 참고한다. 분석하기 jps 명령어로 pid를 확인한 뒤, jmap dump:&lt;dump-options&gt; &lt;pid&gt;를 활용하여 덤프 파일을 생성하고 해당 파일을 MAT(Eclipse Memory Analyzer)로 분석하였다. 테스트 환경 CPU : 2.6 GHz 6코어 Intel Core i7 메모리 : 32GB 2667 MHz DDR4 java 8 메이븐 디펜던시 설정 &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;/properties&gt; &lt;groupId&gt;org.example&lt;/groupId&gt; &lt;artifactId&gt;performance-test&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.8.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.poi&lt;/groupId&gt; &lt;artifactId&gt;poi&lt;/artifactId&gt; &lt;version&gt;4.0.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.poi&lt;/groupId&gt; &lt;artifactId&gt;poi-ooxml&lt;/artifactId&gt; &lt;version&gt;4.0.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-compress&lt;/artifactId&gt; &lt;version&gt;1.19&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt;테스트 수행 클래스import org.apache.poi.ss.usermodel.Cell;import org.apache.poi.ss.usermodel.Row;import org.apache.poi.ss.usermodel.Sheet;import org.apache.poi.ss.usermodel.Workbook;import org.apache.poi.ss.util.CellReference;import java.io.FileOutputStream;public class PoiTester { private final int ROW_CNT = 10000; private final int COL_CNT = 10; private final long THREAD_WAIT_TIME = 60000L; private Workbook workbook; private String fileName; public PoiTester(Workbook workbook, String fileName) { this.workbook = workbook; this.fileName = fileName; } public void test() { long beginTime = System.currentTimeMillis(); Workbook wb = workbook; Sheet sh = wb.createSheet(); for (int rownum = 0; rownum &lt; ROW_CNT; rownum++) { Row row = sh.createRow(rownum); for (int cellnum = 0; cellnum &lt; COL_CNT; cellnum++) { Cell cell = row.createCell(cellnum); String address = new CellReference(cell).formatAsString(); cell.setCellValue(address); } } try { long endTime = System.currentTimeMillis(); long totalTime = ( endTime - beginTime ) / 1000; System.out.println(fileName+\" 경과 시간 : \"+totalTime+\"초\"); Thread.sleep(THREAD_WAIT_TIME); FileOutputStream out = new FileOutputStream(fileName+\".xlsx\"); wb.write(out); out.close(); } catch (Exception e) { e.printStackTrace(); } }}테스트 케이스1. XSSF테스트 코드import org.apache.poi.xssf.usermodel.XSSFWorkbook;public class XssfTest { public static void main(String[] args) { XSSFWorkbook workbook = new XSSFWorkbook(); String fileName = \"xssf_ver\"; PoiTester tester = new PoiTester(workbook, fileName); tester.test(); }} 실행 결과 : xssf_ver 경과 시간 : 1초Histogram 히스토그램을 살펴보면 ElementXobj 클래스 XSSFSheet 클래스에서 메모리의 대부분을 차지하고 있음을 알 수 있다. XSSFSheet 하나가 차지하는 메모리 크는 80byte 이지만, 해당 객체가 점유하고 있는 객체들을 모두 합치면 약 70Mb 정도를 차지하고 있는 것을 알 수 있다. 또한, 아래쪽에 보면 XSSFRow 즉, 행이 10000개 만들어진 것도 확인할 수 있다.Dominator Tree Dominator Tree를 살펴보면 ElementXobj 객체로 인해 대부분의 메모리가 점유되고 있다. .xlsx 파일은 XML 기반이라고 하는데, XSSF는 이 XML을 처리하는데 메모리를 상당히 많이 차지하는 것 같다. .xlsx 의 XML과 관련된 글은 네이버 기술블로그의 다음 글 을 참고해보면 좋을 것 같다. 테스트 케이스2. SXSSF(auto-flush 사용하지 않음)테스트 코드import org.apache.poi.xssf.streaming.SXSSFWorkbook;public class SxssfNoAutoFlush { public static void main(String[] args) { SXSSFWorkbook workbook = new SXSSFWorkbook(-1); // -1 : not use auto-flush String fileName = \"sxssf_no_autoflush_ver\"; PoiTester tester = new PoiTester(workbook, fileName); tester.test(); }} 실행 결과 : sxssf_no_autoflush_ver 경과 시간 : 10초Histogram 히스토그램을 살펴보면 XSSFSheet에 비해 SXSSFSheet 객체 자체가 점유하는 메모리가 작고, 모든 객체가 점유하는 메모리를 합쳐도 15Mb 정도로 이전 70Mb에 비해 약 5배 줄어든 것을 확인할 수 있다. 또한, 아래쪽에 보면 SXSSFRow 즉, 행이 10000개 만들어진 것도 확인할 수 있다. 10000개의 행이 모두 메모리 상에 있는 것을 통해 auto-flush가 적용되지 않은 것을 확인할 수 있다. XSSFRow가 10000개 였을 때 0.24Mb를 차지했는데, SXSSFRow는 0.4Mb를 차지하는 것을 볼 수 있다. Dominator Tree XSSF를 사용할 때와는 다르게 ElementXobj 객체로 인한 낭비는 보이지 않는 것 같다.테스트 케이스3. SXSSF(auto-flush 사용)테스트 코드import org.apache.poi.xssf.streaming.SXSSFWorkbook;public class SxssfAutoFlush { public static void main(String[] args) { SXSSFWorkbook workbook = new SXSSFWorkbook(1000); // keep 1000 rows in memory, exceeding rows will be flushed to disk String fileName = \"sxssf_autoflush_ver\"; PoiTester tester = new PoiTester(workbook, fileName); tester.test(); }} 실행 결과 : sxssf_autoflush_ver 경과 시간 : 2초참고. SXSSFWorkbook의 기본 윈도우 사이즈는 100이다.public static final int DEFAULT_WINDOW_SIZE = 100;public SXSSFWorkbook() { this((XSSFWorkbook)null);}public SXSSFWorkbook(XSSFWorkbook workbook) { this(workbook, 100);}Histogram SXSSFSheet 객체와 그 안에 모든 객체가 점유하는 메모리를 합쳐도 1.6Mb 정도이다. XSSF에 비해 약 50배, auto-flush를 사용하지 않을 때에 비해 약 10배의 메모리를 덜 점유한다. 또한, 아래쪽에 보면 SXSSFRow 즉, 행이 1000개만 있는 것을 볼 수 있다. 지정한 row access window size(위 코드에서는 1000) 이상이 메모리에 적재되면 디스크로 flush하는 auto-flush 기능이 잘 작동하는 것을 알 수 있다. Dominator Tree 메모리 낭비가 없다고 봐도 무방할 것 같다.회사 코드에 적용하기 이제 회사 코드에 적용해보고 얼만큼의 효과가 있을지 확인해보자. 데이터 건수는 약 5600건 이다. (보안망으로 인해 테스트 결과는 핸드폰 카메라로 찍었습니다. 화질이 좋지 않으점 양해 부탁드립니다.)Before (XSSF 사용) 메모리 약 300Mb 차지 엑셀 파일 생성전까지 모든 row를 메모리에 저장After (SXSSF 사용 - default window size) 메모리 약 8Mb 차지 window size 만큼(default : 100) 메모리에 저장결론 추측 : XSSF는 XML 기반의 무언가를 처리함에 있어서 메모리 낭비가 심하다. XSSF는 엑셀 파일 생성 전까지 모든 행을 메모리에 올려두기 때문에, 데이터가 많은 경우 OOM이 발생할 수 있다. poi를 활용해 엑셀을 만든다면 SXSSF를 auto-flush 기능을 활성화해서 사용하자.(특별한 이유가 없다면 default size를 사용하면 될 것 같다.)" }, { "title": "Java - 자바로 엑셀을 만들기 위한 POI 라이브러리 살펴보기", "url": "/posts/excel-using-poi-library/", "categories": "지식 더하기, 이론", "tags": "Java", "date": "2021-07-22 00:29:00 +0900", "snippet": "회사 프로젝트에서 조회한 화면에 대해 엑셀 파일로 다운받게 만들어주는 기능을 구현해야 했다. 방법을 찾다보니 POI 라이브러리라는게 있었고 이를 활용해서 구현할 수 있었다.하지만, 다른 파트에서 해당 라이브러리를 사용하다 OOM(Out Of Memory)에러가 발생했고, 구현체를 살펴보니 XSSF의 usermodel을 사용하고 있었다.해당 파트에서는 ...", "content": "회사 프로젝트에서 조회한 화면에 대해 엑셀 파일로 다운받게 만들어주는 기능을 구현해야 했다. 방법을 찾다보니 POI 라이브러리라는게 있었고 이를 활용해서 구현할 수 있었다.하지만, 다른 파트에서 해당 라이브러리를 사용하다 OOM(Out Of Memory)에러가 발생했고, 구현체를 살펴보니 XSSF의 usermodel을 사용하고 있었다.해당 파트에서는 문제 해결을 위해 알아보던 중, SXSSF가 더 효율적이라는 것을 찾아냈고 이를 적용하여 OOM 문제를 해결했다.(이 글 에서도 비슷한 경험을 공유해주신다.)아직 우리 파트에선 OOM이 발생하진 않았지만, 현재 XSSF를 사용하고 있기 때문에 OOM이 발생할 잠재적 가능성을 갖고있는 것이다.따라서, SXSSF를 적용하기에 앞서 어떤 원리로 이런 차이가 만들어지는건지 알고 싶었다.HSSF &amp; XSSF HSSF는 Excel ‘97(-2007) 파일 형식(.xls)를 지원한다. XSSF는 Excel 2007 OOXML(.xlsx) 파일 형식을 지원한다. HSSF, XSSF는 다음과 같은 기능을 제공한다. low level structures 효율적인 읽기 전용 액세스를 위한 이벤트 모델(eventmodel) API XLS 파일을 만들고, 읽고, 수정하기 위한 전체 사용자 모델(usermodel) API 권고사항 스프레드시트 데이터만 읽는 경우, 파일 형식에 따라 org.apache.poi.hssf.eventusermodel 패키지 또는 org.apache.poi.xssf.eventusermodel 패키지의 eventmodel api를 사용 스프레드시트 데이터를 수정, 생성하는 경우 usermodel API를 사용 usermodel 시스템은 low level의 eventusermodel보다 memory footprint가 더 높지만 작업하기는 훨씬 간단하다. XSSF가 지원되는 Excel 2007(.xlsx) 파일은 XML 기반, HSSF가 지원되는 Excel ‘97 파일(.xls)은 바이너리 기반이다. XML을 처리하는데 필요한 memory footprint가 더 높다. ※ memory footprint : 프로그램이 실행하는 동안 사용하거나 참조하는 기본 메모리의 양SXSSF Since 3.8-beta3, POI provides a low-memory footprint SXSSF API built on top of XSSF. SXSSF는 매우 큰 스프레드시트를 생성하거나 힙 공간이 제한될 때 사용할 수 있는 XSSF의 API 호환 스트리밍 확장이다. SXSSF는 sliding window 내에 있는 행에 대한 액세스를 제한하여 low memory footprint를 가능하게 한다. (window : 메모리 버퍼의 일정 영역) 반면, XSSF는 문서의 모든 행에 대한 액세스를 제공한다. Sliding Window Memory Buffer Architecture (출처 : https://www.researchgate.net/figure/Sliding-Window-Memory-Buffer-Architecture_fig1_272854151) 더 이상 window에 없는 이전 행은 디스크에 기록될 때 액세스할 수 없게 된다. auto-flush 모드에서는 access window의 크기를 지정하여 일정한 수의 행을 메모리에 보관할 수 있다. 이 값에 도달했을 때 추가 행을 만들면, 가장 낮은 인덱스를 가진 행이 access window에서 제거되고 디스크에 기록된다. public static void main(String[] args) throws Throwable { SXSSFWorkbook wb = new SXSSFWorkbook(100); // keep 100 rows in memory, exceeding rows will be flushed to disk Sheet sh = wb.createSheet(); for(int rownum = 0; rownum &lt; 1000; rownum++){ Row row = sh.createRow(rownum); for(int cellnum = 0; cellnum &lt; 10; cellnum++){ Cell cell = row.createCell(cellnum); String address = new CellReference(cell).formatAsString(); cell.setCellValue(address); } } // Rows with rownum &lt; 900 are flushed and not accessible for(int rownum = 0; rownum &lt; 900; rownum++){ Assert.assertNull(sh.getRow(rownum)); } // ther last 100 rows are still in memory for(int rownum = 900; rownum &lt; 1000; rownum++){ Assert.assertNotNull(sh.getRow(rownum)); } FileOutputStream out = new FileOutputStream(\"/temp/sxssf.xlsx\"); wb.write(out); out.close(); // dispose of temporary files backing this workbook on disk wb.dispose(); } window 크기를 동적으로 조정하고 필요에 따라 행을 flushRows(int keepRows)하는 명시적 호출로 주기적으로 잘라낼 수 있다. public static void main(String[] args) throws Throwable { SXSSFWorkbook wb = new SXSSFWorkbook(-1); // turn off auto-flushing and accumulate all rows in memory Sheet sh = wb.createSheet(); for(int rownum = 0; rownum &lt; 1000; rownum++){ Row row = sh.createRow(rownum); for(int cellnum = 0; cellnum &lt; 10; cellnum++){ Cell cell = row.createCell(cellnum); String address = new CellReference(cell).formatAsString(); cell.setCellValue(address); } // manually control how rows are flushed to disk if(rownum % 100 == 0) { ((SXSSFSheet)sh).flushRows(100); // retain 100 last rows and flush all others // ((SXSSFSheet)sh).flushRows() is a shortcut for ((SXSSFSheet)sh).flushRows(0), // this method flushes all rows } } FileOutputStream out = new FileOutputStream(\"/temp/sxssf.xlsx\"); wb.write(out); out.close(); // dispose of temporary files backing this workbook on disk wb.dispose();} 스트리밍 특성으로 인해 XSSF와 비교할 때 다음과 같은 제한이 있다. 한 시점에는 제한된 수의 행만 액세스할 수 있습니다. Sheet.clone()은 지원되지 않습니다. Formula evaluation이 지원되지 않는다. Spreadsheet API Feature Summary 출처 : https://poi.apache.org/components/spreadsheet/실제 메모리 사용량 비교해보기 XSSF vs SXSSF 클래스 메모리 사용량 비교해보기더 공부할 부분 sliding window memory 아키텍처참고 자료 https://poi.apache.org/components/spreadsheet/how-to.html#sxssf" }, { "title": "Java - 자바 메모리 단면 분석하기", "url": "/posts/heap-analyze-with-jmap-and-mat/", "categories": "지식 더하기, 이론", "tags": "Java", "date": "2021-07-20 23:00:00 +0900", "snippet": "언제 메모리 단면을 분석해야할까 ?메모리 단면인 ‘힙 덤프(Heap Dump)’는 생성하는데 비용이 비싸기 때문에, 메모리가 부족해지는 현상이 지속해서 발생하거나 OOM 에러가 발생했을 때 생성해야 한다.여기서 말하는 비용이란 다음과 같다 덤프 파일을 생성하는 동안 서비스가 불가능한 상황이 된다. 덤프 생성시 많은 시간이 소요될 수 있다. 큰 파...", "content": "언제 메모리 단면을 분석해야할까 ?메모리 단면인 ‘힙 덤프(Heap Dump)’는 생성하는데 비용이 비싸기 때문에, 메모리가 부족해지는 현상이 지속해서 발생하거나 OOM 에러가 발생했을 때 생성해야 한다.여기서 말하는 비용이란 다음과 같다 덤프 파일을 생성하는 동안 서비스가 불가능한 상황이 된다. 덤프 생성시 많은 시간이 소요될 수 있다. 큰 파일(대부분 점유하고 있는 메모리 크기만큼의 파일)이 생성된다. 몇몇 JDK 버전은 jmap과 같은 도구를 사용할 경우, 한 번밖에 덤프 파일을 생성할 수 없다.OOM은 에러가 발생하므로 인식할 수 있겠지만, 메모리가 점점 부족해지는 것은 어떻게 파악할 수 있을까? jstat 사용 WAS의 모니터링 콘솔 Java Visual VM, Jconsole과 같은 JMX 기반의 모니터링 도구 APM(Application Performance Management) 활용 verbosegc, Xlog 옵션 활용jstat 예시 (JDK 11 사용) 실행하는 자바 애플리케이션의 pid를 알기 위해 jps 명령어를 활용한다 $ jps18387 Launcher18388 HoldMemory3339618389 Jps jstat -gcutil &lt;pid&gt; &lt;interval&gt;을 활용하여 모니터링 할 수 있다. 아래 항목 중 4번째 컬럼인 O(Old 또는 Tenured 영역)의 메모리 사용량이 GC 이후에도 증가한다면 메모리 릭이 발생하고 있다고 판단할 수 있다. 해당 영역의 메모리 사용량은 애플리케이션이 동작하는 동안에는 계속 증가하는 것이 기본이다. 따라서, 일반적인 상황에서 Old 영역의 메모리가 계속 증가한다고 해서 메모리 릭이라고 판단해선 안된다. $ jstat -gcutil 18388 2sS0 S1 E O M CCS YGC YGCT FGC FGCT CGC CGCT GCT0.00 100.00 9.09 10.13 95.47 88.35 2 0.032 0 0.000 0 0.000 0.0320.00 100.00 9.09 10.13 95.47 88.35 2 0.032 0 0.000 0 0.000 0.0320.00 100.00 18.18 21.23 95.47 88.35 4 0.062 0 0.000 0 0.000 0.0620.00 100.00 18.18 21.23 95.47 88.35 4 0.062 0 0.000 0 0.000 0.0620.00 100.00 45.45 30.12 95.47 88.35 6 0.093 0 0.000 0 0.000 0.0930.00 100.00 45.45 30.12 95.47 88.35 6 0.093 0 0.000 0 0.000 0.0930.00 100.00 16.67 46.96 95.47 88.35 8 0.129 0 0.000 0 0.000 0.1290.00 100.00 16.67 46.96 95.47 88.35 8 0.129 0 0.000 0 0.000 0.1290.00 100.00 17.24 58.04 95.47 88.35 9 0.162 0 0.000 0 0.000 0.1620.00 100.00 17.24 58.04 95.47 88.35 9 0.162 0 0.000 0 0.000 0.1620.00 100.00 8.70 63.48 95.51 88.35 10 0.199 0 0.000 2 0.001 0.2000.00 100.00 8.70 63.48 95.51 88.35 10 0.199 0 0.000 2 0.001 0.2000.00 100.00 43.24 64.98 95.51 88.35 11 0.215 0 0.000 4 0.002 0.216 메모리 단면 생성하기 메모리 단면은 일반적으로 다음과 같이 생성할 수 있다. 자바 프로세스 실행 시 옵션에 포함하여 자동 파일 생성 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=경로 OOM 에러가 발생하지 않은 이상 애플리케이션 성능에 전혀 영향을 끼치지 않는다. 실행 옵션과 상관없이 명령어를 사용하여 파일 생성 jmap 명령어 리눅스의 gcore와 같은 OS에서 제공하는 코어 덤프 명령어 (파일 크기가 자바 힙 덤프에 비해 훨씬 크다) jmap 사용해서 힙 덤프 생성하기 옵션 내용 -dump:[live],format=&lt;format&gt;,file=&lt;filename&gt; 힙 덤프 파일을 생성 -finalizerinfo GC가 되려고 기다리고 있는 객체들의 정보 출력 -clstats 클래스 로더의 통계 정를 제공 -histo[:live] 힙 영역의 메모리 점유 상태를 가장 많이 점유한 객체부터 출력 -F -dump와 -histo 옵션과 같이 사용되며, 덤프를 강제로 발생시킬 때 사용 jmap -dump:&lt;dump-options&gt; &lt;pid&gt; 명령어 사용하여 아래와 같이 파일을 생성할 수 있다. $ jmap -dump:format=b,file=dumptest.bin 20195Heap dump file created MAT 활용하여 메모리 단면 분석하기 MAT는 Eclipse Memory Analyzer로서 활발하게 개발 및 유지보수가 이루어지고 있다. MAT 이외에도 다양한 메모리 분석 도구가 있다. 먼저 MAT를 다운 받는다. 생성한 힙 덤프 파일을 MAT에서 열게되면 아래와 같은 화면을 볼 수 있다.Leak Suspect Details를 누르면 아래 항목들을 확인할 수 있다. Description 해당 객체에 대한 설명 Shortest Paths To the Accumulation Point 메모리를 점유하고 있는 객체가 가장 상단에, 그 객체를 생성하고 호출한 객체가 하단에 트리 형태로 나타난다. Accumulated Objects 메모리를 점유하고 있는 객체가 가장 하단에, 그 객체를 생성하고 호출한 객체가 가장 상단에 트리 형태로 나타난다. Accumulated Objects by Class 클래스별로 객체를 점유하는 대상 목록 Histogram 클래스별 객체의 개수와 그 크기를 가장 큰 값부터 확인할 수 있다. 어떤 클래스 타입이 가장 많은 메모리를 가지고 있는지 쉽게 파악할 수 있다. 하지만, 메모리를 가장 많이 사용하는 객체를 식별하기는 쉽지 않다. 예를 들어 아래 화면에서 byte[]는 가장 많은 양의 메모리를 차지하고 있다. 그러나 어떤 객체가 실제로 해당 바이트 배열을 보유하고 있는지 식별할 수 없다. Shallow Heap/Retained Heap Shallow Heap은 객체 자체의 크기이다. Retained Heap은 객체 자체의 크기 + 해당 객체에서 갖고있는 객체들의 크기이다. 예를 들어, 아래 HoldMemory 객체 자체는 16바이트의 크기를 가진다. 그러나 해당 객체가 갖고 있는 모든 객체의 크기가 700Mb를 초과한다. 따라서, HoldMemory에 의해 유지되는 HashMap이 메모리 문제의 원인일 것으로 추론할 수 있다. Dominator Tree 각 클래스별(클래스 로더 단위)로 점유하고 있는 메모리의 양이 가장 많은 클래스부터 트리 형태로 나타낸다. 예를 들어, 아래 화면에서는 HoldMemory 객체가 가장 큰 메모리를 차지하고 있음을 알 수 있다. 또한, 해당 객체가 700Mb 이상의 메모리를 포함하는 해시 맵을 보유하고 있음을 알 수 있다. Dominator Tree 하나의 객체가 많은 양의 메모리를 차지할 때 분석하기에 유용하다. 만약 여러 개의 작은 객체로 인해 메모리 릭이 발생한다면, 히스토그램을 사용하여 메모리를 가장 많이 인스턴스를 확인하는 것이 좋다.Duplicated Classes 여러 클래스 로더에서 중첩되게 로딩한 클래스의 대한 정보를 확인할 수 있다. 클래스 로더에 문제가 있을 경우 확인하는 용도로 주로 사용한다.실제로 사용해보기 XSSF vs SXSSF 클래스 메모리 사용량 비교해보기참고 자료 이상민, 『자바 트러블슈팅』, 제이펍(2019), 13~14장. https://www.cleantutorials.com/jconsole/heap-dump-analysis-using-eclipse-memory-analyzer-tool-mat https://eclipsesource.com/blogs/2013/01/21/10-tips-for-using-the-eclipse-memory-analyzer/" }, { "title": "Java - 자바 힙 메모리 사이즈와 OutOfMemoryError", "url": "/posts/heap-size-and-oome/", "categories": "지식 더하기, 이론", "tags": "Java", "date": "2021-07-20 23:00:00 +0900", "snippet": "지금까지 OutOfMemoryError(OOM)을 만나게 되면, 정확한 진단없이 그냥 성능 팀에서 하라는 대로 Xms, Xmx 옵션을 활용해서 힙 크기를 조절했던 것 같다.내가 스스로 판단할 수 있게끔 지식을 갖추고 연습해보자.힙 메모리 사이즈기본값 공식 문서 에 보면 default로 세팅되는 초기 힙사이즈와 최대 힙사이즈에 대해 나와있다. 결론부...", "content": "지금까지 OutOfMemoryError(OOM)을 만나게 되면, 정확한 진단없이 그냥 성능 팀에서 하라는 대로 Xms, Xmx 옵션을 활용해서 힙 크기를 조절했던 것 같다.내가 스스로 판단할 수 있게끔 지식을 갖추고 연습해보자.힙 메모리 사이즈기본값 공식 문서 에 보면 default로 세팅되는 초기 힙사이즈와 최대 힙사이즈에 대해 나와있다. 결론부터 얘기하면, 초기 힙사이즈는 물리 메모리의 1/64, 최대 힙사이즈는 물리 메모리의 1/4이다. 나의 경우 RAM이 32G 이므로, 초기 힙사이즈, 최대 힙사이즈는 각각 500MB, 8GB 정도가 나와야할 것이다. 아래의 InitialHeapSize와 MaxHeapSize를 확인해보면 예상대로 계산되는 것을 확인할 수 있다.$ java -XX:+PrintFlagsFinal -version 2&gt;&amp;1 | grep -i -E 'heapsize|metaspacesize|version' size_t ErgoHeapSizeLimit = 0 {product} {default} size_t HeapSizePerGCThread = 43620760 {product} {default} size_t InitialBootClassLoaderMetaspaceSize = 4194304 {product} {default} size_t InitialHeapSize = 536870912 {product} {ergonomic} size_t LargePageHeapSizeThreshold = 134217728 {product} {default} size_t MaxHeapSize = 8589934592 {product} {ergonomic} size_t MaxMetaspaceSize = 18446744073709535232 {product} {default} size_t MetaspaceSize = 21807104 {pd product} {default} size_t MinHeapSize = 8388608 {product} {ergonomic} uintx NonNMethodCodeHeapSize = 5839372 {pd product} {ergonomic} uintx NonProfiledCodeHeapSize = 122909434 {pd product} {ergonomic} uintx ProfiledCodeHeapSize = 122909434 {pd product} {ergonomic} size_t SoftMaxHeapSize = 8589934592 {manageable} {ergonomic} java version \"15.0.1\" 2020-10-20내 애플리케이션에 적절한 힙 사이즈는 ? 딱 나눠 떨어지는 공식 같은 것은 없는 것 같다. 아래 사항을 참고하여 적절한 힙 사이즈를 찾기 위해 노력해보자.아래 내용들은 공식 문서 에서 발췌하였다.해당 문서는 WebLogic Server를 기준으로 작성되었다.힙 사이즈와 GC GC에 대한 허용 속도는 애플리케이션에 따라 다르며 GC의 실제 시간과 빈도를 분석한 후 조정해야 한다. 힙 크기가 클수록 Full GC 속도가 느려지고 빈도는 줄어든다. 힙 크기가 작을수록 Full GC는 빠르지만 자주 발생한다. 힙 크기를 조정하는 목적은 JVM이 GC 수행하는 데 드는 시간을 최소화하는 동시에, 한 번에 처리할 수 있는 클라이언트 수를 최대화하는 것이다.verbosegc 사용하기 HotSpot VM의 GC 옵션(verbosec)을 사용하면 GC에 투입되는 시간과 리소스의 양을 정확하게 측정할 수 있다. 로그 파일을 통해 진단 결과를 확인한다. 애플리케이션을 실행하는 동안 최대 부하에서 성능을 모니터링 한다. verbosec 옵션을 사용하여 JVM에 대한 자세한 가비지 수집 출력을 켜고 표준 오류 및 표준 출력을 모두 로그 파일을 통해 다음을 확인한다. 실행옵션 예시 : -XX:+UseSerialGC -Xms1024m -Xmx1024m -verbose:gc 자바9 부터는 -verbose:gc 옵션 대신 -Xlog:gc라는 옵션을 사용해야한다. GC 수행 빈도와 수행 시간(Full GC는 3~5초 이상 걸리지 않아야 한다.) Full GC 후 사용 가능한 힙 메모리. 힙의 85% 이상이 사용 가능한 상태라면 경우 힙 크기를 더 작게 설정하는 것을 고려해볼 수 있다. 힙 크기가 시스템에서 사용 가능한 RAM보다 크지 않아야 한다. 시스템이 페이지를 디스크로 “swap” 하지 않도록 가능한 큰 힙 크기를 사용한다. 시스템의 사용 가능한 RAM 크기는 하드웨어 구성과 시스템에서 프로세스를 실행하는 데 필요한 메모리 요구 사항에 따라 달라진다. 시스템이 GC에 너무 많은 시간을 소비하는 경우 힙 크기를 줄인다. 일반적으로 사용 가능한 RAM의 80%(운영 체제나 다른 프로세스에서 사용하지 않음)를 JVM에 사용해야 한다. 주의사항 JVM에서 사용하는 최대 메모리 양이 사용 가능한 물리적 RAM 양을 초과하지 않도록 설정해야 한다. 이 값을 초과하면 OS가 페이징을 시작하고 성능이 크게 저하된다. 프로덕션 환경에서는 최소 힙 크기와 최대 힙 크기를 동일한 값으로 설정한다. 힙을 지속적으로 늘리거나 줄이는 데 사용되는 JVM 리소스를 낭비하지 않도록 하기 위해 OutOfMemoryError의 다양한 원인 살펴보기 OutOfMemoryError가 발생하는 경우는 대표적으로 다음과 같다. 가비지 컬렉터가 새로운 객체를 생성할 공간을 더 이상 만들어주지 못하고, 힙 영역의 메모리 또한 증가될 수 없을 때 네이티브 라이브러리 코드에서 스왑 영역이 부족하여, 더 이상 네이티브 할당을 할 수 없을 때 다양한 시스템 로그를 통해 OutOfMemoryError가 발생하는 여러가지 원인에 대해 살펴보자1. Java heap space 자바의 힙 영역에서 더 이상 객체를 생성하기 어려울 때 발생한다.이를 유발하는 다양한 케이스가 있을 수 있다. Exception in thread \"main\".: java.lang.OutOfMemoryError: Java heap space 메모리 크기를 너무 작게 잡거나, 메모리 크기를 지정하지 않은 경우 -Xms, -Xmx 실행옵션 확인하기 오래된 객체들이 계속 참조되고 있어 GC가 되지 않는 경우 static을 잘못 사용하고 있진 않은지, 애플리케이션이 의도치 않게 특정 객체를 계속 참조하고 있지는 않은지 확인하기 finalize 메서드를 개발자가 개발한 클래스에 구현해 놓은 경우 JVM에는 GC 대상 객체들을 큐에 쌓고 처리하기 위한 데몬 스레드가 존재한다. 이 스레드가 객체들을 처리하기도 전에 finalize로 인해 큐에 너무 많은 객체가 보관되어 있고, 처리가 불가능한 경우 문제가 발생할 수 있다. 스레드의 우선순위를 너무 높일 경우 개발된 프로그램의 스레드 우선순위를 너무 높게 지정해 놓아서, 스레드를 메모리에 생성하는 속도가 GC를 처리하는 속도보다 빠르면 문제가 발생할 수 있다. 큰 덩어리의 객체가 여러 개 있을 경우 예를 들어, 힙 메모리를 256MB로 지정하고 한번 호출되면 100MB의 메모리를 점유하는 화면을 세 번 호출하는 경우 2. Metaspace (Java 8 부터) Java 8 부터는 Permanent이 없어지고 Metaspace가 생겼다. Exception in thread \"main\".: java.lang.OutOfMemoryError: Metaspace Java 8 이전의 JVM에서는 Permgen Space라는 메시지가 나왔지만, Java 8 부터는 Metaspace 에러가 발생한다. 너무 많은 클래스가 자바 프로세스에 로딩될 경우 발생할 수 있다. -XX:MaxMetaspaceSize 옵션을 사용하여 크기를 조절할 수 있다.3. Requested array size exceeds VM limit Exception in thread \"main\".: java.lang.OutOfMemoryError: Requested array size exceeds VM limit 배열의 크기가 힙 영역의 크기보다 더 크게 지정되었을 때 발생한다. 고정된 크기가 아닌 계산된 변수로 배열 크기를 지정할 경우 발생할 수 있다.4. request &lt;size&gt; bytes for &lt;reason&gt;. Out of swap space? Exception in thread \"main\".: java.lang.OutOfMemoryError: request &lt;size&gt; bytes for &lt;reason&gt;. Out of swap space? 네이티브 힙 영역이 부족할 때, 즉 OS의 메모리(Swap 영역)가 부족한 상황이 되었을 때 발생한다. 애플리케이션에서 호출하는 네이티브 메서드에서 메모리를 반환하지 않는 경우 다른 애플리케이션에서 메모리를 반환하지 않는 경우 5. &lt;reason&gt; &lt;stacktrace&gt; (Native method) Exception in thread \"main\".: java.lang.OutOfMemoryError: &lt;reason&gt; &lt;stacktrace&gt; (Native method) 4번과 마찬가지로 네이티브 힙 영역에 메모리를 할당할 때 발생되는 에러이다. 4번의 경우는 JVM 코드에서 발생될 때, 이 경우는 JNI나 네이티브 코드에서 발생한다는 뜻이다.메모리 문제와 GC 튜닝 너무 잦은 Full GC가 발생하면 성능에 많은 영향을 미친다.이런 경우, 무작정 GC를 튜닝하기 보다는 GC가 많이 발생하지 않도록 하는 것이 먼저이다.따라서, 다음과 같은 규칙을 잘 따랐는지 살펴봐야한다. 임시 메모리의 사용을 최소화 객체의 재사용 XML 처리 시 메모리를 많이 점유하는 DOM 보다 SAX를 사용 너무 많은 데이터를 한 번에 보여주는 비즈니스 로직 제거※ 자바 프로세스 id(pid)만 알면 jstat을 사용하여(java -gcutil &lt;pid&gt; &lt;interval&gt;) 각 영역별로 메모리를 얼마나 사용하는지 확인할 수 있다.jstat은 $JAVA_HOME/bin 디렉토리에 존재한다.더 공부할 부분 finalize를 사용하면 안되는 이유 스레드 우선순위 gc 로그 보는 법 swap 영역 ?참고 자료 이상민, 『자바 트러블슈팅』, 제이펍(2019), 12장. https://docs.oracle.com/javase/8/docs/technotes/guides/vm/gc-ergonomics.html https://docs.oracle.com/cd/E12839_01/web.1111/e13814/jvm_tuning.htm#PERFM150" }, { "title": "카프카의 전반적인 구성과 동작 방식 살펴보기", "url": "/posts/kafka-basic-concept/", "categories": "", "tags": "", "date": "2021-07-15 14:00:00 +0900", "snippet": "※ 해당 내용은 ‘실전 아파치 카프카(한빛미디어)’ 2장을 읽고 필요한 부분을 정리한 내용입니다.분산 메세징 처리를 위한 시스템 구성 출처 : https://www.gavstech.com/all-about-apache-kafka-an-evolved-log/1. 파티션 브로커상의 데이터를 읽고 쓰는 것은 파티션이라는 단위로 분할되어 있다. 파티...", "content": "※ 해당 내용은 ‘실전 아파치 카프카(한빛미디어)’ 2장을 읽고 필요한 부분을 정리한 내용입니다.분산 메세징 처리를 위한 시스템 구성 출처 : https://www.gavstech.com/all-about-apache-kafka-an-evolved-log/1. 파티션 브로커상의 데이터를 읽고 쓰는 것은 파티션이라는 단위로 분할되어 있다. 파티션 단위로 나누는 이유는 토픽에 대한 대량의 메세지 입출력을 지원하기 위함이다. 즉, 파티션은 브로커 클러스터 안에 분산 배치되어 메세지 수신/전달을 분산해서 수행함으로써 처리량을 높인다. 2. 컨슈머 그룹 단일 토픽이나 여러 파티션에서 메세지를 취득하기 위해 ‘컨슈머 그룹’이라는 개념이 존재한다. 카프카 클러스터 전체에서 글로벌 ID를 컨슈머 그룹 전체에서 공유한다. 여러 컨슈머는 자신이 소속한 컨슈머 그룹을 식별해, 읽어들일 파티션을 분류하고 재시도를 제어한다.3. 오프셋 파티션 내에서 메세지의 위치(파티션에서 수신한 메세지에는 일련번호가 부여된다)를 나타내는 정보 오프셋을 활용해 컨슈머가 취득하는 메세지의 범위 및 재시도를 제어한다. 오프셋의 종류는 다음과 같다. Log-End-Offset(LEO) : 파티션 데이터의 끝을 나타낸다. 브로커에 의해 파티션에 관한 정보로 관리, 업데이트된다. Current Offset : 컨슈머가 어디까지 메세지를 읽었는지를 나타낸다. 컨슈머에서 데이터를 취득하면 업데이트된다. Commit Offset : 컨슈머가 어디까지 커밋했는지를 나타낸다. 컨슈머 그룹마다 보관, 관리, 업데이트된다. 4. 주키퍼 Apache Zookeeper는 분산 처리를 위한 관리 도구이다.(coordination service system) 카프카의 분산 메세징 처리를 위한 메타 데이터(토픽, 파티션 등)를 관리한다. 주키퍼 클러스터의 구조상 홀수로 구성하는 것이 일반적이다.코디네이션 서비스의 필요성 (출처 : 조대협님 블로그) 분산 시스템을 설계 하다보면, 다음과 같은 문제에 부딪힌다. 분산된 시스템간의 정보 공유를 어떻게 할 것인지 클러스터에 있는 서버들의 상태 체크를 어떻게 할 것인지 분산된 서버들간에 동기화를 위한 락(lock) 처리를 어떻게 할 것인지 코디네이션 서비스는 분산 시스템 내에서 중요한 상태 정보나 설정 정보 등을 유지함으로써 위와 같은 문제를 해결해준다. 따라서, 코디네이션 서비스의 장애는 전체 시스템의 장애를 유발하기 때문에, 이중화 등을 통하여 고가용성을 제공해야 한다. ZooKeeper는 이러한 특성을 잘 제공하기 때문에 유명한 분산 솔루션에 많이 사용되고 있다. 코디네이션 서비스는 데이터 액세스가 빨라야 하며, 자체적으로 장애에 대한 대응성을 가져야 한다. Zookeeper는 자체적으로 클러스터링을 제공하여 장애에도 데이터 유실 없이 fail over/fail back이 가능하다. 메세지 송수신을 위한 시스템 구성1. 메세지와 토픽 메세지 카프카에서 다루는 데이터의 최소 단위 Key, Value를 갖는다. 토픽 메세지를 종류별로 관리하는 스토리지 브로커에 배치되어 관리된다. 2. 브로커 메세지를 수신/전달하는 역할을 담당한다. 하나의 서버(또는 인스턴스)당 하나의 데몬 프로세스로 동작한다. 여러 대의 클러스터로 구성할 수 있으며, 브로커를 추가함으로써 처리량 향상(스케일 아웃)이 가능하다. ※ 클러스터 : 여러 대의 컴퓨터들이 연결되어 하나의 시스템처럼 동작하는 컴퓨터들의 집합브로커의 데이터 보관 기간 스토리지 용량 제한이 있기 때문에 기간을 무제한으로 둘 수는 없다. 일반적으로 데이터 삭제를 위해 다음 두 가지 정책으로 사용한다. 오래된 메세지 삭제 브로커 파라미터 : cleanup.policy : delete 지정한 시간보다 오래된 데이터가 삭제된다(default : 1주) 축적 데이터 크기가 지정한 데이터 크기보다 커진 경우(default : 크기 제한 없음) 압축 브로커 파라미터 : cleanup.policy : compact 최신 Key의 데이터를 남겨두고 중복하는 Key의 오래된 메세지가 삭제된다. 동일한 Key에 대해서는 항상 최신의 Value만 얻을 수 있으면 되는 상황에서 사용한다. 3. 프로듀서 프로듀서/컨슈머를 구현하는 기능은 브로커로 데이터를 보내고 브로커에서 데이터를 받기 위한 라이브러리로 제공된다. 프로듀서 API를 이용하여 브로커에 데이터를 송신하기 위해 구현된 애플리케이션이다. 프로듀서 API를 내포한 도구, 미들웨어를 통해 이용하는 형태 등 다양하다.프로듀서의 메세지 송신 파티션에 메세지를 송신할 때, 버퍼 기능처럼 프로듀서의 메모리를 이용하여 일정량 축적 후 송신(배치 처리)할 수 있다. 수 바이트 ~ 수십 바이트의 작은 메세지를 대량으로 브로커에 송신하는 상황을 가정해보자. 이런 경우 하나의 메세지당 1회씩 송신되면, 네트워크의 지연으로 인해 처리량에 영향을 줄 수 있다. 하지만, 처리의 지연시간은 증가되므로 처리량과 지연 시간의 트레이드 오프를 고려한 설계가 필요하다. 배치 처리시, 메세지를 송신하는 트리거는 다음과 같다. 지정된 크기에 도달한 경우 (설정값 : batch.size) 지정한 대기 시간에 도달한 경우 (설정값 : linger.ms) 파티셔닝 프로듀서에서 송신하는 메세지를 어떻게 파티션을 보낼지 결정하는 것 메세지 Key의 해시 값을 사용한 송신 메세지의 Key를 명시적으로 지정함으로써 Key에 따라 송신처 파티션을 결정한다. 파티션 클래스에는 partitionId라는 멤버 변수가 있어 ID에 의해 관리된다. 라운드 로빈에 의한 송신 라운드 로빈 방식 : 순서대로 할당 메세지 Key를 null로 지정한 경우, 여러 파티션으로의 메세지 송신을 라운드 로빈 방식으로 실행한다. 출처 : 사사키 도루 외 4인, 『실전 아파치 카프카』, 한빛미디어(2020), p68 파티셔닝을 이용하는 경우, 데이터 편차에 따른 파티션의 편향에 주의해야 한다. 극단적인 경우에는 파티션 편향으로 인해 리소스를 부분적으로 사용할 수 없는 상태가 된다. 출처 : 사사키 도루 외 4인, 『실전 아파치 카프카』, 한빛미디어(2020), p69메세지의 순서 보증 카프카는 기본적으로 파티션을 여러 개로 구성하여 확장성을 높인다. 이로 인해, 프로듀서에서 메세지를 송신한 순서대로 컨슈머에서 메세지를 수신한다는 보장이 없다. 메세지 순서 보증을 위해 단일 파티션으로 구성하게 되면 카프카의 강점인 확장성을 잃게 된다. 메세지의 종류(토픽)별 순서를 보증하기 위해서는 파티셔닝에서 살펴본 ‘해시에 의한 분할’을 활용할 수 있다. 전체 메세지의 순서를 보증하는 것은 구현 난이도가 높다. 또한, 컨슈머 구현이 프로듀서 데이터 전송 순서에 의존하게 되면 카프카 전후의 시스템적인 분리에 제한을 줄 수 있다. 메세지 순서 보증을 위한 정렬 기능을 구현해야 한다면, 어디에서(브로커 or 컨슈머) 구현할지는 시스템 전체를 고려하여 판단해야 한다.4. 컨슈머 컨슈머 API를 이용해 브로커에서 메세지를 취득하도록 구현된 애플리케이션이다.PULL 형 브로커에 있는 메세지를 컨슈머가 취득해가는 방식(PULL 형)의 이점을 살펴보자. 컨슈머 시스템이 다운되거나 일시적으로 이상이 있어도 브로커에 미치는 영향이 적다. 만약 브로커에서 컨슈머로 PUSH 해주는 방식이라면, 컨슈머의 장애를 매번 브로커에서 대응해줘야 한다. 카프카를 경유하는 메세지와 후속 시스템이 많을수록 시스템 운용 부하, 성능 부하로 이어질 수 있다. 컨슈머가 주체적으로 데이터를 수신, 진행 관리하기 때문에, 후속 시스템의 확장과 축소가 쉽다.컨슈머의 메세지 취득 메세지 취득은 브로커의 디스크에 보관되어 있는 동안 가능하다. 컨슈머는 취득 대상의 토픽과 파티션에 대해 Current Offset ~ LEO까지 취득을 요청한다. 이것을 반복함으로써 게속적인 메세지 취득을 할 수 있다. 메세지의 유입 빈도가 동일한 경우, 요청 간격이 길수록 모인 메세지가 많아진다. 일정 간격을 두고 요청함으로써 처리량을 높일 수 있다. 프로듀서의 배치 처리와 마찬가지로, 처리량과 지연 시간의 트레이드 오프를 고려한 설계가 필요하다. 컨슈머의 롤백 Offset Commit을 통해 컨슈머 처리 실패, 고장 시 롤백 메세지 재취득을 할 수 있다. 컨슈머에 의한 데이터 취득이 2회 발생하는 다음과 같은 시나리오를 살펴보자. 출처 : 사사키 도루 외 4인, 『실전 아파치 카프카』, 한빛미디어(2020), p64~65 위 시나리오의 4번에서 Current Offset이 Commit Offset까지 되돌아온 만큼의 메세지(6,7,8)에 대한 대처는 컨슈머 측에게 맡긴다. 즉, 재송신된(At Least Once) 메세지 6,7,8에 대해 어떻게 메세지 중복 처리를 할 것인지에 대한 방안이 필요하다. 데이터의 견고성을 높이는 복제 구조 카프카는 장애시에도 수신한 메세지를 잃지 않기 위해 복제(Replication) 구조를 갖추고 있다. 파티션은 단일 또는 여러 개의 레플리카로 구성되어 토픽 단위로 레플리카 수를 지정할 수 있다. 레플리카 중 하나는 Leader 나머지는 Follower라고 불린다. Follower는 Leader로부터 메세지를 취득하여 복제를 유지한다. 프로듀서/컨슈머간 데이터 교환은 Leader가 맡게된다. 출처 : 사사키 도루 외 4인, 『실전 아파치 카프카』, 한빛미디어(2020), p.72In-Sync Replica Leader 레플리카의 복제 상태를 유지하고 있는 레플리카는 In-Sync Replica(ISR)로 분류된다. 파라미터 replica.lag.time.max.ms에서 정한 시간보다도 오랫동안 복제의 요청 및 복제가 이루어지지 않을 경우 Leader 레플리카 복제 상태를 유지하지 않는 레플리카로 간주한다. 모든 레플리카가 ISR로 되어 있지 않은 파티션을 Under Replicated Partitions라고 한다. 복제 수와는 별개로 최소 ISR 수(min.insync.replica) 설정이 가능하다. 출처 : https://www.javatpoint.com/kafka-topic-replicationHigh Watermark 복제가 완료된 최신 오프셋 컨슈머는 High Watermark까지 기록된 메세지를 취득할 수 있다. 출처 : 사사키 도루 외 4인, 『실전 아파치 카프카』, 한빛미디어(2020), p.73Ack 설정 브로커 → 프로듀서로 Ack를 어느 타이밍에 송신할 것인지를 설정하는 것은 성능과 (브로커 서버 장애시) 데이터 유실 방지에 큰 영향을 준다. Ack 설정 설명 0 프로듀서는 Ack를 기다리지 않고 다음 메세지를 송신한다. 1 Leader Replica에 메세지가 전달되면 Ack를 반환한다. all 모든 ISR의 수만큼 복제되면 Ack를 반환한다. 프로듀서는 Ack가 돌아오지 않고 타임아웃된 메세지에 대해 송신 실패로 간주한다. Ack를 반환하는 타이밍에는 메세지가 디스크가 아닌 메모리(OS 버퍼)에 기록되어있는 상태이다.ISR과 Ack를 통한 메세지 처리 제어 ISR과 Ack 설정에 따라 메세지 쓰기를 제어할 수 있다. 상황은 브로커 4대, 레플리카 수는 3으로 브로커 1대가 고장나 레플리카를 하나 잃어버린 경우라고 가정한다.1. min.insync.replicas=3 (레플리카 수와 동일), Ack=all 비정상 상태로 간주되어 잃어버린 레플리카가 ISR로 복귀할 때까지 메세지를 처리하지 않는다. 출처 : 사사키 도루 외 4인, 『실전 아파치 카프카』, 한빛미디어(2020), p.752. min.insync.replicas=2, Ack=all 브로커 1대가 고장나더라도 최소 ISR 수를 만족하므로 Ack를 반환하고 처리를 계속한다. 처리를 계속할 수는 있지만, 복구 전에 브로커가 더 고장나게 되면 처리 중인 메세지를 손실할 위험이 높아진다. 출처 : 사사키 도루 외 4인, 『실전 아파치 카프카』, 한빛미디어(2020), p.75 시스템 요구 사항과 제약 조건에 따라, ‘메세지를 잃지 않는 것’과 ‘시스템의 처리가 계속 되는 것’ 사이의 균형을 min.insync.replicas와 Ack 설정을 통해 조절해야 한다.참고자료 사사키 도루 외 4인, 『실전 아파치 카프카』, 한빛미디어(2020), 2장" }, { "title": "카프카 개요", "url": "/posts/kafka-introduction/", "categories": "", "tags": "", "date": "2021-07-14 22:25:00 +0900", "snippet": "※ 해당 내용은 ‘실전 아파치 카프카(한빛미디어)’ 1장을 읽고 필요한 부분을 정리한 내용입니다.Apache Kafka란? 카프카는 분산 메세징 시스템이다 분산 분산 시스템(distributed system, 또는 분산 컴퓨팅) 서로 다른 시스템에 위치한 구성 요소들간에 통신하고 협력하여 구성되는 시스템 ...", "content": "※ 해당 내용은 ‘실전 아파치 카프카(한빛미디어)’ 1장을 읽고 필요한 부분을 정리한 내용입니다.Apache Kafka란? 카프카는 분산 메세징 시스템이다 분산 분산 시스템(distributed system, 또는 분산 컴퓨팅) 서로 다른 시스템에 위치한 구성 요소들간에 통신하고 협력하여 구성되는 시스템 구성 요소는 컴퓨터, 물리적 서버, 가상 시스템, 컨테이너 등이 될 수 있다. 사용자에게는 하나의 시스템처럼 보인다. 일반적으로 분산 시스템은 확장성, 안정성, 성능 면에서 이점을 지닐 수 있다. 메세징 시스템 메세지를 받고 받은 메세지를 다른 시스템이나 장치에 보낸다 분산 이벤트 스트리밍 플랫폼이라고도 불린다 이벤트 스트리밍이란 ? 데이터베이스, 센서, 모바일 장치, 클라우드 서비스 및 소프트웨어 애플리케이션과 같은 이벤트 소스로부터 스트림의 형태로 이벤트(데이터)를 실시간으로 캡처하는 작업이다. 이벤트 스트리밍 플랫폼은 이러한 이벤트 스트림을 나중에 검색할 수 있도록 영구적으로 저장하고, 이벤트 스트림을 실시간으로 조작, 처리 및 대응한다. 탄생 배경 링크드인(LinkedIn)에서 생성되는 로그를 처리하여, 사용자들의 웹사이트 활동을 추적하는 것을 목적으로 개발되었다.(더 자세한 내용은 링크드인 블로그 참조) 당시, 많은 기업들에서는 빅데이터를 어떻게 활용할 것인지가 화두였고 이를 위해 로그를 활용하기 시작했다. 링크드인이 실현하려고 했던 목표는 다음과 같다. 데이터를 높은 처리량으로 실시간 처리 수백 밀리초 ~ 수 초 내에 데이터 수집에서 처리까지 완료한다. 임의의 타이밍에 데이터를 읽는다 데이터를 사용하는 목적에 따라 꼭 실시간이 아니라 배치 처리도 가능하도록 한다. 다양한 제품과 시스템에 쉽게 연동한다 링크드인에서 데이터의 발생원이 되는 시스템이 다양했기 때문에, 여러 시스템을 통해 데이터를 받아들여야 했다. 메세지를 잃지 않는다 사용자 활동을 추적하는 것이 목표였기 때문에 약간의 중복이 있더라도 메세지를 잃지 않는 것이 중요했다. 메세지 중복을 허용하지 않도록 건마다 관리하면, 처리 오버헤드가 커지기 때문에 ‘높은 처리량으로 실시간 처리’라는 목적을 달성하기가 어렵다. 목표 실현하기 링크드인은 위에서 언급한 목표를 실현하기 위해 크게 4가지 방법을 사용했다.1. 메세징 모델과 스케일 아웃형 아키텍처 목표 높은 처리량으로 실시간 처리 임의의 타이밍에 데이터 읽기 다양한 제품과 시스템에 쉽게 연동 이러한 목표 달성을 위해 카프카는 메세징 모델을 채택했다.일반적인 메세징 모델 메세징 모델은 일반적으로 Producer, Broker, Consumer로 구성된다. 대표적인 메세징 모델로는 Queuing 모델과 Publish/Subscribe(Pub/Sub) 모델이 있다. Queuing 모델 브로커 안에 있는 큐에 프로듀서의 메세지가 담기고, 컨슈머가 큐에서 메세지를 추출한다. 여러 개의 컨슈머로 확장하여 메세지를 추출할 수 있어 병럴 처리가 가능하다. 하나의 컨슈머가 메세지를 받으면 다른 컨슈머는 해당 메세지를 받을 수 없다. 즉, 하나의 메세지는 여러 컨슈머 중 어느 하나에서 처리된다. 큐에서 추출되어 컨슈머에 도달한 메세지는 사라진다. 출처 : https://www.mailgun.com/blog/set-up-message-queue-asynchronous-sending/ Pub/Sub 모델 Publisher = Producer Subscriber = Consumer Publisher는 브로커 내의 Topic이라고 불리는 스토리지에 메세지를 발행한다. Subscriber는 여러 개의 토픽 중 원하는 것만 구독하여 메세지를 수신한다. 여러 Subscriber가 동일한 토픽을 구독하면, 큐잉 모델과는 달리 하나의 메세지가 여러 Subscriber(컨슈머)에게 도달할 수 있다. 출처 : https://dev.to/tranthanhdeveloper/point-to-point-and-publish-subscribe-messaging-model-41j0카프카의 메세징 모델과 스케일 아웃형 아키텍처 높은 처리량을 실현하기 위해서는 어떻게 확장성 있는 구성을 할 수 있을지가 관건이다 카프카는 컨슈머 그룹이라는 개념을 도입해서 두 가지 메세징 모델의 특징을 살린 모델을 구현한다. Queuing 모델의 특징 : 병렬 처리(여러 컨슈머가 분산 처리로 메세지 소비) Pub/Sub 모델의 특징 : 동일한 메세지를 여러 구독자에게 전달하고, 토픽을 기반으로 메세지 분류 결과적으로, 카프카의 메세징 모델은 Pub/Sub 모델을 기반으로 컨슈머 그룹 내에서 분산 처리를 할 수는 구조이다. 카프카는 고가용성과 확장성을 위해 브로커를 복수로 구성하여 스케일 아웃형 아키텍처를 구성한다.2. 디스크로의 데이터 영속화 목표 임의의 타이밍에 데이터를 읽는다. 메세지를 잃지 않는다. 배치 처리를 위해 데이터를 일정 기간마다 메모리에서만 유지하는 것은 용량 면에서 불가능하다. 따라서, 카프카의 메세지 영속화는 디스크에서 이루어진다. 디스크에 영속화하지만 높은 처리량을 제공한다. 장기 보존을 목적으로 영속화가 가능하기 때문에 카프카를 ‘스토리지 시스템’으로도 간주할 수 있다. 커밋 로그를 축적하기 위한 스토리지 시스템을 예로 들 수 있다. 카프카의 영속화 카프카는 브로커의 메모리에 실리면 송신 완료(메모리에서 디스크로의 flush는 OS에게 맡김)라는 사상을 갖고 있다. flush 간격을 OS에 전적으로 맡기지 않고 카프카 파라미터로 지정할 수도 있다. 따라서, 카프카에서의 데이터 영속화는 반드시 데이터 자체에 대한 내결함성(fault-tolerance)을 향상시키기 위한 것은 아니라고 이해할 수 있다. 오히려 메세지 복제 구조를 통해, 단일 브로커의 고장이 발생하더라도 데이터 손실로 이어지지 않는다고 이해하는 것이 자연스럽다.3. 이해하기 쉬운 API 제공 목표 다양한 제품과 시스템에 쉽게 연동 카프카는 프로듀서와 컨슈머가 쉽게 접속할 수 있도록 Connect API를 제공한다. 프로듀서와 컨슈머는 이를 이용하여 브로커와의 상호 교환이 가능하다. API를 기반으로 타 제품과의 접속에 관한 공통 부분을 프레임워크화한 Kafka Connect와 제품별 커넥터도 있다. 카프카의 스트림 데이터 처리를 위한 Streams API와 이를 이용한 Kafka Streams도 제공된다.4. 전달 보증 목표 메세지를 잃지 않는다. 프로듀서 입장에서는 당연히 중간에 메세지가 유실되지 않고 컨슈머까지 전달되어야 한다. 이를 위해 카프카는 ‘전달 보증(delivery guarantees)’ 기능을 제공한다. 종류 브로커에서 메시지 처리 재전송 유무 중복 삭제 유무 메시지 유실 여부 At Most Once 0 또는 1회 X X 유실될 수 있다. At Least Once 1회 또는 그 이상 O X 유실 가능성이 낮다. Exactly Once 1회 O O 유실 가능성이 매우 낮다. 카프카 개발 초기에는 높은 처리량을 구현해야 했기 때문에 Exactly Once 수준의 보증은 미루고 최소한 메세지 유실을 방지하고자 At Least Once 수준으로 전달을 보증했다.Ack와 Offset Commit Ack 브로커가 메세지를 수신했을 때 프로듀서에게 수신 완료했다는 응답 이것을 이용해 프로듀서는 재전송 여부를 판단할 수 있다. acks = 0이면 at most once 수준의 전달 보증 acks = 1이면 at least once 수준의 전달 보증 (default) acks = all이면 exactly once 수준의 전달 보증 (acks 이외에 enable.idempotence 등의 설정도 필요하다.) Offset Commit Offset : 컨슈머가 어디까지 메세지를 받았는지를 관리하기 위한 지표 Offset Commit : 오프셋을 이용해 전달 보증을 실현하는 구조 브로커는 오프셋 커밋을 통해 오프셋을 업데이트한다. 즉, 컨슈머가 메세지를 처리(수신)한 기록을 남기는 것이다. 이를 통해 메세지를 재전송할 때도 어디서부터 재전송하면 되는지 판단할 수 있다. Exactly Once 구현 카프카의 유용성이 높아지면서 Exactly Once 수준의 전달을 보증하고자 하는 요구가 높아졌다. 이를 위해 카프카에 트랜잭션 개념을 도입하게 되었다. Exactly Once를 보증하기 위해서는 프로듀서, 컨슈머 측에서의 구현 도 필요하다. 출처 : 사사키 도루 외 4인, 『실전 아파치 카프카』, 한빛미디어(2020), p51 출처 : 사사키 도루 외 4인, 『실전 아파치 카프카』, 한빛미디어(2020), p51 요약카프카의 특징 확장성 : 여러 서버로 scale out할 수 있기 때문에 데이터 양에 따라 시스템 확장이 가능하다. 영속성 : 수신한 데이터를 디스크에 유지할 수 있기 때문에 언제라도 데이터를 읽을 수 있다 유연성 : 연계할 수 있는 제품이 많기 때문에 허브 역할을 한다. 신뢰성 : 메세지 전달 보증을 하므로 데이터 분실을 걱정하지 않아도 된다.카프카의 목표와 실현 방법참고자료 사사키 도루 외 4인, 『실전 아파치 카프카』, 한빛미디어(2020), 1장 https://kafka.apache.org/documentation/# https://blog.stackpath.com/distributed-system/" }, { "title": "MSA 기반의 회사 프로젝트에 단위 테스트 코드 적용하기", "url": "/posts/applying-unit-test-code-in-company/", "categories": "", "tags": "", "date": "2021-07-13 22:25:00 +0900", "snippet": "들어가기 전부끄러운 얘기지만 내가 회사에서 경험했던 프로젝트에는 테스트 코드가 없었다. 제작 당시 테스트 코드 작성에 대한 얘기가 아예 나오지 않았던 것은 아니지만, 당시 여론은‘제작 기간 맞추기도 쉽지 않은데 언제 그걸 하고 앉아있냐’라는 의견이 주를 이뤘다. 나 또한 테스트 코드의 중요성에 대해 잘 알지 못했고, 빡빡한 제작기간으로 인해 밥먹듯이 ...", "content": "들어가기 전부끄러운 얘기지만 내가 회사에서 경험했던 프로젝트에는 테스트 코드가 없었다. 제작 당시 테스트 코드 작성에 대한 얘기가 아예 나오지 않았던 것은 아니지만, 당시 여론은‘제작 기간 맞추기도 쉽지 않은데 언제 그걸 하고 앉아있냐’라는 의견이 주를 이뤘다. 나 또한 테스트 코드의 중요성에 대해 잘 알지 못했고, 빡빡한 제작기간으로 인해 밥먹듯이 야근을 해댔기 때문에개발기간 맞추기에만 급급했던 것 같다.개발, 테스트 기간을 거쳐 시스템을 가동했고, 다양한 에러들이 터져나오며 정말 밤낮으로 디버깅을 했다. 어떤 문제에 대한 디버깅이 해당 문제는 해결했지만,기존에 잘 동작하고 있던 부분에 영향을 미쳐서 또 다른 문제를 발생시키는 경우가 상당히 많았다. 여기서 나는 테스트 코드 작성을 하지 않은 것에 대한 댓가(?)를 치루는 것 같았다.폭풍같은 시간들이 지나고 현재는 나름대로 안정화가 되어 운영을 하고 있지만, 여전히 테스트 코드의 부재는 코드를 리팩토링을 한다거나 새로운 요구사항에 대한 코드를 추가했을 때,자동화된 테스트를 할 수 없으며 일일이 손으로 테스트를 해야한다는 불편함이 있다.따라서 나는 지금이라도 테스트 코드를 작성해야겠다고 생각했고, 마이크로서비스 환경에서 단위 테스트하는 방법을 공부한 뒤회사 프로젝트에 적용해보았다.단위 테스트 코드 작성하기적용 범위 일단은 가장 핵심이 되는 비즈니스 로직(스프링 기준으로 ServiceImpl)들에 적용을 해보았다. 컨트롤러와 이벤트/메시지 핸들러 테스트는 추후에 적용해보고 내용을 추가할 예정이다.무엇을 검증할 것인가 ? 일단 내가 속한 팀의 비즈니스 로직 특성은 일반적으로 다음과 같다. 하나 이상의 마이크로서비스에서 데이터를 조회해온 뒤 데이터를 가공한다. 가공한 데이터를 하나 이상의 서비스에 CUD(CREATE / UPDATE / DELETE) 처리를 한다. 타서비스와 통신하는 방식에는 크게 두 가지가 있다. R(조회) : Rest API CUD : 메시지 (브로커 : Kafka) 위 특징을 기반으로, 비즈니스 로직에서 검증해야 할 것들은 다음과 같다고 생각했다. 데이터 조회를 위해 특정 서비스를 호출했는지 조회할 때 필요한 항목이 세팅이 됐는지 타서비스에 CUD 처리를 위해 발행하는 이벤트에 데이터가 제대로 세팅이 되었는지 검증 데이터 : 토픽명, 반드시 세팅되어야 할 항목의 값 등 잘못된 데이터가 들어왔을 때, 적절한 예외를 발생시키는지 어떻게 검증할 것인가 ? 데이터 조회를 위해 특정 서비스를 호출했는지 Mockito를 활용해서 타서비스를 호출하는 OtherServiceProxy 부분을 모킹하고, 조회 메서드 호출시 기대한 데이터를 반환하도록 스텁화한다. verify를 통해 해당 메서드가 호출되었는지 확인한다. 조회할 때 필요한 항목이 세팅이 됐는지 단순 값인 경우 Mockito의 ArgumentMatchers eq() 활용 타서비스에 CUD 처리를 위해 발행하는 이벤트에 데이터가 제대로 세팅이 되었는지 Mockito의 ArgumentCaptor 활용 적용 코드 실제 회사 코드를 첨부할 수는 없기 때문에, 대략적인 흐름을 파악할 수 있는 코드를 작성했다.이렇게 하는게 맞는지는 모르겠으나, 최소한 없을 때 보다는 리팩토링 할 때 좀 더 안전하게 할 수 있을것 같다.추후에 더 공부하고 새로운 것들을 알게되면 보완할 예정이다.public class CoreServiceImplTest { @Mock private AServiceProxy aServiceProxy; @Mock private BServiceProxy bServiceProxy; @InjectMocks private CoreServiceImpl logic; private Map&lt;String, Object&gt; inputMessage = new HashMap&lt;&gt;(); private AServiceData getAServiceSampleData() { AServiceData data = new AServiceData(); data 세팅 ... return data; } private BServiceData getBServiceSampleData() { BServiceData data = new BServiceData(); data 세팅 ... return data; } private void initInputMsg() { inputMessage에 값 세팅... } private void initStub() { when(aServiceProxy.조회메서드(eq(value1), eq(value2), isNull())) .thenReturn(getAServiceSampleData); when(bServiceProxy.조회메서드(anyList()) .thenReturn(getBServiceSampleData); } @Before public void initMock() { MockitoAnnotations.initMocks(this); initInputMsg(); initStub(); } @Test public void 타서비스_테이블에_저장() { logic.invoke(inputMessage); ArgumentCaptor&lt;SaveData&gt; captor = ArgumentCaptor.forClass(SaveData.class); verify(aServiceProxy).저장메서드(captor.capture()); SaveData data = captor.getValue(); assertThat(data).isNotNull(); assertThat(저장한 데이터의 값).isEqualTo(기대하는 값); . . . } @Test public void 조회결과_없는경우_예외처리() { when(aServiceProxy.조회메서드(eq(value1), eq(value2), isNull())) .thenReturn(null); assertThrownBy(() -&gt; logic.invoke(inputMessage).isInstanceOf(특정Exception.class); }// 이하생략...}느낀점 개발하는 도메인을 제대로 파악하지 못한다면 어떤 값이 중요한지, 어떤 값을 검증해야 할지 잘 모르기 때문에 제대로된 테스트 코드 작성이 어려울 것 같다. 메서드는 최대한 나눠져 있는게 테스트하기에 수월한 것 같다. 꾸준히 연습하자.더 공부할 부분 @RunWith(MockitoJUnitRunner.class) vs initMock() JUnit5" }, { "title": "MSA 환경에서 테스트하기(2) - 통합 테스트", "url": "/posts/msa-testing-part2/", "categories": "", "tags": "", "date": "2021-07-11 22:25:00 +0900", "snippet": "※ 해당 내용은 ‘마이크로서비스 패턴(크리스 리처드슨)’ 10장을 읽고 필요한 부분을 정리한 내용입니다. 서비스가 서로 올바르게 상호 작용하는지에 대해서는 단위 테스트만으로는 확인할 수 없다.예를 들어, 실제 DB에 저장을 했는지, 커맨드 메세지를 올바른 포맷으로, 올바른 채널에 전송했는지 등에 대한 부분이다.이를 위해, 서비스를 전부 띄워 놓고 일...", "content": "※ 해당 내용은 ‘마이크로서비스 패턴(크리스 리처드슨)’ 10장을 읽고 필요한 부분을 정리한 내용입니다. 서비스가 서로 올바르게 상호 작용하는지에 대해서는 단위 테스트만으로는 확인할 수 없다.예를 들어, 실제 DB에 저장을 했는지, 커맨드 메세지를 올바른 포맷으로, 올바른 채널에 전송했는지 등에 대한 부분이다.이를 위해, 서비스를 전부 띄워 놓고 일일이 API를 호출해 보는 종단 간 테스트를 해보면 가장 확실하겠지만이런 테스트는 느리고 취약하며 비용이 많이 든다.따라서, 다른 서비스와 제대로 상호 작용하는지 확인하기 위해서는 단위 테스트 바로 윗 단계인 ‘통합 테스트’가 필요하다.컨슈머 주도 계약 테스트(consumer-driven contract test) 통합 테스트를 진행하기에 앞서 ‘계약’이라는 개념에 대해 알고가자.두 서비스 간의 상호 작용은 두 서비스 사이의 합의 또는 계약이다. 예를 들어, 주문 서비스와 주문 이력 서비스는서로에게 발행될 이벤트 메세지의 구조와 채널에 대해 합의해야 한다.API 게이트웨이와 도메인 서비스 역시 REST API 끝점에 대해 합의해야 한다. 서비스가 클라이언트의 기대에 부합하는지 확인하는 테스트 클라이언트는 어떠한 서비스를 호출하는 서비스(API 게이트웨이, 다른 도메인 서비스 등)이다. 컨슈머(호출하는 서비스)-프로바이더(호출되는 서비스)의 관계를 맺는다. 프로바이더의 API가 컨슈머가 기대한 바와 일치하는지 확인하는 것. 즉, 프로바이더에 대한 통합 테스트이다. 비즈니스 로직을 체크하는 테스트가 아니다. 다음 사항을 확인 컨슈머가 기대한 HTTP 메서드와 경로인가 ? (헤더가 있는 경우) 컨슈머가 기대한 헤더를 받는가 ? (요청 본문이 있는 경우) 요청 본문을 받는가 ? 컨슈머가 기대한 상태 코드, 헤더, 본문이 포함된 응답을 반환하는가 ? 컨슈머/프로바이더간 상호 작용을 계약(contract)이라는 샘플 모음집으로 정의하는 것 예를 들어, REST API의 계약은 HTTP 요청/응답 샘플을 모아 놓은 것 Spring Cloud Contract를 사용하여 컨슈머 계약 테스트를 진행할 수 있다. 프로세스 컨슈머 팀은 개발한 서비스가 프로바이더와 상호 작용하는 방법이 기술된 계약을 작성해서 깃 풀 리퀘스트 등을 통해 프로바이더 팀에 전달 프로바이더 팀은 계약을 JAR로 패키징해서 메이븐 저장소에 발행 컨슈머 쪽 테스트는 저장소에서 JAR 파일을 내려받는다. 주문 서비스의 API를 소비하는 컨슈머 개발 팀은 계약 테스트 스위트를 추가하고, 기대대로 주문 서비스 API가 동작하는지 확인 출처 : Chris Richardson, 『Microservice Patterns』, p.302 통합 테스트 작성통합 테스트 전략 통합 테스트는 전체 서비스를 실행시키지 않는다.테스트 효과에 영향을 끼치지 않으면서 테스트를 간소화하기 위해 두 가지 전략을 사용한다. 각 서비스의 어댑터(가능하면 어댑터의 지원 클래스까지)를 테스트 예를 들어, JPA 영속화 테스트를 위해서는 API를 호출하는게 아니라 OrderRepository 클래스를 직접 테스트 전체 서비스 대신 소수의 클래스로 테스트 범위를 좁히면 테스트가 단순/신속해진다. 출처 : Chris Richardson, 『Microservice Patterns』, p.319 계약(두 서비스 간 상호 작용의 구체적인 사례)을 활용 계약의 구조는 서비스 간 상호 작용의 종류마다 다르다. 소비자 측 테스트 컨슈머 어댑터에 대한 테스트로서 계약을 이용하여 프로바이더를 모킹한 스텁을 구성 프로바이더를 실행할 필요 없이 컨슈머 통합 테스트를 작성할 수 있다. 프로바이더 측 테스트 프로바이더의 어댑터에 대한 테스트로서 어댑터의 디펜던시를 목으로 잡아 놓고 계약을 이용하여 어댑터를 테스트 출처 : Chris Richardson, 『Microservice Patterns』, p.320 영속화 테스트 영속화 테스트 영역 서비스의 DB 접근 로직이 잘 동작하는지 확인해야한다. 일반적으로 다음과 같은 절차를 거친다. 설정 - DB 스키마 생성 및 DB 트랜잭션 시작 실행 - DB 작업 수행 확인 - DB 상태, 조회한 객체 assertion 정리 - 트랜잭션 롤백 등 DB에 변경한 내용 undo @RunWith(SpringRunner.class)@SpringBootTest(classes = OrderJpaTestConfiguration.class)public class OrderJpaTest { @Autowired private OrderRepository orderRepository; @Autowired private TransactionTemplate transactionTemplate; @Test public void shouldSaveAndLoadOrder() { long orderId = transactionTemplate.execute((ts) -&gt; { Order order = new Order(CONSUMER_ID, AJANTA_ID, chickenVindalooLineItems()); orderRepository.save(order); return order.getId(); }); transactionTemplate.execute((ts) -&gt; { Order order = orderRepository.findById(orderId).get(); assertNotNull(order); assertEquals(OrderState.APPROVAL_PENDING, order.getState()); assertEquals(AJANTA_ID, order.getRestaurantId()); assertEquals(CONSUMER_ID, order.getConsumerId().longValue()); assertEquals(chickenVindalooLineItems(), order.getLineItems()); return null; }); }}@Configuration@EnableJpaRepositories@EnableAutoConfigurationpublic class OrderJpaTestConfiguration {} 테스트에서 사용된 DB를 어떻게 프로비저닝(시스템을 즉시 사용할 수 있는 상태로 준비해 두는 것) 하느냐가 중요하다 테스트 도중에 DB 인스턴스를 실행하는 효과적인 방법은 도커를 활용하는 것이다. 이러한 방법을 통해 영속화 통합 테스트를 하는 동안에 MySQL 같은 DB를 실행할 수 있다. REST 요청/응답형 상호 작용 테스트 REST 클라이언트/서비스는 REST 끝점 및 요청/응답 본문의 구조에 대해 합의해야 한다.즉, 클라이언트는 정확한 끝점에 HTTP 요청을 보내야하고 서비스는 기대한 응답을 반환해야 한다. REST 요청/응답 테스트 영역 테스트 구성도 출처 : Chris Richardson, 『Microservice Patterns』, p.323 컨슈머 측 테스트 : OrderServiceProxy가 주문 서비스를 올바르게 호출했는지 OrderServiceProxy ⟷ HTTP 스텁 서버(호출될 도메인 서비스의 동작을 흉내) WireMock은 HTTP 서버를 효과적으로 모킹하는 툴로서, HTTP 스텁 서버를 구현할 수 있다. WireMock을 관리하고 계약에 명시된 HTTP 요청에 응답하도록 구성하는 작업은 Spring Cloud Contract의 몫이다. @RunWith(SpringRunner.class)@SpringBootTest(classes=TestConfiguration.class, webEnvironment= SpringBootTest.WebEnvironment.NONE)@AutoConfigureStubRunner(ids = {\"net.chrisrichardson.ftgo:ftgo-order-service-contracts\"})@DirtiesContextpublic class OrderServiceProxyIntegrationTest { @Value(\"${stubrunner.runningstubs.ftgo-order-service-contracts.port}\") private int port; private OrderDestinations orderDestinations; private OrderServiceProxy orderService; @Before public void setUp() throws Exception { orderDestinations = new OrderDestinations(); String orderServiceUrl = \"http://localhost:\" + port; System.out.println(\"orderServiceUrl=\" + orderServiceUrl); orderDestinations.setOrderServiceUrl(orderServiceUrl); orderService = new OrderServiceProxy(orderDestinations, WebClient.create()); } @Test public void shouldVerifyExistingCustomer() { OrderInfo result = orderService.findOrderById(\"99\").block(); assertEquals(\"99\", result.getOrderId()); assertEquals(\"APPROVAL_PENDING\", result.getState()); } @Test(expected = OrderNotFoundException.class) public void shouldFailToFindMissingOrder() { orderService.findOrderById(\"555\").block(); }} @Configurationpublic class TestConfiguration {} 프로바이더 측 테스트 : REST API 끝점이 OrderController에 제대로 구현되었는지 MockMvc/Rest Assured Mock Mvc ⟷ OrderController Spring Cloud Contract는 계약을 이용하여 주문 서비스 통합 테스트 코드를 생성 public abstract class BaseHttp { private StandaloneMockMvcBuilder controllers(Object... controllers) { CommonJsonMapperInitializer.registerMoneyModule(); MappingJackson2HttpMessageConverter converter = new MappingJackson2HttpMessageConverter(JSonMapper.objectMapper); return MockMvcBuilders.standaloneSetup(controllers).setMessageConverters(converter); } @Before public void setup() { OrderService orderService = mock(OrderService.class); OrderRepository orderRepository = mock(OrderRepository.class); OrderController orderController = new OrderController(orderService, orderRepository); when(orderRepository.findById(OrderDetailsMother.ORDER_ID)).thenReturn(Optional.of(OrderDetailsMother.CHICKEN_VINDALOO_ORDER)); when(orderRepository.findById(555L)).thenReturn(empty()); RestAssuredMockMvc.standaloneSetup(controllers(orderController)); }} public class OrderControllerTest extends BaseHttp { @Test public void test() { // do something }} 발행/구독 상호 작용 테스트 발행기/컨슈머가 바라보는 메세지 채널 및 도메인 이벤트 구조가 서로 일치하는지 확인해야한다. 발행/구독 테스트 영역 테스트 구성도 출처 : Chris Richardson, 『Microservice Patterns』, p.327 프로바이더 측 테스트 : OrderDomainEventPublisher가 계약대로 이벤트를 발행하는지 확인@RunWith(SpringRunner.class)@SpringBootTest(classes = MessagingBase.TestConfiguration.class, webEnvironment = SpringBootTest.WebEnvironment.NONE)@AutoConfigureMessageVerifierpublic abstract class MessagingBase { @Configuration @EnableAutoConfiguration @Import({EventuateContractVerifierConfiguration.class, TramEventsPublisherConfiguration.class, TramInMemoryConfiguration.class}) public static class TestConfiguration { @Bean public OrderDomainEventPublisher orderAggregateEventPublisher(DomainEventPublisher eventPublisher) { return new OrderDomainEventPublisher(eventPublisher); } } @Autowired private OrderDomainEventPublisher orderAggregateEventPublisher; protected void orderCreated() { orderAggregateEventPublisher.publish(CHICKEN_VINDALOO_ORDER, Collections.singletonList(new OrderCreatedEvent(CHICKEN_VINDALOO_ORDER_DETAILS, AJANTA_RESTAURANT_NAME))); }}class MessageTest extends MessagingBase { @Test public void validate_orderCreatedEvent() { // 메세지가 기대한 채널로 발행되었는지 확인 }} 컨슈머 측 테스트 : OrderHistoryEventHandlers가 계약대로 이벤트를 소비하는지 확인 각 테스트 메서드는 스플링 클라우드를 호출해서 계약에 명시된 이벤트 발행 OrderHistoryEventHandlers가 OrderHistoryDao를 올바르게 호출하는지 확인 @RunWith(SpringRunner.class)@SpringBootTest(classes = OrderHistoryEventHandlersTest.TestConfiguration.class, webEnvironment = SpringBootTest.WebEnvironment.NONE)@AutoConfigureStubRunner(ids = {\"net.chrisrichardson.ftgo:ftgo-order-service-contracts\"} )@DirtiesContextpublic class OrderHistoryEventHandlersTest { @Configuration @EnableAutoConfiguration @Import({OrderHistoryServiceMessagingConfiguration.class, TramCommandProducerConfiguration.class, TramInMemoryConfiguration.class, EventuateContractVerifierConfiguration.class}) public static class TestConfiguration { @Bean public ChannelMapping channelMapping() { return new DefaultChannelMapping.DefaultChannelMappingBuilder().build(); } @Bean public OrderHistoryDao orderHistoryDao() { return mock(OrderHistoryDao.class); } } @Autowired private StubFinder stubFinder; @Autowired private OrderHistoryDao orderHistoryDao; @Test public void shouldHandleOrderCreatedEvent() throws InterruptedException { when(orderHistoryDao.addOrder(any(Order.class), any(Optional.class))).thenReturn(false); stubFinder.trigger(\"orderCreatedEvent\"); // orderCreatedEvent 스텁을 트리거하여 OrderCreated 이벤트 발생 eventually(() -&gt; { // OrderHistoryEventHandlers가 orderHistoryDao.addOrder() 호출했는지 확인 ArgumentCaptor&lt;Order&gt; orderArg = ArgumentCaptor.forClass(Order.class); ArgumentCaptor&lt;Optional&lt;SourceEvent&gt;&gt; sourceEventArg = ArgumentCaptor.forClass(Optional.class); verify(orderHistoryDao).addOrder(orderArg.capture(), sourceEventArg.capture()); Order order = orderArg.getValue(); Optional&lt;SourceEvent&gt; sourceEvent = sourceEventArg.getValue(); assertEquals(\"Ajanta\", order.getRestaurantName()); }); }}비동기 요청/응답 상호 작용 테스트 예를 들어, 주문 서비스는 주방 서비스 등 여러 서비스에 커맨드 메세지를 전송하고 수신한 응답 메세지를 사가로 처리한다.따라서, 커맨드를 전송하는 서비스인 ‘요청자’와 커맨드 처리 후 응답을 반환하는 서비스인 ‘응답자’가 바라보는커맨드 메세지 채널명과 커맨드/응답 메세지의 구조는 반드시 일치해야 한다. 비동기 요청/응답 테스트 영역 테스트 구성도 출처 : Chris Richardson, 『Microservice Patterns』, p.331 컨슈머 측(주문 서비스) 테스트 : KitchenServiceProxy가 커맨드 메세지를 전송하고 응답 메세지를 제대로 처리하는지 확인@RunWith(SpringRunner.class)@SpringBootTest(classes= KitchenServiceProxyIntegrationTest.TestConfiguration.class, webEnvironment= SpringBootTest.WebEnvironment.NONE)@AutoConfigureStubRunner(ids = // 주방 서비스 스텁이 메세지에 응답하도록 구성 {\"net.chrisrichardson.ftgo:ftgo-kitchen-service-contracts\"} )@DirtiesContextpublic class KitchenServiceProxyIntegrationTest { @Configuration @EnableAutoConfiguration @Import({TramCommandProducerConfiguration.class, TramInMemoryConfiguration.class, EventuateContractVerifierConfiguration.class}) public static class TestConfiguration { // TramSagaInMemoryConfiguration @Bean public DataSource dataSource() { EmbeddedDatabaseBuilder builder = new EmbeddedDatabaseBuilder(); return builder.setType(EmbeddedDatabaseType.H2) .addScript(\"eventuate-tram-embedded-schema.sql\") .addScript(\"eventuate-tram-sagas-embedded.sql\") .build(); } @Bean public EventuateTramRoutesConfigurer eventuateTramRoutesConfigurer(BatchStubRunner batchStubRunner) { return new EventuateTramRoutesConfigurer(batchStubRunner); } @Bean public SagaMessagingTestHelper sagaMessagingTestHelper() { return new SagaMessagingTestHelper(); } @Bean public SagaCommandProducer sagaCommandProducer() { return new SagaCommandProducer(); } @Bean public KitchenServiceProxy kitchenServiceProxy() { return new KitchenServiceProxy(); } } @Autowired private SagaMessagingTestHelper sagaMessagingTestHelper; @Autowired private KitchenServiceProxy kitchenServiceProxy; @Test public void shouldSuccessfullyCreateTicket() { CreateTicket command = new CreateTicket(AJANTA_ID, OrderDetailsMother.ORDER_ID, new TicketDetails(Collections.singletonList(new TicketLineItem(CHICKEN_VINDALOO_MENU_ITEM_ID, CHICKEN_VINDALOO, CHICKEN_VINDALOO_QUANTITY)))); CreateTicketReply expectedReply = new CreateTicketReply(OrderDetailsMother.ORDER_ID); String sagaType = CreateOrderSaga.class.getName(); CreateTicketReply reply = sagaMessagingTestHelper // 커맨드 전송 및 응답 대기 .sendAndReceiveCommand(kitchenServiceProxy.create, command, CreateTicketReply.class, sagaType); assertEquals(expectedReply, reply); // 응답 확인 }} 프로바이더 측(주방 서비스) 테스트 : KitchenServiceCommandHandler가 커맨드 처리 후 응답을 반환하는지 확인@RunWith(SpringRunner.class)@SpringBootTest(classes = AbstractKitchenServiceConsumerContractTest.TestConfiguration.class, webEnvironment = SpringBootTest.WebEnvironment.NONE)@AutoConfigureMessageVerifierpublic abstract class AbstractKitchenServiceConsumerContractTest { @Configuration @Import({KitchenServiceMessageHandlersConfiguration.class, EventuateContractVerifierConfiguration.class}) public static class TestConfiguration { @Bean public KitchenService kitchenService() { return mock(KitchenService.class); } } @Autowired private KitchenService kitchenService; @Before public void setup() { reset(kitchenService); when(kitchenService.createTicket(eq(1L), eq(99L), any(TicketDetails.class))) .thenReturn(new Ticket(1L, 99L, new TicketDetails(Collections.emptyList()))); }}참고자료 크리스 리처드슨, 『마이크로서비스 패턴』, 길벗(2020), 10장" }, { "title": "MSA 환경에서 테스트하기(1) - 단위 테스트", "url": "/posts/msa-testing-part1/", "categories": "", "tags": "", "date": "2021-07-10 22:25:00 +0900", "snippet": "※ 해당 내용은 ‘마이크로서비스 패턴(크리스 리처드슨)’ 9장을 읽고 필요한 부분을 정리한 내용입니다.테스트 개요자동화 테스트 작성 설정 → 실행 → 확인 → 정리 설정 - SUT(System Under Test, 테스트할 대상)와 그 디펜던시로 구성된 테스트 픽스처(test fixture)를 초기화한다. 실행 - SUT 호출 (ex : 테스트 ...", "content": "※ 해당 내용은 ‘마이크로서비스 패턴(크리스 리처드슨)’ 9장을 읽고 필요한 부분을 정리한 내용입니다.테스트 개요자동화 테스트 작성 설정 → 실행 → 확인 → 정리 설정 - SUT(System Under Test, 테스트할 대상)와 그 디펜던시로 구성된 테스트 픽스처(test fixture)를 초기화한다. 실행 - SUT 호출 (ex : 테스트 클래스의 특정 메서드) 확인 - 호출 결과 및 SUT의 상태를 단언(assertion) 정리 - 필요한 경우 설정 단계에서 초기화한 DB 트랜잭션을 롤백하는 등의 뒷정리Stub/Mock을 이용한 테스트 SUT는 대부분 디펜던시를 갖고 있고, 이런 디펜던시 때문에 테스트가 복잡하고 느려질 수 있다. 예를 들어, OrderController 클래스는OrderService를 호출하고, OrderService 역시 다른 수많은 애플리케이션/인프라 서비스에 의존할 수 있다. 이런 경우,OrderController만 따로 테스트하고 싶다면 ? SUT가 의존하는 디펜던시를 테스트 더블(디펜던시의 동작을 흉내낸 객체)로 대체한다. 테스트 더블은 스텁, 목 두 종류가 있다. 스텁(stub) - SUT에 값을 반환하는 객체 목(mock) - 스텁의 일종으로, SUT가 정확하게 디펜던시를 호출했는지 확인하는 객체 일반적으로 테스트 더블은 목 객체 프레임워크인 Mockito를 활용해서 구현한다. 출처 : Chris Richardson, 『Microservice Patterns』, p.296 자동화 테스트의 종류 일반적으로 ‘범위’에 따라 테스트는 다음과 같이 나뉜다. 단위 테스트(unit test) - 서비스의 작은 부분(ex : 클래스)을 테스트 통합 테스트(integration test) - 테스트 대상 서비스가 인프라 서비스, 타 서비스 등과 연동되어 잘 작동하는지 테스트 컴포넌트 테스트(component test) - 개별 서비스에 대한 인수 테스트(acceptance test) 종단 간 테스트(end-to-end test) - 전체 애플리케이션에 대한 인수 테스트 종단 간 테스트는 중간에 있는 수 많은 디펜던시까지 실행시켜야 하기 때문에, 가능한 한 작성하지 않는 것이 최선이다. 출처 : Chris Richardson, 『Microservice Patterns』, p.299 단위 테스트 작성 단위 테스트는 서비스의 아주 작은 부속품인 단위(unit)가 제대로 동작하는지 확인하는 테스트이다.일반적으로 단위는 클래스이므로 단위 테스트의 목표는 해당 클래스가 잘 동작하는지 확인하는 것이다. 단위 테스트에는 두 가지 종류가 있다. 독립(solitary) 단위 테스트 - 클래스 디펜던시를 목 객체로 나타내고 클래스를 따로 테스트 공동(sociable) 단위 테스트 - 클래스와 디펜던시를 테스트 어떤 종류의 단위테스트를 할지는 클래스의 책임과 아키텍처에서의 역할마다 다르다. 다음은 일반적으로 많이 쓰는 테스트 전략이다. 엔티티와 값 객체(Value Object) 같은 도메인 객체는 공동 단위 테스트 수행 여러 서비스에 걸쳐 데이터 일관성을 유지하는 사가는 공동 단위 테스트 수행 컨트롤러와 도메인 서비스 클래스는 독립 단위 테스트 수행 인바운드/아웃바운드 메시징 게이트웨이는 독립 단위 테스트 수행 출처 : Chris Richardson, 『Microservice Patterns』, p.308 도메인 서비스 테스트 이 클래스에 있는 메서드는 일반적으로 엔티티와 리파지토리를 호출하며 도메인 이벤트를 발행한다. 이런 종류의 클래스를 효과적으로 테스트 하려면,리파지토리 및 메시징 클래스 같은 디펜던시를 모킹(mocking)하고 독립 단위 테스트를 수행해야 한다. 일반적으로 다음과 같은 프로세스로 진행된다. 설정 : 서비스 디펜던시의 목 객체를 구성 실행 : 서비스 메서드를 호출 확인 : 서비스 메서드가 올바른 값을 반환하고 디펜던시가 올바르게 호출되었는지 확인 public class OrderServiceTest { private OrderService orderService; private OrderRepository orderRepository; private DomainEventPublisher eventPublisher; private RestaurantRepository restaurantRepository; private SagaManager&lt;CreateOrderSagaState&gt; createOrderSagaManager; private SagaManager&lt;CancelOrderSagaData&gt; cancelOrderSagaManager; private SagaManager&lt;ReviseOrderSagData&gt; reviseOrderSagaManager; private OrderDomainEventPublisher orderAggregateEventPublisher; @Before public void setup() { orderRepository = mock(OrderRepository.class); eventPublisher = mock(DomainEventPublisher.class); restaurantRepository = mock(RestaurantRepository.class); createOrderSagaManager = mock(SagaManager.class); cancelOrderSagaManager = mock(SagaManager.class); reviseOrderSagaManager = mock(SagaManager.class); // Mock DomainEventPublisher AND use the real OrderDomainEventPublisher orderAggregateEventPublisher = mock(OrderDomainEventPublisher.class); orderService = new OrderService(orderRepository, eventPublisher, restaurantRepository, createOrderSagaManager, cancelOrderSagaManager, reviseOrderSagaManager, orderAggregateEventPublisher, Optional.empty()); } @Test public void shouldCreateOrder() { when(restaurantRepository.findById(AJANTA_ID)).thenReturn(Optional.of(AJANTA_RESTAURANT)); when(orderRepository.save(any(Order.class))).then(invocation -&gt; { Order order = (Order) invocation.getArguments()[0]; order.setId(ORDER_ID); return order; }); Order order = orderService.createOrder(CONSUMER_ID, AJANTA_ID, CHICKEN_VINDALOO_MENU_ITEMS_AND_QUANTITIES); verify(orderRepository).save(same(order)); verify(orderAggregateEventPublisher).publish(order, Collections.singletonList(new OrderCreatedEvent(CHICKEN_VINDALOO_ORDER_DETAILS, RestaurantMother.AJANTA_RESTAURANT_NAME))); verify(createOrderSagaManager).create(new CreateOrderSagaState(ORDER_ID, CHICKEN_VINDALOO_ORDER_DETAILS), Order.class, ORDER_ID); } // TODO write tests for other methods}컨트롤러 테스트 컨트롤러 클래스는 각각 지정된 REST API 끝점을 담당한 여러 메서드로 구성된다. 메서드의 매개변수는 경로 변수(path variable)처럼HTTP 요청에서 추출된 값을 나타낸다. 컨트롤러 메서드는 도메인 서비스 또는 리파지토리를 호출해서 응답 객체를 반환한다. 컨트롤러에서 호출하는 도메인 서비스, 리파지토리 같은 것들을 모킹하여 컨트롤러에 대해 독립 단위 테스트를 수행하는 것이 좋다. 컨트롤러 클래스를 인스턴스화하고 메서드를 호출할 수도 있지만, 이렇게 하면 요청 라우팅 같은 중요한 기능은 테스트할 수 없다. 따라서 목 MVC 테스트 프레임워크를 활용하는 것이 효율적이다. Spring MockMvc, Rest Assured Mock이 대표적인 예이다. HTTP 요청을 보내서 반환된 HTTP 응답을 단언(assertion)할 수 있기 때문에진짜 네트워크 호출을 하지 않아도 HTTP 요청 라우팅 및 자바 객체 ⟷ JSON 변환이 가능하다. 출처 : https://terasolunaorg.github.io/guideline/5.4.1.RELEASE/en/UnitTest/ImplementsOfUnitTestpublic class OrderControllerTest { private OrderService orderService; private OrderRepository orderRepository; private OrderController orderController; @Before public void setUp() throws Exception { orderService = mock(OrderService.class); orderRepository = mock(OrderRepository.class); orderController = new OrderController(orderService, orderRepository); } @Test public void shouldFindOrder() { when(orderRepository.findById(1L)).thenReturn(Optional.of(CHICKEN_VINDALOO_ORDER)); given(). standaloneSetup(configureControllers(orderController)). when(). get(\"/orders/1\"). then(). statusCode(200). body(\"orderId\", equalTo(new Long(OrderDetailsMother.ORDER_ID).intValue())). body(\"state\", equalTo(OrderDetailsMother.CHICKEN_VINDALOO_ORDER_STATE.name())). body(\"orderTotal\", equalTo(CHICKEN_VINDALOO_ORDER_TOTAL.asString())) ; } @Test public void shouldFindNotOrder() { when(orderRepository.findById(1L)).thenReturn(Optional.empty()); given(). standaloneSetup(configureControllers(new OrderController(orderService, orderRepository))). when(). get(\"/orders/1\"). then(). statusCode(404) ; } private StandaloneMockMvcBuilder configureControllers(Object... controllers) { CommonJsonMapperInitializer.registerMoneyModule(); MappingJackson2HttpMessageConverter converter = new MappingJackson2HttpMessageConverter(JSonMapper.objectMapper); return MockMvcBuilders.standaloneSetup(controllers).setMessageConverters(converter); }}이벤트/메세지 핸들러 테스트 메세지 어댑터는 컨트롤러처럼 도메인 서비스, 레파지토리 등을 호출하는 단순 클래스이다.즉, 메세지 어댑터의 각 메서드는 메세지/이벤트에서 꺼낸 데이터를 서비스 메서드에 넘겨 호출한다.따라서, 메세지 어댑터는 컨트롤러와 비슷한 방법으로 단위 테스트를 수행할 수 있다. 테스트별로 메세지 어탭터 인스턴스를 생성하고 메세지를 채널에 전송한 후, 서비스 목이 정확히 호출되었는지 확인한다. 메세징 인프라는 스터빙하기 때문에 실제 메세지 브로커는 관여하지 않는다. Eventuate Tram Mock Messaging 프레임워크를 이용해서 테스트한다.public class OrderEventConsumerTest { private OrderService orderService; private OrderEventConsumer orderEventConsumer; @Before public void setUp() throws Exception { orderService = mock(OrderService.class); orderEventConsumer = new OrderEventConsumer(orderService); } @Test public void shouldCreateMenu() { CommonJsonMapperInitializer.registerMoneyModule(); given(). eventHandlers(orderEventConsumer.domainEventHandlers()). when(). aggregate(\"net.chrisrichardson.ftgo.restaurantservice.domain.Restaurant\", AJANTA_ID). publishes(new RestaurantCreated(AJANTA_RESTAURANT_NAME, RestaurantMother.AJANTA_RESTAURANT_MENU)). then(). verify(() -&gt; { verify(orderService).createMenu(AJANTA_ID, AJANTA_RESTAURANT_NAME, new RestaurantMenu(RestaurantMother.AJANTA_RESTAURANT_MENU_ITEMS)); }) ; }}참고자료 크리스 리처드슨, 『마이크로서비스 패턴』, 길벗(2020), 9장" }, { "title": "엔티티에서 데이터베이스 키워드/예약어 사용시 발생하는 문제", "url": "/posts/jpa-entity-table-reserved-keyword/", "categories": "경험하기, 이슈 노트", "tags": "JPA", "date": "2021-07-07 00:29:00 +0900", "snippet": "상황 주문 테이블에 맵핑되는 엔티티를 만들기 위해 ‘Order’라는 클래스 생성 @Getter@Setter@Entitypublic class Order { @Id private Long id; private OrderStatus orderStatus;} 스프링 부트 실행하고 h2 db에 테이블 생성되나 확인하려고 하는데 아래와...", "content": "상황 주문 테이블에 맵핑되는 엔티티를 만들기 위해 ‘Order’라는 클래스 생성 @Getter@Setter@Entitypublic class Order { @Id private Long id; private OrderStatus orderStatus;} 스프링 부트 실행하고 h2 db에 테이블 생성되나 확인하려고 하는데 아래와 같은 에러 발생하면서 테이블 생성이 안됨 Caused by: org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement \" CREATE TABLE ORDER[*] ( ID BIGINT NOT NULL, ORDER_STATUS INTEGER, PRIMARY KEY (ID) ) \"; expected \"identifier\"; SQL statement: 원인 Order라는 클래스 이름이 데이터베이스 키워드/예약어(예. ORDER, GROUP, SELECT, WHERE 등)에 이미 있기 때문에 스키마 생성에 실패한 것해결방법1. escape 문자(\\) 사용@Getter@Setter@Entity(name = \"\\\"order\\\"\")public class Order { @Id private Long id; private OrderStatus orderStatus;}2. backtick 문자(`) 사용@Getter@Setter@Entity(name = \"`order`\")public class Order { @Id private Long id; private OrderStatus orderStatus;}3. globally_quoted_identifiers 설정값 세팅4. 키워드/예약어 사용하지 않기※ 1~3 번의 경우 네이티브 쿼리 작성시 테이블 명을 “order”로 적어줘야 한다.선택한 방법 네이티브 쿼리 사용시에도 불편하고, 김영한님 강의에서도 관례적으로 ORDERS라는 이름으로 사용한다고 하니 테이블 명을 ORDERS로 선언하자 @Getter@Setter@Entity(name = \"ORDERS\")public class Order { @Id private Long id; private OrderStatus orderStatus;} 참고 자료 https://www.chrouki.com/posts/escape-sql-reserved-keywords-jpa-hibernate/ popit.kr/가짜뉴스아웃-하이버네이트-데이터베이스-스키마/" }, { "title": "엔티티에 있는 enum 타입 필드를 DB 컬럼에 어떻게 맵핑시킬 수 있을까 ?", "url": "/posts/enum-field-in-entity/", "categories": "", "tags": "", "date": "2021-07-07 00:29:00 +0900", "snippet": " 예를 들어 주문 엔티티의 ‘주문 상태’와 같이 enum 타입으로 선언하기 적합한 필드들이 있다. 문자열, 숫자, 날짜 등도 아닌 enum 타입은 DB에 어떻게 저장시켜야할까 ?@Enumerated1. EnumType.ORDINAL @Enumerated(EnumType.ORDINAL)을 사용하게되면 JPA는 ENUM 클래스의 ordinal()메서드...", "content": " 예를 들어 주문 엔티티의 ‘주문 상태’와 같이 enum 타입으로 선언하기 적합한 필드들이 있다. 문자열, 숫자, 날짜 등도 아닌 enum 타입은 DB에 어떻게 저장시켜야할까 ?@Enumerated1. EnumType.ORDINAL @Enumerated(EnumType.ORDINAL)을 사용하게되면 JPA는 ENUM 클래스의 ordinal()메서드를 사용하여 값을 저장한다(0부터 시작해서 상수가 선언된 순서대로 값이 맵핑) 따라서, 아래의 경우는 DB의 Article 테이블 status 컬럼에 0이 저장된다. 문제점 중간에 새 값을 추가하거나 순서를 재정렬하면 기존 데이터 정합성이 깨진다. public enum Status { OPEN, REVIEW, APPROVED, REJECTED;}@Entitypublic class Article { @Id private int id; private String title; @Enumerated(EnumType.ORDINAL) private Status status;}Article article = new Article();article.setId(1);article.setTitle(\"ordinal title\");article.setStatus(Status.OPEN);2. EnumType.STRING @Enumerated(EnumType.STRING)을 사용하게되면 JPA는 ENUM 클래스의 name()메서드를 사용하여 값을 저장한다(상수 이름을 문자열로 반환) 따라서, 아래의 경우는 DB의 Article 테이블 status 컬럼에 OPEN이 저장된다. 문제점 enum 값을 변경하게 되면 기존 데이터 정합성이 깨진다. 필요 이상으로 많은 공간을 소비한다. @Entitypublic class Article { @Id private int id; private String title; @Enumerated(EnumType.STRING) private Status status;}Article article = new Article();article.setId(1);article.setTitle(\"ordinal title\");article.setStatus(Status.OPEN);@PostLoad, @PrePersist JPA 콜백 메서드인 @PostLoad, @PrePersist를 활용하여 DB에 저장할 때의 값과 가져와서 맵핑할 값을 분리한다. @PrePersist : 새로운 엔티티를 저장하기 이전에 호출됨 @PostPersist : 새로운 엔티티를 저장한 이후에 호출됨 아래와 같이 사용하게 되면 DB의 Article 테이블 priorityValue 컬럼에 300이 저장된다. 문제점 하나의 값을 위해 두 개의 속성을 가져야한다. JPQL 쿼리에서 Enum 값을 사용할 수 없다. public enum Priority { LOW(100), MEDIUM(200), HIGH(300); private int priority; private Priority(int priority) { this.priority = priority; } public int getPriority() { return priority; } public static Priority of(int priority) { return Stream.of(Priority.values()) .filter(p -&gt; p.getPriority() == priority) .findFirst() .orElseThrow(IllegalArgumentException::new); }}@Entitypublic class Article { @Id private int id; private String title; @Basic private int priorityValue; // DB에 저장될 값 @Transient // 엔티티 객체의 데이터와 테이블의 컬럼과 매핑하고 있는 관계를 제외하기 위해 사용 private Priority priority; // DB에서 가져와서 맵핑될 값 @PostLoad void fillTransient() { if (priorityValue &gt; 0) { this.priority = Priority.of(priorityValue); } } @PrePersist void fillPersistent() { if (priority != null) { this.priorityValue = priority.getPriority(); } }}Article article = new Article();article.setId(3);article.setTitle(\"callback title\");article.setPriority(Priority.HIGH);@Converter 위 방법들의 한계를 극복하기 위해, JPA 2.1에서는 엔티티 속성을 DB 값으로 변환할 수 있도록 @Converter, AttributeConverter를 제공한다. 아래와 같이 사용하게 되면 DB의 Article 테이블 category 컬럼에 M이 저장된다. 장점 새로운 값을 추가하거나 상수명을 바꾼다 하더라도 기존 데이터에 지장을 주지않는다. 하나의 엔티티 속성으로 DB 컬럼과 맵핑될 수 있다. 굳이 단점(?)이라고 한다면 부가적인 코드(생성자, getter 등)가 추가된다는 점 일것 같다.public enum Category { SPORT(\"S\"), MUSIC(\"M\"), TECHNOLOGY(\"T\"); private String code; private Category(String code) { this.code = code; } public String getCode() { return code; }}@Entitypublic class Article { @Id private int id; private String title; private Category category;}@Converter(autoApply = true) // Category 타입으로 선언된 필드에 자동으로 컨버팅 적용public class CategoryConverter implements AttributeConverter&lt;Category, String&gt; { @Override public String convertToDatabaseColumn(Category category) { if (category == null) { return null; } return category.getCode(); } @Override public Category convertToEntityAttribute(String code) { if (code == null) { return null; } return Stream.of(Category.values()) .filter(c -&gt; c.getCode().equals(code)) .findFirst() .orElseThrow(IllegalArgumentException::new); }}Article article = new Article();article.setId(4);article.setTitle(\"converted title\");article.setCategory(Category.MUSIC);그럼 무조건 @Converter를 사용하면 될까? 위 내용을 공부를 하고 난 뒤 인프런 김영한님 강의 를 듣던 중@Enumerated(EnumType.String)을 사용하시는 것을 보고 질문을 남겼고 갓영한님의 답변을 들을 수 있었다.[질문]안녕하세요 선생님 강의 재밌게 잘 듣고 있습니다 !강의를 듣기 전에 스스로 고민해보는 시간을 갖기 위해 공부하다가@Enumerated(EnumType.String) 경우는 선언한 상수의 이름이 바뀌게 되면(바뀔일은 드물겠지만)기존에 저장됐던 데이터 와의 정합성 문제가 있고,문자열로 선언되어 필요 이상으로 많은 공간을 할당하게 된다는 문제점이 있다고 봤습니다.결과적으로 해당 글에서는 JPA2.1 부터 도입된 @Converter를 사용하는 것을 추천한다고 하는데 실무에서는 어떤지 궁금합니다!혹시나 해서 해당 글 출처 남깁니다 : https://www.baeldung.com/jpa-persisting-enums-in-jpa[답변]안녕하세요. 이재윤님이부분은 선택인데요.저의 경우 실무에서도 @Enumerated(EnumType.String)을 주로 사용합니다.@Converter를 통해서 좀 더 최적화를 하셔도 됩니다. 그런데 ENUM의 길이가 그렇게 긴 것도 아니고,최근 DB 성능도 좋아서 이 부분을 최적화해도 크게 많은 이점을 얻지는 못합니다.감사합니다.더 공부해야할 부분 JPA Entity 라이프사이클 @Transient, @Basic 등과 영속성참고자료 https://www.baeldung.com/jpa-persisting-enums-in-jpa https://www.baeldung.com/jpa-entity-lifecycle-events" }, { "title": "WEB - HTTP의 연결모델", "url": "/posts/http-connection-model/", "categories": "지식 더하기, 이론", "tags": "WEB", "date": "2021-06-30 00:25:00 +0900", "snippet": " connection management allows considerable boosting of performance in HTTPShort-lived connections HTTP 통신을 한 번 할 때마다 TCP에 의해 연결/종료 된다. 따라서, 매 통신 마다 새로운 연결을 맺어야한다. 초기 통신에서는 작은 사이즈의 텍스트를 보내는 정도였기...", "content": " connection management allows considerable boosting of performance in HTTPShort-lived connections HTTP 통신을 한 번 할 때마다 TCP에 의해 연결/종료 된다. 따라서, 매 통신 마다 새로운 연결을 맺어야한다. 초기 통신에서는 작은 사이즈의 텍스트를 보내는 정도였기 때문에 별 문제가 되지 않았다. 하지만, HTTP가 널리 보급되어감에 따라 하나의 HTML 문서에 여러 이미지, js 파일 등이 포함되고 그것들을 획득하기 위해 여러번 요청을 보내야한다. 즉, 아래와 같은 프로세스를 매 요청마다 반복해야 하는 것이다.장/단점장점 CPU, 메모리와 같은 서버의 자원을 지속적으로 점유하지 않는다.단점 매 요청마다 TCP 연결/종료가 발생하기 때문에 통신량이 늘어나게 된다.Persistent Connection HTTP/1.1과 일부 HTTP/1.0에서는 TCP 연결 문제를 해결하기 위해 ‘지속연결’ 이라는 방법을 고안했다. 어느 한쪽이 명시적으로 연결을 종료하지 않는 이상 TCP 연결을 계속 유지한다. HTTP/1.1에서는 표준 동작이지만 HTTP/1.0에서는 정식 사양이 아니었다. 클라이언트, 서버 모두 지속 연결을 지원해야 지속 연결이 가능하다. 출처 : https://developer.mozilla.org/en-US/docs/Web/HTTP/Connection_management_in_HTTP_1.x장/단점장점 TCP 연결/종료에 대한 오버헤드가 줄어들기 때문에 서버에 대한 부하가 경감된다. 오버헤드를 줄인 만큼 HTTP 요청/응답이 더 빠르게 이루어진다. 여러 요청을 병행해서 보낼 수 있다. 즉, 요청에 대한 응답이 오기전에 다른 요청을 또 보낼 수 있다(파이프라인화) 새로운 연결 및 TLS 핸드셰이크 감소로 CPU 사용량 및 왕복이 감소한다.단점 필요한 모든 데이터가 수신되었을 때 클라이언트가 연결을 닫지 않으면 서버에서 연결을 유지하는 데 필요한 리소스를 다른 클라이언트에서 사용할 수 없게 된다. 서버가 TCP 연결을 닫는 동시에 클라이언트가 서버에 요청을 전송하는 경우, 경합 조건(race condition)이 발생할 수 있다. 따라서, 서버는 연결을 닫기 직전에 클라이언트에 408 Request Timeout 상태 코드를 전송해야 하며,요청을 전송한 후 클라이언트가 408 상태 코드를 수신하면 서버에 대한 새 연결을 열고 요청을 다시 보낼 수 있다. 일부 클라이언트는 요청을 재전송하지 않으며, 요청을 재전송하는 많은 클라이언트는 요청에 역 HTTP 메서드가 있는 경우에만 재전송한다. 과부하 상태에서는 DoS 공격을 당할 수 있습니다.keep-alive Connection : keep-alive HTTP 헤더 속성 중 하나이다. HTTP 1.0에서는 keep-alive 헤더가 포함되지 않는 한 연결이 지속되지 않는 것으로 간주되었다. HTTP 1.1에서는 기본 속성이 되었다. 원하지 않는 경우엔 Connection : close를 헤더에 추가하면 ‘Short-lived connections’ 연결 모델을 사용한다. keep-alive Timeout Socket에 I/O Access가 마지막으로 종료된 시점부터 정의된 시간까지 Access가 없더라도 세션을 유지하는 구조이다. 서버 자원은 유한하기 때문에, 여러 개의 커넥션을 계속 유지하게 되면 서버의 가용성을 떨어뜨릴 수 있다. HTTP pipelining 커넥션 지연을 회피하고자 같은 영속적인 커넥션을 통해서, 응답을 기다리지 않고 요청을 연속적으로 보내는 기능이다. GET, HEAD, PUT, DELETE 메서드같은 idempotent(멱등한) 메서드만 가능하다(POST는 불가능).왜냐하면 실패가 발생한 경우에는 단순히 파이프라인 컨텐츠를 다시 반복하면 되기 때문이다. 대부분의 모던 브라우저는 이 기능을 기본적으로 활성화하지 않는다. 모든 HTTP/1.1 호환 프록시와 서버들은 파이프라이닝을 지원해야 하지만, 실제로는 많은 프록시와 서버들은 제한을 가지고 있다. 버그가 있는 프록시들이 많은데, 이들은 웹 개발자들이 쉽게 예상하거나 분석하기 힘든 이상하고 오류가 있는 동작을 야기한다. 파이프라이닝은 정확히 구현해내기 복잡합니다: 전송 중인 리소스의 크기, 사용될 효과적인 RTT, 그리고 효과적인 대역폭은 파이프라인이 제공하는 성능 향상에 직접적으로 영향을 미친다. 파이프라이닝은 HOL(Head-of-line blocking) 문제에 영향을 받습니다. 이런 이유들로, 파이프라이닝은 더 나은 알고리즘인 멀티플렉싱으로 대체되었는데, 이는 HTTP/2에서 사용된다. ※ HOL(Head-of-line blocking) : 단일 TCP 연결에서, 이전 요청이 완료될 때까지 이후의 요청이 기다려야 하는 것 출처 : https://developer.mozilla.org/en-US/docs/Web/HTTP/Connection_management_in_HTTP_1.xMultiplexing 여러가지의 자원을 한 번의 요청을 통해 받을 수 있어 client마다 하나의 TCP connection이면 충분하다는 개념이다 HTTP2.0부터 지원 가능하게된 배경 HTTP2에서는 데이터를 전송할 때, 일반 문자열이 아닌 바이너리로 인코딩하여 전송한다. 바이너리 포맷의 데이터를 사용하게 되어, 이전에는 하나로 모여있던 데이터를 프레임이라는 단위로 나눠서 관리 / 전송할 수 있게되었다. HTTP2에서는 Frame과 Stream이라는 개념이 추가되었다. Frame은 HTTP2 통신에서 데이터를 주고받을 수 있는 가장 작은 단위이다. 헤더 프레임, 데이터 프레임으로 구성되어 있다. 스트림은 클라이언트와 서버 사이에 맺어진 연결을 통해 양방향으로 데이터를 주고받는 한개 이상의 메시지를 의미한다. 메시지는 HTTP1.1처럼 요청과 응답의 단위이다. 메시지는 다수의 Frame으로 구성되어 있다. 즉, 프레임 → 메시지 → 스트림이 되는 구조이다 출처 : https://developers.google.com/web/fundamentals/performance/http2 HTTP2에서는 스트림 하나가 다수개의 요청과 응답을 처리할 수 있는 구조로 바뀌었다. HTTP1 시절에는, 요청과 응답이 메시지라는 단위로 완벽하게 구분되어 있었지만, HTTP2에서는 스트림이라는 단위로 요청과 응답이 묶일 수 있는 구조가 만들어졌다. 따라서, 응답 프레임들은 요청 순서에 상관없이 만들어진 순서대로 클라이언트에 전달될 수 있다. 결과적으로, HTTP1 때처럼 중간에 응답이 막히면 대기하고 있던 Response들이 모두 기다려야하는 HOL 이슈에서 벗어날 수 있게 되었다. 출처 : https://developers.google.com/web/fundamentals/performance/http2 출처 : https://www.programmersought.com/article/86161816904/더 공부해야할 부분 TCP/IP HTTP2, HTTP3 HTTP Connection Pooling참고 자료 우에노 센, 『그림으로 배우는 Http &amp; Network Basic』, 영진닷컴(2015) https://www.tutorialspoint.com/Connectionless-Services https://en.wikipedia.org/wiki/HTTP_persistent_connection https://developer.mozilla.org/en-US/docs/Web/HTTP/Connection_management_in_HTTP_1.x https://goodgid.github.io/HTTP-Keep-Alive/ https://nuli.navercorp.com/community/article/1132452?email=true https://americanopeople.tistory.com/115 https://developers.google.com/web/fundamentals/performance/http2" }, { "title": "Message Broker 비교해보기", "url": "/posts/message-broker-comparison/", "categories": "", "tags": "", "date": "2021-06-29 00:25:00 +0900", "snippet": " ‘나만의 웨딩 플래너’라는 MSA 기반의 토이 프로젝트를 진행하는데 필요한 Message Broker를 선택하기 위해몇 가지 Message Broker에 대해 알아보고 결정하자RabbitMQ 2007년에 출시되었으며 가장 먼저 만들어진 메시지 브로커 중 하나이다. Erlang으로 개발되었으며, Erlang은 기본적으로 Erlang 클러스터 노드...", "content": " ‘나만의 웨딩 플래너’라는 MSA 기반의 토이 프로젝트를 진행하는데 필요한 Message Broker를 선택하기 위해몇 가지 Message Broker에 대해 알아보고 결정하자RabbitMQ 2007년에 출시되었으며 가장 먼저 만들어진 메시지 브로커 중 하나이다. Erlang으로 개발되었으며, Erlang은 기본적으로 Erlang 클러스터 노드의 쿠키를 동기화하여 분산 컴퓨팅을 지원한다. 따라서, Zookeeper와 같은 third-party 클러스터 관리자를 사용하지 않는다. 클러스터에는 일반 클러스터 모드와 미러 클러스터 모드의 두 가지 모드가 있다. priority queuing, delay queuing과 같은 다양한 기능 제공 영속성 : 영구 및 임시 메시지가 모두 지원 AMQP(Advanced Message Queuing Protocol)를 구현한다. 따라서, STOMP, MQTT, WebSockets과 같은 다양한 프로토콜을 지원한다. 점대점, 요청/응답, pub/sub 모델 모두 지원 (동기, 비동기 통신 모두 지원) RabbitMQ 사용자는 메시지 전달을 위한 정교한 규칙을 설정할 수 있다(보안, 조건부 라우팅 등) 강력한 인증 및 표현식 기반의 인가를 갖는다. 관리자가 스트림 기록에 액세스하려는 경우, 아파치 카산드라와 종종 함께 사용된다. 디스크에 메시지를 저장할 수 있지만 메시지의 순서가 보장되지는 않는다. 큐 모니터링을 위한 GUI와 최적의 REST API를 제공한다. RabbitMQ는 큐 미러링을 통해 고가용성을 실현한다. RabbitMQ의 큐 개념은 Kafka의 파티션과 유사하다. complex routing을 지원하는 많은 기능을 갖추고 있다. master-slave 구조를 갖는다 RabbitMQ의 아키텍처는 완벽한 복제 설계로 인해 확장성이 떨어진다. 출처 : https://www.researchgate.net/publication/347866161_A_Fair_Comparison_of_Message_Queuing_SystemsActiveMQ JMS(Java Message Service) 클라이언트와 함께 Java로 작성되었다. 이를 통해 OpenWire, STOMP, REST, XMPP, AMQP와 같은 다양한 통신 프로토콜을 지원한다. master-slave 구조를 갖는다 master 브로커만 서비스를 제공할 수 있다. slave 브로커는 master 브로커와 동기화하며, 오류로 인해 master가 불능이 되면, Zookeeper가 slave 중 새로운 master를 선택한다. pub/sub, 점대점 모델을 모두 지원한다. 큐와 토픽을 다르게 관리한다. 토픽에 대한 메세지 순서 보장은 없다. 반면, 큐는 Exclusive Queue를 사용함으로써 메세지 순서가 보장된다. 큐에 대해서는 최소 한번의 전달이 보장되며, 토픽의 경우에 전달이 보장되지 않는다. 큐에 있는 메세지는 디스크 파일이나 데이터베이스에 저장되지만, 토픽에 있는 메세지는 기본적으로 보존되지 않는다. JMS는 메시징 미들웨어의 샤딩 메커니즘을 명시하지 않기 때문에 샤딩 기능이 없으며, 사용자가 필요에 따라 직접 구현해야 한다. 출처 : https://www.researchgate.net/publication/347866161_A_Fair_Comparison_of_Message_Queuing_SystemsApache Kafka 2011년 Linkedin에서 높은 처리량과 짧은 지연 시간 처리를 다루기 위해 만들었다. peer-to-peer 구조를 갖는다. 대량의 데이터를 장기간 저장하도록 구축된 높은 처리량 분산 큐 시스템이다. 분산 스트리밍 플랫폼인 Kafka는 pub/sub 서비스를 복제한다. 영속성이 요구되는 일대다 상황에 이상적이다. pub/sub 모델을 사용하지만 publisher 대신 producer를 subscriber 대신 consumer라는 용어를 사용한다. 토픽을 파티션으로 분할할 수 있다. 이를 통해, 여러 컨슈머가 스트림의 일부를 처리할 수 있고 수평 스케일링을 가능하게 해준다. 카프카는 메시지가 배치될 때 가장 잘 작동한다. 즉, 작은 메시지를 많이 수신하는 대신 대규모 데이터 배치로 적게 수신하므로써 성능이 향상된다. 가장 작은 배치의 크기는 100바이트, 가급적이면 1-10KB가 좋다. 배치 크기가 클수록 구현에 따라 지연 시간이 약간 증가한다(일반적으로 몇 밀리초) 하나의 파티션 내의 메세지는 순서가 보장된다. 기본적으로 최소 한번의 전달을 보증하며, producer가 비동기식으로 제출하도록 설정하면 최대 한 번까지 전달을 보증한다. 모니터링을 위해 GUI가 사전 제공되지 않기 때문에 일반적으로 Kibana를 함께 사용한다. 출처 : https://www.researchgate.net/publication/347866161_A_Fair_Comparison_of_Message_Queuing_SystemsRedis 고성능 key-value 저장소 또는 메시지 브로커로 사용할 수 있는 in-memory 저장소이다. 영속성이 없지만 Disk/DB에 메모리를 덤프한다. 실시간 데이터 처리에도 이상적이다. 원래, 레디스는 일대일도 아니었고 일대다도 아니었다. 하지만 Redis5.0 pub/sub을 도입하면서 기능이 향상됐고 일대다 옵션이 됐다. 메모리에만 저장하기 때문에 카프카보다도 빠르다.비교해보기전반적인 특징 출처 : https://www.researchgate.net/publication/347866161_A_Fair_Comparison_of_Message_Queuing_Systems성능처리량(Throughput) 출처 : https://www.researchgate.net/publication/347866161_A_Fair_Comparison_of_Message_Queuing_Systems 모두 배치 처리 기술을 사용했다. 즉,메시지가 임계값에 도달하기 위해 누적된다. 균일하게 전송되므로 전송 오버헤드가 감소하고 처리량이 증가한다. 따라서 메시지 크기가 커질수록 배치 처리를 기다리는 시간이 줄어들고 그에 따라 처리량도 증가한다.지연(Latency) 출처 : https://www.researchgate.net/publication/347866161_A_Fair_Comparison_of_Message_Queuing_Systems 카프카와 Rabbit MQ는 다른 시스템보다 긴 지연 시간을 보이는데, 이는 배치 처리를 채택하여 지연 시간을 높이는 대신 처리량을 개선했기 때문이다.나의 선택은 ? 솔직히, 아직 하나도 제대로 써본 경험이 없어서 위에서 조사한 특징들이 어떤 의미를 갖는 것인지 잘 모르겠다 .. Kafka가 처리량, 데이터 영속성 등에서 우수한 것 같으니 Kafka를 먼저 공부하고 사용해보면서 직접 느껴보자더 공부해야할 부분 메세지 브로커에 대한 전반적인 이해 Kafka참고 자료 https://otonomo.io/redis-kafka-or-rabbitmq-which-microservices-message-broker-to-choose/ https://dattell.com/data-architecture-blog/kafka-vs-rabbitmq-how-to-choose-an-open-source-message-broker/ https://www.confluent.io/blog/kafka-fastest-messaging-system/ https://www.researchgate.net/publication/347866161_A_Fair_Comparison_of_Message_Queuing_Systems" }, { "title": "WEB - HTTP의 stateless한 특징과 쿠키, 세션", "url": "/posts/stateless-cookie-session/", "categories": "지식 더하기, 이론", "tags": "WEB", "date": "2021-06-27 00:25:00 +0900", "snippet": " 쿠키와 세션을 비교하며 특징을 외우기는 했지만, 정작 나오게된 배경에 대해서는 크게 생각해보지 않았던 것 같다. 이번 포스팅을 통해 쿠키, 세션이 나오게 된 배경인 HTTP의 무상태성과 쿠키, 세션의 특징에 대해 정리해보자.HTTP의 특징 - Stateless HTTP의 대표적인 특징 중 하나는 상태를 갖지 않는다(Stateless)는 것이다. ...", "content": " 쿠키와 세션을 비교하며 특징을 외우기는 했지만, 정작 나오게된 배경에 대해서는 크게 생각해보지 않았던 것 같다. 이번 포스팅을 통해 쿠키, 세션이 나오게 된 배경인 HTTP의 무상태성과 쿠키, 세션의 특징에 대해 정리해보자.HTTP의 특징 - Stateless HTTP의 대표적인 특징 중 하나는 상태를 갖지 않는다(Stateless)는 것이다. 그렇다면 ‘상태’란 무엇일까 ? 몇 가지 자료를 찾아본 결과, ‘A는 B이다’ 처럼 ‘상태는 OO다’ 라고 딱 떨어지게 말하는 것은 어려운 것 같고, 『The Definitive Guide to Linux Network Programming』 4장 에서는 상태를 다음과 같이 표현하고 있다. If a server maintains state, it means that the server maintains information about all of the current connections with its clients and the communications sent between them. 즉, 상태란 ‘서버와 클라이언트 사이에 맺은 커넥션을 통해 서로간에 주고받는 모든 정보’ 라고 이해하면 될 것 같다. ‘주고받는 정보’의 가장 대표적인 예로 사용자의 로그인 여부를 생각해 볼 수 있다. 이러한 하나의 데이터뿐 아니라, 어떤 요청이 왔었는지 어떤 응답을 했었는지 또한 주고받는 정보가 될 수 있다. 따라서 stateless 하다는 것은 이러한 정보를 서버측에서 유지(저장)하지 않는다는 것을 의미한다. 상태를 갖지 않음으로써(정보를 저장하지 않음으로써) 얻는 장/단점 장점 서버 확장성이 좋다. 즉, 서버에서 클라이언트 요청에 대한 정보를 저장하고 있지 않기 때문에, 여러 대의 서버 중 어떤 서버로 요청을 해도 동일한 응답을 해줄 수 있다. 따라서 시스템 설계시, 상태 유지는 최소한으로만 사용하면서 설계하는 것이 좋다. 서버의 리소스(CPU, 메모리 등)를 아낄 수 있다. 단점 반복되는 데이터 전송으로 인해, 네트워크 성능을 저하시킬 수 있다. 쿠키 쿠키란 웹 브라우저가 (로컬 PC에)보관하는 데이터(key-value pair)이다. 웹 서버와 웹 브라우저 양쪽에서 생성할 수 있다. 동작 방식은 생성 → 저장 → 전송 단계로 구성된다. 쿠키가 등장하게 된 배경 추측해보기 처음에는 웹이 정보 공유의 목적으로 시작되었다고 들었다. 그렇다보니 html 문서 정도만 보여주면 됐을 것이므로 상태유지에 대한 필요성을 느낄 수 없었을 것 같다. 하지만 웹을 통해 서비스를 제공하는 등, 기술과 요구사항이 발전함에 따라 상태를 저장하지 않고서는 극복할 수 없는 한계에 자연스럽게 도달한 것 같다. 장점 사용자 컴퓨터에 저장되기 때문에 서버 리소스에 부하를 주지 않는다. 설정에 따라, 세션보다 훨씬 오래 지속될 수도 있다. 단점 쿠키는 일반 텍스트로 저장되고, 누구나 쿠키를 열거나 변조할 수 있기 때문에 보안 위험이 발생할 수 있다. 쿠키를 수동으로 암호화 및 암호 해독할 수 있지만 암호화 및 암호 해독에 필요한 시간 때문에 추가 코딩이 필요하고 애플리케이션 성능에 영향을 줄 수 있다. 쿠키 사이즈, 개수에 제한이 있다. 사용자가 브라우저 설정에서 쿠키를 비활성화할 수 있다. 따라서, 그러한 가능성을 예측해서 코딩해야 한다. 복잡한 데이터 구조는 저장할 수 없고 일반 텍스트(문자열)만 허용한다. 참고 : 2018년 기준 브라우저별 최대 쿠키 사이즈, 개수 Web Browser Maximum cookies Maximum size per cookie 크롬 180 4096 bytes 파이어폭스 150 4097 bytes 오페라 180 4096 bytes 세션 세션이란 웹 컨테이너에 저장하는 상태 정보이다. 웹 컨테이너는 기본적으로 한 웹 브라우저마다 한 세션을 생성한다. 세션은 사용자가 애플리케이션에서 로그아웃하거나 웹 브라우저를 닫으면 종료된다. 세션은 주로 쿠키와 함께 사용된다. 서버가 클라이언트 ID(톰캣의 경우 JSESSIONID)를 어떤 방법으로 추적할 것인지 정의한 것을 ‘세션 트래킹 모드’라고 한다. 트래킹 모드에는 쿠키 사용 모드, URL Rewriting 모드, SSL 모드 가 있는데 대부분 서버에서 쿠키 사용 모드를 기본값으로 하고 있고 이 모드 사용을 권장하고 있다. 그래서 특별한 설정이 없으면 클라이언트를 구분하기 위해 쿠키를 활용한다. 장점 서버측 메모리의 한계까지 저장할 수 있다. 서버에 저장되므로 쿠키보다 보안 측면에서 안전하다. 데이터를 Hash Table에 저장 : 한번에 많은 정보를 하나의 세션 객체에 저장가능 단점 서버에 데이터를 저장하므로 사용자가 많아질수록 서버에 부하가 커진다. 더 공부해야할 부분 웹, HTTP 전반에 대한 이해참고 자료 우에노 센, 『그림으로 배우는 Http &amp; Network Basic』, 영진닷컴(2015) 최범균, 『JSP2.3 웹프로그래밍 기초부터 중급까지』, 가메출판사(2015) https://link.springer.com/chapter/10.1007%2F978-1-4302-0748-1_4 인프런 김영한님 강의 https://www.webcodeexpert.com/2013/03/what-is-cookie-advantages-and.html https://thecodinglog.github.io/web/2020/08/11/what-is-session.html" }, { "title": "API Gateway 비교해보기", "url": "/posts/api-gateway-comparison/", "categories": "", "tags": "", "date": "2021-06-27 00:25:00 +0900", "snippet": " ‘나만의 웨딩 플래너’라는 MSA 기반의 토이 프로젝트를 진행하는데 필요한 API Gateway를 만들기 위해 어떤 프레임워크를 선택해야할지몇 가지 API Gateway 프레임워크에 대해 알아보고 결정하자API Gateway 역할대표적인 역할 몇 가지를 살펴보자 요청 라우팅 요청을 HTTP 메서드, 경로에 따라 서비스로 라우팅 ...", "content": " ‘나만의 웨딩 플래너’라는 MSA 기반의 토이 프로젝트를 진행하는데 필요한 API Gateway를 만들기 위해 어떤 프레임워크를 선택해야할지몇 가지 API Gateway 프레임워크에 대해 알아보고 결정하자API Gateway 역할대표적인 역할 몇 가지를 살펴보자 요청 라우팅 요청을 HTTP 메서드, 경로에 따라 서비스로 라우팅 API 조합 여러 서비스를 호출한 결과를 조합 엣지 기능 인증, 인가, 캐싱, 로깅 등 프로토콜 변환 클라이언트 친화적인 프로토콜과 비친화적인 프로토콜간에 상호 변환 Spring Cloud Netflix Zuul Spring Cloud 프로젝트에서 Netflix Zuul을 병합시켜 만든 프레임워크 동기/블로킹 특성을 갖는다. Tomcat 서버를 사용한다 공식적으로 2018년 12월부터 더 이상의 개발은 없고 유지보수만 지원한다. Spring boot 2.4.X부터는 zuul이 더 이상 지원되지 않는다. 경로 기반의 라우팅만 지원된다. 쿼리 아키텍처가 지원되지 않는다. 출처 : https://netflixtechblog.com/announcing-zuul-edge-service-in-the-cloud-ab3af5be08eeSpring Cloud Gateway 스프링5, 스프링 부트2, 스프링 웹플럭스 등의 프레임워크를 토대로한 프레임워크 Gateway Handler를 사용하여 수신 요청을 적절한 대상으로 간단하고 효과적으로 라우팅 Netty 서버를 사용하여 반응형/비동기 특성을 지원 출처 : https://cloud.spring.io/spring-cloud-gateway/reference/html/Netflix Zuul2 Netty 서버를 사용하여 반응형/비동기 특성을 지원 HTTP2, WebSocket 지원 Zuul2에는 Spring Cloud Neflix Zuul(Zuul1.x)과 같은 내장 지원이 없다. 따라서, 사용하고자 할 경우 Zuul만 따로 서비스를 운영해야 한다.즉, 기존의 다른 스프링 마이크로 서비스와 통합할 수 없다. 출처 : https://netflixtechblog.com/open-sourcing-zuul-2-82ea476cb2b3나의 선택은 ? 일단은 토이 프로젝트이고 API Gateway를 처음 적용해보는 것이므로 러닝 커브가 비교적 낮을 것 같은 Spring Cloud Zuul을 사용하기로 결정 공식 문서 에도 나와있지만 Spring Cloud Gateway를 사용하려면Spring Boot 2.0, Spring WebFlux, and Project Reactor 등에 익숙하지 않으면 이것들에 먼저 익숙해져야 한다고 나와있다. 다음 프로젝트에서는 반응형 프로그래밍, WebFlux, Netty 등에 대해 학습하고 Spring Cloud Gateway를 사용해보자 적용 프로젝트 repo더 공부해야할 부분 반응형 프로그래밍 WebFlux Netty Spring Cloud Gateway참고 자료 크리스 리처드슨, 『마이크로서비스 패턴』, 길벗(2020), 8장 https://www.programmersought.com/article/28121145498/ https://cloud.spring.io/spring-cloud-gateway/multi/multi_gateway-starter.html https://www.novatec-gmbh.de/en/blog/api-gateways-an-evaluation-of-zuul-2/ https://medium.com/@niral22/spring-cloud-gateway-tutorial-5311ddd59816" }, { "title": "간단한 웹 서버 구현하기", "url": "/posts/nextstep-toy-webserver/", "categories": "경험하기, 작업 노트", "tags": "HTTP", "date": "2021-06-21 22:25:00 +0900", "snippet": "들어가기 전『자바 웹 프로그래밍 Next Step』 4장에 나와있는 웹 서버 요구사항들을 구현해보았다.구현하기 전, 스스로 아래와 같은 사항을 다짐하고 진행해봤다. 최대한 책에 나온 힌트를 보지 않고 해결해보자. 반드시 테스트 코드를 작성하자. 빠르게 구현하는 것 보다 중요한건 코드 한 줄 한 줄의 의미를 제대로 아는 것. 의식적으로 리팩토링 ...", "content": "들어가기 전『자바 웹 프로그래밍 Next Step』 4장에 나와있는 웹 서버 요구사항들을 구현해보았다.구현하기 전, 스스로 아래와 같은 사항을 다짐하고 진행해봤다. 최대한 책에 나온 힌트를 보지 않고 해결해보자. 반드시 테스트 코드를 작성하자. 빠르게 구현하는 것 보다 중요한건 코드 한 줄 한 줄의 의미를 제대로 아는 것. 의식적으로 리팩토링 연습을 하자.처음엔 html 페이지만 클라이언트로 전달하는 요구사항이었는데 나중엔 Map을 임시 DB로 활용하여 회원가입하는 것까지 하게되었다.다 만들어보고 나니 엄밀히 말하면 매우 간단한 ‘WAS’라고 할 수 있을 것 같다.연속으로 시간을 할애하진 못했지만, 다 합치면 대략 2주 정도 걸린 것 같다.※ 전체 코드 repository요구사항 별 구현요구사항 별로 구현을 하면서 몰랐던 내용들과 배운 내용, 구현 사항 등을 기록해보았다.요구사항 1 http://localhost:8080/index.html로 접속했을 때 webapp 디렉토리의 index.html 파일을 읽어 클라이언트에 응답한다.구현하기 전 들었던 생각들 index.html 파일을 어떤식으로 클라이언트에 넘겨줘야할까 ? index.html 파일 읽어서 byte 형태로 만들어준 뒤 DataOutputStream을 통해 response 넘기면 되지 않을까.. ? http://localhost:8080/index.html 라는 요청왔을 때 ‘/’ 이후의 파일명을 어떻게 가져오지 ?구현 베이스 코드에 있던 ‘RequestHandler’ 클래스 내부에 구현했다. // 클라이언트가 요청한 html 파일 이름 가져온다 private String getRequestHtmlName(InputStream in) throws IOException { BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(in)); String requestInfo = bufferedReader.readLine(); if(!requestInfo.contains(\".html\")) { return null; } String htmlPage = requestInfo.split(\" \")[1]; return htmlPage.substring(1); // 맨 앞에 '/' 지우기 위함 } // http response를 위해 파일을 byte로 변환 (절대경로 ver) private byte[] convertHtmlToByte(String fileName) throws IOException { String rootPath = System.getProperty(\"user.dir\"); String filePath = rootPath + \"/webapp/\" + fileName; File file = new File(filePath); return Files.readAllBytes(file.toPath()); } // http response를 위해 파일을 byte로 변환 (상대경로 ver) private byte[] convertHtmlToByte(String fileName) throws IOException { String filePath = \"./webapp/\" + fileName; File file = new File(filePath); return Files.readAllBytes(file.toPath()); }구현하면서 알게된 것들 브라우저에서 넘어오는 request가 어떤식으로 구성되어있는지 connection.getInputStream()을 디버거로 찍었는데 요청에 대한 문자열 정보가 보이지 않는다. (connection은 Socket 객체) InputStream은 바이트 단위로 숫자를 저장하기 때문에 원하는 값 볼 수 없었던 것.(참고) BufferdReader, InputStreamReader를 활용하여 byte array인 InputStream을 읽어보니 요청의 맨 첫번째 줄에 html 파일에 관련된 정보가 있다. 상대경로를 사용할 경우, ./ (현재 위치)는 bin, src 폴더를 포함하는 해당 자바 프로젝트 폴더의 위치이다.요구사항 2 GET 방식으로 회원가입하기구현하기 전 들었던 생각들 요구사항1 에서 구현한 메서드를 개선해야 할 것 같다. js, css 파일에 대한 요청도 response로 넘겨줘야 하기 때문에 getRequestHtmlName(), convertHtmlToByte() 라는 메서드명을 좀 더 추상적으로 변경하여 html 파일 이외에도 처리가 가능하도록 만들어 보자. client 관련 파일 디렉토리 최상위 폴더인 “webapp”도 변수에 추가하지 말고 클래스 변수로 선언하고 final로 상수화 하는게 나을 것 같다. 본격적으로 회원가입에 요청에 대해 생각해보면, 지금까지 했던 페이지 응답과는 달리 회원가입 이라는 ‘비즈니스 로직’을 수행해야 한다. 따라서, 비즈니스 로직 수행을 담당하는 ‘무언가’를 만들어야 할 것 같다. 또한, RequestHandler에서 현재는 페이지 응답에 관련된 것만 수행하는데 다양한 request를 적절하게 처리할 수 있도록 변경해야 할 것 같다. 정적 자원 요청, 비즈니스 로직 요청 등 각기 다른 요청을 어떻게 구분해야 할까 ? 비즈니스 로직을 요구하는 요청을 처리할 실제 로직에 해당 요청을 어떻게 맵핑시키면 좋을까 ? 사용자 요청과 해당 요청을 처리하는데 필요한 메서드를 맵핑시켜주는 역할을 하는 LogicMapper 생성. RequestHandler 내에 코드를 추가해서 할 수도 있겠지만, 그렇게 되면 책임이 많아지기 때문에 코드가 점점 복잡해질 것이다. 따라서, RequestHandler의 책임을 ‘클라이언트의 요청을 받아들이고 어떤 요청인지 판단 하는 것’으로 한정하는게 좋을 것 같다고 생각했다. 구현 요청의 타입을 정적 자원(html, css, js 등) 요청과 비즈니스 로직 요청 두 개로 나눠보았다.일단 아래처럼 구현해봤는데 이렇게 하드코딩으로 분기시키면 변화에 유연하게 대응하기도 어렵고 생각지 못한 케이스도 있을 것 같다.어떻게 하는게 좋을지 고민해보고 추후에 책도 참고해보자public enum RequestType { REQUEST_STATIC_RESOURCE, REQUEST_BUSINESS_LOGIC} private RequestType getRequestType(HttpRequest httpRequest) { String httpMethod = httpRequest.getHttpMethod(); String requestUrl = httpRequest.getRequestUrl(); switch (HttpMethod.valueOf(httpMethod)) { case GET: if(requestUrl.contains(\"?\")) { return RequestType.REQUEST_BUSINESS_LOGIC; } else if(requestUrl.equals(\"/\") || requestUrl.contains(\".\")) { return RequestType.REQUEST_FILE; } else { return RequestType.REQUEST_BUSINESS_LOGIC; } case POST: break; } return null; } 비즈니스 로직 맵핑을 위해 일단 아래와 같이 구현했는데, 리플렉션을 마구 썼다. 이 방법 말고는 없을까 ?사실, executeMethodWithParamsForGetRequest에서 파라미터 맵핑하는 부분도 파라미터 객체에 기본 생성자가 없으면paramClass.getDeclaredConstructor().newInstance() 이 부분에서 에러 난다.그리고 LogicMapper가 최초 한 번만 생성되는 것도 보장되어야 하는 등 아직 미흡한 부분이 많다. public class LogicMapper { // 실행할 비즈니스 로직에 대한 정보 static class Execution &lt;T&gt; { private T targetInstance; // 실행될 비즈니스 로직 객체 private Class logicClass; // 실행될 비즈니스 로직 클래스 private String methodName; // 실행될 메서드 명 private Class paramClass; // 파라미터 클래스 public Execution(T targetInstance, Class logicClass, String methodName) { this.targetInstance = targetInstance; this.logicClass = logicClass; this.methodName = methodName; } public Execution(T targetInstance, Class logicClass, String methodName, Class paramClass) { this.targetInstance = targetInstance; this.logicClass = logicClass; this.methodName = methodName; this.paramClass = paramClass; } public T getTargetInstance() { return targetInstance; } public Class getLogicClass() { return logicClass; } public String getMethodName() { return methodName; } public Class getParamClass() { return paramClass; } } private static Map&lt;String, Execution&gt; getMappingUrl = new HashMap&lt;&gt;(); static { initGetRequest(); } // 요청에 따른 실행 맵핑 private static void initGetRequest() { getMappingUrl.put(\"/user/create\", new Execution(UserLogic.getInstance(), UserLogic.class, \"signup\", User.class)); } // 비즈니스 로직 실행되도록 public byte[] doRequestLogic(HttpRequest httpRequest) throws Exception { HttpMethod httpMethod = HttpMethod.valueOf(httpRequest.getHttpMethod()); String requestUrl = httpRequest.getRequestUrl(); byte[] response = {}; switch (httpMethod) { case GET: response = requestUrl.contains(\"?\") ? executeMethodWithParamsForGetRequest(requestUrl) : executeMethodWithoutParamsForGetRequest(requestUrl); break; } return response; } // 요청에 파라미터 있는 경우 public byte[] executeMethodWithParamsForGetRequest(String requestUrl) throws Exception { String[] info = requestUrl.split(\"\\\\?\"); String url = info[0]; Map&lt;String, String&gt; params = HttpRequestUtils.parseQueryString(info[1]); Execution execution = Optional.ofNullable(getMappingUrl.get(url)).orElseThrow(NoSuchMethodError::new); Class paramClass = execution.getParamClass(); Object instance = paramClass.getDeclaredConstructor().newInstance(); for(String key : params.keySet()) { Optional.ofNullable(paramClass.getDeclaredField(key)).ifPresent((field) -&gt; { field.setAccessible(true); try { field.set(instance, params.get(key)); } catch (IllegalAccessException e) { e.printStackTrace(); } }); } execution.getLogicClass().getMethod(execution.getMethodName(), execution.getParamClass()).invoke(execution.getTargetInstance(), instance); return \"SUCCESS\".getBytes(); } // 요청에 파라미터 없는 경우 public byte[] executeMethodWithoutParamsForGetRequest(String requestUrl) throws NoSuchMethodException, InvocationTargetException, IllegalAccessException { Execution execution = Optional.ofNullable(getMappingUrl.get(requestUrl)).orElseThrow(NoSuchMethodError::new); execution.getLogicClass().getMethod(execution.getMethodName(), execution.getParamClass()).invoke(execution.getTargetInstance()); return \"SUCCESS\".getBytes(); } }구현하면서 알게된 것들 리플렉션 사용해서 대상클래스.getDeclaredConstructor().newInstance() 실행시 대상 클래스에 기본 생성자 없으면 에러 난다.요구사항 3 POST 방식으로 회원가입하기구현하기 전 들었던 생각들 POST 방식으로 넘어오는 (body에 담겨있는)파라미터는 어떻게 받지 ?구현if(httpMethod.equals(\"POST\")) { int contentLen = 0; for(String line = bufferedReader.readLine(); (!line.isEmpty() &amp;&amp; line!=null); line=bufferedReader.readLine()) { if(line.contains(\"Content-Length\")) { String[] info = line.split(\":\"); contentLen = Integer.parseInt(info[1].trim()); break; } } if (contentLen &gt; 0) { char[] body = new char[contentLen]; bufferedReader.read(body); String params = new String(body); }}구현하면서 알게된 것들 POST 방식의 경우 bufferedReader의 readLine() 사용하게 되면 http header는 읽지만 body는 읽어오지 못한다. 원인을 찾아보니 header의 마지막 부분(“”) readLine() 하는 부분에서 계속 hanging 되어있다. hanging 되는 이유는 request header 마지막 라인이 공백인데 readLine()의 경우 line의 끝에 개행 문자가 없는 경우, 값이 올 때 까지 계속 기다린다고 한다. 따라서, 공백인 경우를 체크하는 로직이 필요하고, Content-Length를 구해서 그 크기만큼 나머지를 read() 하면 된다. readLine() 대신 read()를 사용하는 이유는, body로 넘어오는 파라미터 또한 끝에 개행문자가 없기 때문에 readLine() 사용시 대기상태에 빠진다. http request 포맷 Request-Line*(( general-header | request-header | entity-header ) CRLF)CRLF[ message-body ] 요구사항 4 회원가입을 완료하면 /index.html 페이지로 이동한다. url 또한 user/create가 아닌 /index.html로 변해야 한다.구현하기 전 들었던 생각들 회원가입 후 index.html을 클라이언트로 넘겨주면 페이지는 뜨겠지만 url은 user/create가 그대로 남아있을 것이다. 회원가입 후 클라이언트가 index.html을 요청하도록 하려면 어떻게 해야할까 ? 비즈니스 로직 수행하는 메서드의 다양한 타입의 리턴값을 LogicMapper에 추상화 시킨 두 개의 메서드 executeMethodWithParams(), executeMethodWithoutParams()에서 어떻게 받아서 처리해야할까?구현 먼저 웹 서버의 관점에서 생각해봤을때, 웹 서버에서 클라이언트로 응답하는 형태는 크게 ‘html 페이지 / 데이터(json 등)’ 이라고 생각했다.따라서, 클라이언트의 요청에 대해 적절한 응답을 하기 위해 필요한 것은 위에서 정의한 ‘응답의 형태’와 ‘실제 데이터’ 두 가지라고 생각하여 이를 나타내는 ExecutionResult 클래스를 만들었다.이에 따라 기존에 정의된 executeMethodWithParams(), executeMethodWithoutParams() 메서드의 리턴값을 ExecutionResult로 바꾸고 다양한 타입의 리턴값을 해당 클래스의 속성 중‘실제 데이터’(Object) 부분에 세팅했다.public enum ResponseType { HTML_PAGE, DATA, EMPTY} public ExecutionResult doRequestLogic(HttpRequest httpRequest, HttpResponse httpResponse) throws Exception { HttpMethod httpMethod = httpRequest.getHttpMethod(); String requestUrl = httpRequest.getRequestUrl(); Map&lt;String,String&gt; params = httpRequest.getParams(); Execution execution = null; switch (httpMethod) { case GET: execution = Optional.ofNullable(getMappingUrl.get(requestUrl)).orElseThrow(NoSuchMethodError::new); break; case POST: execution = Optional.ofNullable(postMappingUrl.get(requestUrl)).orElseThrow(NoSuchMethodError::new); break; } switch (execution.getResponseType()) { case HTML_PAGE: httpResponse.setStatusCode(HttpStatusCode3xx.REDIRECTION); break; case DATA: httpResponse.setStatusCode(HttpStatusCode2xx.OK); } ExecutionResult result = (params!=null) ? executeMethodWithParams(execution, params, httpResponse) : executeMethodWithoutParams(execution, httpRequest); return result; }public class ExecutionResult { private ResponseType responseType; private Object returnData; public ExecutionResult(ResponseType responseType, Object returnData) { this.responseType = responseType; this.returnData = returnData; } public ResponseType getResponseType() { return responseType; } public Object getReturnData() { return returnData; }} LogicMapper에서 실행한 결과를 RequestHandler에서 받은 후 응답 타입에 따라 HTTP Response에 적절한 데이터 세팅해준다.페이지 리다이렉트인 경우 httpResponse.setHeader(\"Location\", redirectUrl);을 통해 헤더에 Location : redirectUrl을 세팅해준다. case REQUEST_BUSINESS_LOGIC: ExecutionResult result = requestLogicMapper.doRequestLogic(httpRequest, httpResponse); switch (result.getResponseType()) { case HTML_PAGE: String redirectPage = (String) result.getReturnData(); String redirectUrl = \"http://localhost:8080\"+redirectPage; // TODO : 하드코딩 말고 request origin으로 ? httpResponse.setHeader(\"Location\", redirectUrl); break; case DATA: break; case EMPTY: httpResponse.setStatusCode(HttpStatusCode2xx.OK); httpResponse.setHeader(\"Content-Type\", \"text/html;charset=utf-8\"); httpResponse.setHeader(\"Content-Length\", responseBody.length); break;}구현하면서 알게된 것들 HTTP Response 응답 헤더에 Location : url을 세팅하면 브라우저는 해당 url로 요청을 보낸다. 상태코드 302 Found 클라이언트가 요청한 리소스가 Location 헤더에 주어진 URL에 일시적으로 이동되었음을 가리킨다. 요구사항 5-1 로그인 메뉴를 클릭하면 해당 페이지로 이동한다. 로그인이 성공하면 /index.html로 이동하고, 실패하면 /user/login_failed.html로 이동해야 한다. 앞에서 회원가입한 사용자로 로그인할 수 있어야 한다. 구현하기 전 들었던 생각들 비즈니스 로직 수행하는 메서드들의 파라미터는 개수도 다양하고 타입도 다양한데 LogicMapper에 추상화 시킨 두 개의 메서드 executeMethodWithParams(), executeMethodWithoutParams()에서 어떻게 받아서 처리해야할까? RequestHandler에서 LogicMapper로 넘어오는 파라미터의 형태는 Map인데, LogicMapper에서 맵핑시켜줄 다양한 비즈니스 로직 메서드의 파라미터를 일일이 맞추려면 추상화된 두 개의 메서드만 갖고는 불가능하다고 생각했다. 왜 이렇게 됐을까를 생각해보니 LogicMapper 입장에서 실제로 실행되는 비즈니스 로직 클래스, 파라미터 타입, 개수 등 알아야할게 너무 많은 것 같았다. 요구사항 2에 보면 LogicMapper의 미흡함에 대해 적어놨는데, 그 때 더 깊게 생각하지 않았던게 결국 발목을 잡은 것 같다. 구현 LogicMapper 개선 (RequestLogicMapper) LogicMapper에서 실제 비즈니스 로직을 바로 실행하는게 아니라, 중간에 LogicExecutor라는 객체를 둔다. LogicMapper에서는 들어온 요청에 따라 실제 로직 메서드가 아닌 LogicExecutor의 특정 메서드를 실행하고 파라미터도 Map 형태로만 넘기도록 수정. 하지만, 로직에서 추가적으로 필요한 파라미터로 HttpRequest, HttpResponse 등도 있을 수 있으므로 일단은 아래 코드처럼 Map, HttpRequest, HttpResponse했는데 해당 파라미터가 필요하지 않더라 받아야하므로 비효율적이다. 실제 Spring MVC에서는 Controller 단에서 HttpServletRequest, HttpServletRespone 등을 자유롭게 받을 수 있던데, 어떻게 가능한건지 공부해보자. public class RequestLogicMapper { static class Execution { private String methodName; private ResponseType responseType; public Execution(String methodName, ResponseType responseType) { this.methodName = methodName; this.responseType = responseType; } public String getMethodName() { return methodName; } public ResponseType getResponseType() { return responseType; } } private LogicExecutor logicExecutor = LogicExecutor.getInstance(); private static Map&lt;String, Execution&gt; getMappingUrl = new HashMap&lt;&gt;(); private static Map&lt;String, Execution&gt; postMappingUrl = new HashMap&lt;&gt;(); static { initGetRequest(); initPostRequest(); } private static void initGetRequest() { getMappingUrl.put(\"/user/create\", new Execution(\"signup\", ResponseType.HTML_PAGE)); getMappingUrl.put(\"/user/list\", new Execution(\"getUserList\", ResponseType.HTML_PAGE)); } private static void initPostRequest() { postMappingUrl.put(\"/user/create\", new Execution(\"signup\", ResponseType.HTML_PAGE)); postMappingUrl.put(\"/user/login\", new Execution(\"login\", ResponseType.HTML_PAGE)); } public ExecutionResult doRequestLogic(HttpRequest httpRequest, HttpResponse httpResponse) throws Exception { HttpMethod httpMethod = httpRequest.getHttpMethod(); String requestUrl = httpRequest.getRequestUrl(); Execution execution = null; switch (httpMethod) { case GET: execution = Optional.ofNullable(getMappingUrl.get(requestUrl)).orElseThrow(NoSuchMethodError::new); break; case POST: execution = Optional.ofNullable(postMappingUrl.get(requestUrl)).orElseThrow(NoSuchMethodError::new); break; } switch (execution.getResponseType()) { case HTML_PAGE: httpResponse.setStatusCode(HttpStatusCode3xx.Found); break; case DATA: httpResponse.setStatusCode(HttpStatusCode2xx.OK); } Map&lt;String, String&gt; params = httpRequest.getParams(); ExecutionResult result = (params != null) ? executeMethodWithParams(execution, params, httpRequest, httpResponse) : executeMethodWithoutParams(execution, httpRequest, httpResponse); return result; } public ExecutionResult executeMethodWithParams(Execution execution, Map&lt;String, String&gt; params, HttpRequest request, HttpResponse response) throws Exception { Method logic = logicExecutor.getClass().getMethod(execution.getMethodName(), Map.class, HttpRequest.class, HttpResponse.class); Object returnObj = logic.invoke(logicExecutor, params, request, response); return new ExecutionResult(execution.getResponseType(), returnObj); } public ExecutionResult executeMethodWithoutParams(Execution execution, HttpRequest request, HttpResponse response) throws Exception { Method logic = logicExecutor.getClass().getMethod(execution.getMethodName(), HttpRequest.class, HttpResponse.class); Object returnObj = logic.invoke(logicExecutor, request, response);; return new ExecutionResult(execution.getResponseType(), returnObj); }} LogicExecutor 구현 LogicExcecutor에서는 Map형태로 받은 파라미터를 가공하여 실제 로직 메서드에 필요한 파라미터 형태로 만들고 해당 메서드를 호출한다. 이렇게 하고 보니 LogicExecutor가 MVC 모델에서 Controller와 약간 비슷한 역할이지 않나 하는 생각이 든다. public class LogicExecutor { private static final LogicExecutor logicExecutor = new LogicExecutor(); private LogicExecutor(){} public static LogicExecutor getInstance() { return logicExecutor; } private UserLogic userLogic = UserLogic.getInstance(); public String signup(Map&lt;String, String&gt; params, HttpRequest request, HttpResponse response) { String id = params.get(\"userId\"); String pw = params.get(\"password\"); String name = params.get(\"name\"); String email = params.get(\"email\"); User newUser = new User(id, pw, name, email); return userLogic.signup(newUser); } public String login(Map&lt;String, String&gt; params, HttpRequest request, HttpResponse response) { String id = params.get(\"userId\"); String pw = params.get(\"password\"); return userLogic.login(id,pw,response); }}요구사항 5-2 로그인이 성공하면 로그인 상태를 유지할 수 있어야 한다. 즉, 로그인이 성공할 경우 요청 헤더의 Cookie 헤더 값이 logined = true. 로그인이 실패하면 logined = false로 전달되어야 한다. 구현public String login(String id, String pw, HttpResponse response) throws IOException { User findUser = findUser(id); if(findUser!=null &amp;&amp; pw.equals(findUser.getPassword())) { response.setCookie(\"logined=true; Path=/\"); return \"/index.html\"; } response.setCookie(\"logined=false; Path=/\"); return \"/user/login_failed.html\";}구현하면서 알게된 것들 Set-Cookie 헤더의 속성 중 Path값을 따로 설정하지 않으면 쿠키를 응답한 화면이 포함된 디렉토리와 그 하위 디렉토리로 요청하는 경우에만 쿠키를 송신한다. (참고)요구사항 6 접근하고 있는 사용자가 “로그인” 상태일 경우(Cookie로 판별) 사용자 리스트 페이지로 접근했을 때 사용자 목록을 출력한다.만약 로그인하지 않은 상태라면 로그인 페이지로 이동한다.구현 public String getUserList(HttpRequest request) { String isLogined = request.getCookie(\"logined\"); if(isLogined!=null &amp;&amp; isLogined.equals(\"true\")) { return \"/user/list.html\"; } return \"/user/login.html\"; }요구사항7 “CSS 지원하기”구현하기 전 들었던 생각들 초반에 요청할 때 css 파일도 요청하고 응답받는데 왜 css 파일이 적용이 안된 화면이 렌더링될까 ?구현 case REQUEST_STATIC_RESOURCE: String requestUrl = httpRequest.getRequestUrl(); responseBody = (requestUrl.equals(\"/\")) ? IOUtils.convertFileToByte(indexPage) : IOUtils.convertFileToByte(requestUrl); String contentType = \"text/html;charset=utf-8\"; if(requestUrl.contains(\".css\")) { contentType = \"text/css\"; } else if(requestUrl.contains(\".js\")) { contentType = \"application/javascript\"; } httpResponse.setStatusCode(HttpStatusCode2xx.OK); httpResponse.setHeader(\"Content-Type\", contentType); httpResponse.setHeader(\"Content-Length\", responseBody.length); break;구현하면서 알게된 것들 브라우저가 css 파일을 읽고 적용하기 위해선 Content-type을 \"text/css\"로 세팅해줘야 한다 js 파일은 Content-type \"application/javascript\"로 세팅해줘야한다.정리 구현한 웹 서버를 도식화 해보면 아래와 같다. 요구사항 5~6에서 파라미터를 원하는 것만 받게 하려면 어떻게 해야할지는 좀 더 공부해보자. " }, { "title": "【나만의 웨딩 매니저】 통신 방식 구조 정의", "url": "/posts/my-wedding-manager-communication-architecture/", "categories": "", "tags": "", "date": "2021-06-17 22:25:00 +0900", "snippet": " 매우 간단하게 초안을 그려봤다. 사실 직접 구현해본게 아니라 이렇게 하는게 맞는지도 모르겠다.앞으로 직접 구현하고 공부하면서 아래 내용을 수정하고 살을 붙여나가야 할 것 같다.구성도 고가용성을 위해 비동기적으로 처리할 수 있는건 다 비동기적으로 처리해보자. 이렇게 해봤을 때 어떤게 좋고 안 좋은지 등에 대해 직접 느껴보고 어떤식으...", "content": " 매우 간단하게 초안을 그려봤다. 사실 직접 구현해본게 아니라 이렇게 하는게 맞는지도 모르겠다.앞으로 직접 구현하고 공부하면서 아래 내용을 수정하고 살을 붙여나가야 할 것 같다.구성도 고가용성을 위해 비동기적으로 처리할 수 있는건 다 비동기적으로 처리해보자. 이렇게 해봤을 때 어떤게 좋고 안 좋은지 등에 대해 직접 느껴보고 어떤식으로 재구성하면 좋을지도 생각해보자 잘 모르겠는 부분 예를 들어 화면에서 지출 등록을 하는 경우, 비동기 요청/응답 방식으로 처리해서 사용자에게 ‘등록이 완료되었습니다’ 라는 식의메세지를 보여줄 수 있나 ?? 사용자가 일정에 대해 알림 설정한 날짜에 어떤식으로 감지하고 사용자에게 보내줘야할까 ??참고자료 크리스 리처드슨, 『마이크로서비스 패턴』, 길벗(2020), 3장" }, { "title": "【나만의 웨딩 매니저】 DB 테이블 구성하기", "url": "/posts/my-wedding-manager-table-architecture/", "categories": "", "tags": "", "date": "2021-06-15 22:25:00 +0900", "snippet": "들어가기 전업무 요건을 파악하고 데이터 모델링을 통해 필요한 테이블을 도출해보자.업무 요건 정의 구현해야할 기능 관점에서 업무 요건을 정리해보았다.회원 관리 로그인 최초 로그인시 회원등록 및 다음 사항 기입 권유 결혼 날짜 등록 (선택) 최대 예산 범위 설정 (선택) 가계(지출/수입) 관리 지출 및 수입 등록,...", "content": "들어가기 전업무 요건을 파악하고 데이터 모델링을 통해 필요한 테이블을 도출해보자.업무 요건 정의 구현해야할 기능 관점에서 업무 요건을 정리해보았다.회원 관리 로그인 최초 로그인시 회원등록 및 다음 사항 기입 권유 결혼 날짜 등록 (선택) 최대 예산 범위 설정 (선택) 가계(지출/수입) 관리 지출 및 수입 등록, 수정, 삭제 원하는 기간 내의 지출/수입 내역을 엑셀파일로 다운받을 수 있다. 예비 지출(지출 등록 날짜가 오늘 날짜보다 이후인 경우)에 대해 다음 날짜 0시를 기준으로 이미 지출된 항목으로 변경한다.일정 관리 일정 등록, 수정, 삭제 일정과 관련하여 사용자가 지정해놓은 날 사용자에게 알림이 간다. 일정 내에서 발생하는 지출에 대해 복수로 등록할 수 있다. 등록한 일정이 캘린더에 맵핑된다.데이터 모델링개념 모델링 회원이 반드시 있어야 지출, 일정이 있다. 한 명의 회원이 여러 번의 지출을 할 수 있으며, 여러 개의 일정이 있을 수 있다. 지출과 일정이 없을 수도 있다. 하나의 일정에 대해 지출이 없거나, 여러 번의 지출이 있을 수 있다. 드레스 투어(일정) - 피팅 비용 + 가계약 비용 + 발렛 파킹 비용 등 회원이 일정에 대해 알림을 신청해 놓는 경우 일정 알림에 추가된다. 알림을 여러 번 받기 원할 수도 있기 때문에 1:N 관계로 구성 추후 운영의 관점에서 생각했을 때, 알림에 대한 정상 처리 여부 등을 확인하기 위해 이력을 관리하는게 좋을 것 같다고 판단하여 일정 알림 이력 엔터티 추가.논리 모델링모델링 과정에서의 의식의 흐름.. email로 사용자를 식별하는 것의 타당성 ? 물론 식별은 되지만, 추후에 OAuth가 아니라 다른 방식으로 로그인하게 되면 꼭 email이 아니게 될 수 있으므로 email 대신 user_id가 나을 것 같다. 일정에 ‘전체 지출’ 컬럼이 추가되는게 나을까 아니면 지출 테이블에 조인걸어서 가져오는게 나을까 ? JOIN 거는 것 보다는 테이블 내에서 가져올 수 있으면 좋을 것 같다. 페이백, 포인트 적립 등도 고려해야 하기 때문에 지출 대신 다른 엔터티 이름이 필요할 것 같다 ! 수입/지출 내역 관리를 어떤 단어로 표현하면 좋을까 ? 계좌라는 의미로 많이 쓰이지만, ‘가계’라는 뜻도 가진 account 정도면 괜찮을 것 같다. expense, income을 따로 두는게 좋을까 ? 사실 지출에 비해 수입은 거의 없을 것이라고 생각하기 때문에, 굳이 테이블을 두 개로 나누지 말고 type으로 구분하면 될 것 같다. 일정과 관련없는 지출/수입이 있을 수 있다. (스드메 예약 비용, 페이백 등)참고. 엔터티 간 관계에서 선 구분 점선 : 비식별 관계 (부모 엔터티의 식별자가 없어도 자식 엔터티의 레코드 생성 가능) 선 : 식별 관계 (부모의 식별자가 자식 엔터티의 레코드를 식별하는데 꼭 필요)느낀점엔터티의 이름, 속성 및 엔터티 간의 관계는 적절한지 등에 대해 먼저 생각해봄으로써, 애플리케이션을 제작하면서 고치려면 번거로울 수 있었던 것들을미리 파악하고 정리할 수 있었던 것 같다. 또한 업무 요건이 상세하게 정의될수록 추후에 변경이 적은 테이블 설계가 가능할 것 같다.더 공부해야할 부분 효율적인 쿼리 짜는 방법 (JOIN 사용할 때 성능에 미치는 영향 등)참고 자료 데이터 모델링 공부했던 부분" }, { "title": "마이크로서비스간 통신(3) - 비동기 방식", "url": "/posts/msa-pattern-chap3-3/", "categories": "", "tags": "", "date": "2021-06-14 22:25:00 +0900", "snippet": "※ 해당 내용은 ‘마이크로서비스 패턴(크리스 리처드슨)’ 3장을 읽고 정리한 내용입니다.비동기 메시징 패턴 응용 통신 메시징은 서비스가 메시지를 비동기적으로 주고받는 통신 방식으로서 메시지 브로커를 사용할 수도 있고 서비스간 직접 통신을 할 수도 있다.비동기 통신이기 때문에 클라이언트가 응답을 기다리며 블로킹하지 않고 클라이언트는 응답을 바로 받지 ...", "content": "※ 해당 내용은 ‘마이크로서비스 패턴(크리스 리처드슨)’ 3장을 읽고 정리한 내용입니다.비동기 메시징 패턴 응용 통신 메시징은 서비스가 메시지를 비동기적으로 주고받는 통신 방식으로서 메시지 브로커를 사용할 수도 있고 서비스간 직접 통신을 할 수도 있다.비동기 통신이기 때문에 클라이언트가 응답을 기다리며 블로킹하지 않고 클라이언트는 응답을 바로 받지 못할 것이라는 전제하에 작성한다.1. 메시지와 채널 메시지는 메시지 채널을 통해 교환된다. 송신자가 채널에 메시지를 쓰면 수신자는 채널에서 메시지를 읽는다. 메시지 헤더와 바디(본문)으로 구성 헤더에는 송신된 데이터에 관한 메타데이터, 메시지ID, 반환주소 등이 포함 본문은 실제로 송신할 데이터 텍스틑 또는 이진 포맷의 데이터 메시지의 종류는 문서, 커맨드, 이벤트 등으로 다양 메시지 채널 채널은 점대점 채널, 발행-구독 채널 두 종류가 있다. 점대점 채널 채널을 읽는 컨슈머 중 딱 하나만 지정해서 메시지 전달 일대일 통신 방식의 서비스가 이 채널을 사용 (ex : 커맨드 메시지) 발행-구독 채널 같은 채널을 바라모는 모든 컨슈머에 메시지 전달 일대다 통신 방식의 서비스가 이 채널을 사용 (ex : 이벤트 메시지) 출처 : https://thebook.io/007035/ch03/03/01/02/ 2. 메시지 상호 작용 스타일 스타일에 따라 메시징으로 직접 구현 가능한 것, 메시징을 토대로 구현해야 하는 것도 있다.요청/응답 및 비동기 요청/응답 클라이언트는 수행할 작업과 매개변수가 담긴 커맨드 메시지를 서비스가 소유한 점대점 메시징 채널로 송신 서비스는 요청 처리 후 응답 메시지를 클라이언트가 소유한 점대점 메시징 채널로 송신 요청 메시지와 응답 메시지는 짝이 맞아야 하는데, 이는 MessageId와 CorrelationId를 통해 맞춰볼 수 있다. 메시징으로 통신하는 클라이언트/서비스 간 상호작용은 비동기적이다. 이론적으로 클라이언트가 응답 수신시까지 블로킹할 수는 있지만, 실제로 클라이언트는 응답을 비동기 처리 하고클라이언트 인스턴스 중 하나가 응답을 처리 출처 : https://thebook.io/007035/ch03/03/02/01/ 단방향 알림 서비스가 소유한 점대점 채널로 메시지를 보내면, 서비스는 이 채널을 구독해서 메시지 처리 서비스는 응답을 반환하지 않는다.발행/구독 클라이언트는 여러 컨슈머가 읽는 발행/구독 채널에 메시지를 발행하고, 서비스는 도메인 객체의 변경 사실을 알리는 도메인 이벤트를 발행 도메인 이벤트를 발행한 서비스는 해당 도메인 클래스의 이름을 딴 발행/구독 채널을 소유 (ex : 주문 서비스는 Order 이벤트를 Order 채널에 발행) 서비스는 자신이 관심 있는 도메인 객체의 이벤트 채널을 구독발행/비동기 응답 발행/구독과 요청/응답 방식을 조합 클라이언트는 응답 채널 헤더가 명시된 메시지를 발행/구독 채널에 발행하고, 컨슈머는 CorrelationId가 포함된 응답 메시지를지정된 응답 채널에 송신 클라이언트는 CorrelationId로 응답을 취합하여 응답 메시지와 요청을 맞추어본다3. 메시지 브로커 메시지 브로커는 서비스가 서로 통신할 수 있게 해주는 인프라 서비스로서, 메시징 기반의 애플리케이션은 대부분 메시지 브로커를 사용한다.서비스가 서로 직접 통신하는 브로커리스 기반의 메시지 아키텍처도 있지만 일반적으로 브로커 기반의 아키텍처가 갖는 이점이 크다.브로커리스 아키텍처 서비스간 메시지를 직접 교환 대표적인 브로커리스 메시징 기술로는 ZeroMQ가 있다. 장점 서비스간 직접 전달되므로 네트워크 트래픽이 가볍고 지연 시간이 짧다. 메시지 브로커가 성능 병목점이나 SPOF(Single Poing Of Failure, 단일 장애점)이 될 일이 없다. 브로커가 없기 때문에 관리 포인트가 적어진다. 단점 서비스가 서로의 위치를 알고 있어야 하므로 서비스 디스커버리 매커니즘을 사용해야 한다. 송/수신자 모두 실행 중이어야 하므로 가용성이 떨어진다. 전달 보장(delivery-guarantee) 같은 매커니즘을 구현하기 어렵다. 브로커 기반 메시징 개요 대표적인 메시지 브로커로서 ActiveMQ, RabbitMQ, 아파치 카프카가 있다. AWS 키네시스, AWS SQS와 같은 클라우드 기반의 메시징 서비스도 있다. 장점 느슨한 결합 클라이언트는 서비스 인스턴스를 몰라도 되기 때문에 서비스 디스커버리 매커니즘도 필요 없다. 메시지 버퍼링 메시지 브로커는 컨슈머가 메시지를 처리할 수 있을 때까지 큐에 메시지를 보관하므로 가용성이 높아진다. 다양한 통신 방식을 지원 단점 성능 병목 가능성 메시지 브로커가 성능 병목점이 될 가능성이 있다.(따라서, 확장성이 좋아야한다) 단일 장애점 가능성 SPOF가 될 수 있기 때문에 가용성이 높아야한다. 운영 복잡도 부가 브로커별로 특징이 있다. 어떤 브로커는 지연 시간이 매우 짧지만 메시지 순서 유지 및 전달 보장이 안된다거나, 메시지를 메모리에만 저장한다. 어떤 브로커는 지연 시간은 길지만, 메시지 전달을 보장하고 디스크에 저장한다. 따라서, 애플리케이션 요건에 따라 적합한 브로커를 선택해야 한다.메시지 브로커로 메시지 채널 구현 메시지 채널은 브로커마다 구현 방식이 조금씩 다르다. 메시지 브로커 점대점 채널 발행-구독 채널 JMS 큐 토픽 아파치 카프카 토픽 토픽 RabbitMQ(AMQP 브로커) 익스체인지+큐 팬아웃 익스체인지, 컨슈머 개별 큐 AWS 키네시스 스트림 스트림 AWS SQS 큐 - 4. 수신자 경합과 메시지 순서 유지 예를 들어, 동일한 점대점 채널을 읽는 서비스 인스턴스가 3개 있고, 송신자는 주문 생성 → 변경 → 취소 이벤트 메시지를 차례로 전송한다고 가정해보자.만약 네트워크 이슈, 가비지 컬렉션 문제 등으로 지연이 발생하고 메시지 처리 순서가 어긋난다면,주문을 생성하기도 전에 취소 처리를 해야하는 상황이 벌어질 수도 있다.따라서, 메시지를 동시 처리하는 경우 각 메시지를 정확히 한 번만 순서대로 처리하는 것이 보장되어야 한다.샤딩된(파티셔닝된) 채널 이용해서 메시지 처리 순서 보장하기 샤딩된 채널은 복수의 샤드로 구성되며, 각 샤드는 채널처럼 작동 송신자는 메시지 헤더에 샤드 키를 지정하고, 메시지 브로커는 샤드 키별로 샤드/파티션에 메시지를 배정 메시지 브로커는 여러 수신자 인스턴스를 묶어 마치 동일한 수신자인 것처럼 취급한다(카프카에선 ‘컨슈머 그룹’) 메시지 브로커는 각 샤드를 하나의 수신자에 배정하고, 수신자가 시동/종료하면 샤드를 재배정한다. 아래 그림에서 같은 주문에 대한 이벤트(주문 생성, 변경, 취소 등)는 동일한 샤드에 발행될 것이고(샤드 키인 orderId가 같으므로)샤드는 하나의 수신자 인스턴스에 배정되기 때문에 메시지 처리 순서가 보장된다. 출처 : https://thebook.io/007035/ch03/03/05-01/ 5. 중복 메시지 처리 시스템이 정상일 때 ‘적어도 한 번 전달’을 보장하는 메시지 브로커는 각 메시지를 한 번만 전달한다.그러나 클라이언트나 네트워크 또는 브로커 자신이 실패할 경우, 같은 메시지를 여러번 전달할 수도 있다.단적인 예로, 주문 생성 → 취소 이벤트를 발행했는데 문제가 생겨 생성 이벤트만 재전송하게 되면,결국 주문이 접수되는 문제가 생길 수 있다.멱등한 로직 작성 멱등하다 : 동일한 입력 값을 반복 호출해도 결과가 달라지지 않는다 (ex : 주문 취소) 메시지 재전송시 순서를 유지하다는 전제하에 멱등한 메시지 로직은 여러 번 호출되도 별 문제가 없다. 하지만 실제로 이렇게 멱등한 애플리케이션 로직은 많지 않다.메시지 추적과 중복 메시지 솎아 내기 예를 들어, 신용카드 결제 승인과 같은 로직은 여러번 호출되면 심각한 문제를 발생시킬 수 있다. 따라서 중복 메시지를 솎아 내는 메시지 핸들러가 필요하다. 하나의 방법으로 컨슈머가 소비하는 메시지 ID를 DB 테이블에 저장하여 메시지 ID를 이용하여 메시지 처리 여부를 추적하면서 중복 메시지를 알아낸다. 즉, 컨슈머는 메시지를 처리할 때 비즈니스 엔터티를 생성/수정하는 트랜잭션의 일부로 메시지 ID를 DB 테이블에 기록한다.6. 트랜잭셔널 메시징 DB 업데이트와 메시지 전송을 한 트랜잭션으로 묶지 않으면, DB 업데이트 후 메시지는 아직 전송되지 않은 상태에서 서비스가 중단될 수 있다.이 두 작업이 서비스에서 원자적으로 수행되지 않으면, 실패할 경우 시스템은 매우 불안정한 상태가 될 수 있다.애플리케이션에서 메시지를 확실하게 발행하려면 어떻게 해야할까 ?DB 테이블을 메시지 큐로 활용 RDBMS 기반의 애플리케이션의 경우 DB 테이블을 임시 메시지 큐로 사용할 수 있다.(트랜잭셔널 아웃 박스 패턴) 로컬 ACID 트랜잭션이기 때문에 원자성은 자동으로 보장된다. NoSQL DB인 경우에는 DB에 레코드로 적재된 비즈니스 엔터티에 발행할 메시지 목록을 가리키는 속성에DB 엔터티 업데이트시 메시지를 덧붙인다. 출처 : https://thebook.io/007035/ch03/03/07/01/ 이벤트 발행 : 폴링 발행기 패턴 메시지 릴레이로 테이블을 폴링해서 미발행 메시지를 조회 메시지 릴레이는 조회한 메시지를 하니씩 각자의 목적지 채널로 보내서 메시지 브로커에 발행하고 OUTBOX 테이블에서 메시지 삭제 DB 폴링은 규모가 작은 경우 적합하고, 자주 하는 경우 비용이 유발된다.이벤트 발행 : 트랜잭션 로그 테일링 패턴 메시지 릴레이로 DB 트랜잭션 로그(커밋 로그)를 테일링한다. 트랜잭션 로그 마이너로 로그를 읽어 변경분을 하나씩 메시지로 브로커에게 발행 트랜잭션 로그 마이너는 로그 항목을 읽고, 삽입된 메시지에 대응되는 각 로그 항목을 메시지로 전환하여 브로커에 발행 이 방식을 응용한 사례로는, 디비지움, 링크드인 데이터버스, DynamoDB, 이벤추에이트 트램 등이 있다. 출처 : https://thebook.io/007035/ch03/03/07/03/ 동기 상호 방식 제거하기 요청을 처리하는 과정에서 타 서비스와 동기 통신을 하면 그만큼 가용성이 떨어지므로 가능한 서비스가 비동기 메시징을 이용하여 통신하도록 설계하는 것이 바람직하다.비동기 방식으로 처리 동기 방식으로 처리 출처 : https://thebook.io/007035/ch03/04/01/ 비동기 방식으로 바꿔보기 출처 : https://thebook.io/007035/ch03/04/02/01/ 데이터 복제 서비스 요청 처리에 필요한 데이터의 레플리카를 유지하는 방법 데이터 레플리카를 통해 다른 서비스와 상호 작용할 필요가 없어진다. 하지만 대용량 데이터의 레플리카를 만드는 것은 매우 비효율적이다. 출처 : https://thebook.io/007035/ch03/04/02/02/ 응답 반환 후 마무리 출처 : https://thebook.io/007035/ch03/04/02/03-01/ 주문 서비스는 다른 서비스를 호출하지 않은 채 주문을 생성한 후, 다른 서비스와 메시지를 교환하여 생성한 Order를 비동기적으로 검증한다. 이렇게 처리하면 다른 서비스가 내려가더라도 주문 서비스는 계속 주문을 생성하고 클라이언트에 응답을 할 수 있다.나중에 문제가 됐던 서비스가 재기동 되면 큐에 쌓인 메시지를 처리하고 밀린 주문을 검증할 수 있다. 이처럼 요청을 완전히 처리하기 전에 클라이언트에 응답하는 서비스는 클라이언트가 조금 복잡해진다. 클라이언트 입장에서 주문 생성 성공 여부를 알아내려면 주기적으로 폴링하거나 주문 서비스가 알림 메시지를 보내줘야 한다. 복잡하지만 동기 방식보다는 이게 더 나은 방법이다. 왜냐하면 분산 트랜잭션 관리 이슈를 이런 방식으로 해결할 수 있기 때문이다. 실제로 적용해보기 토이 프로젝트 ‘나만의 웨딩 매니저’ 통신 방식 구조더 공부해야할 부분 논블로킹 / 비동기 로그 테일링 메시지 브로커별 특징참고 자료 크리스 리처드슨, 『마이크로서비스 패턴』, 길벗(2020), p104-152." }, { "title": "마이크로서비스간 통신(2) - 동기 방식", "url": "/posts/msa-pattern-chap3-2/", "categories": "", "tags": "", "date": "2021-06-14 22:25:00 +0900", "snippet": "※ 해당 내용은 ‘마이크로서비스 패턴(크리스 리처드슨)’ 3장을 읽고 정리한 내용입니다.동기 RPI 패턴 응용 통신 RPI(Remote Procedure Invocation)는 클라이언트가 서비스에 요청을 보내고 서비스가 처리 후 응답을 회신하는 IPC 응답 대기 중에 블로킹하는 클라이언트, 리액티브한 논블로킹 아키텍처를 가진 클라이언트도 있지만,...", "content": "※ 해당 내용은 ‘마이크로서비스 패턴(크리스 리처드슨)’ 3장을 읽고 정리한 내용입니다.동기 RPI 패턴 응용 통신 RPI(Remote Procedure Invocation)는 클라이언트가 서비스에 요청을 보내고 서비스가 처리 후 응답을 회신하는 IPC 응답 대기 중에 블로킹하는 클라이언트, 리액티브한 논블로킹 아키텍처를 가진 클라이언트도 있지만, 메시징으로 통신하는 클라이언트와 달리응답이 제때 도착한다고 가정한다. RPI의 작동 원리 클라이언트의 비즈니스 로직은 프록시 인터페이스 호출 (프록시 인터페이스는 RPI 프록시 어댑터 클래스로 구현) RPI 프록시가 서비스에 전달한 요청은 RPI 서버 어댑터 클래스가 접수 이 클래스는 서비스 인터페이스를 통해 비즈니스 로직 호출 출처 : https://thebook.io/007035/ch03/02-01/ 1. 동기 RPI 패턴 : REST HTTP 동사를 사용해서 URL로 참조되는 리소스를 조작 REST API 설계시, 어떻게 최소한의 호출(요청)로 연관된 많은 객체들을 가져올 수 있을지 고민해야한다. 따라서 데이터를 효율적으로 조회할 수 있게 설계된 GraphQL, Netflix Falcor등의 기술이 각광을 받기 시작했다. 장점 API 자체가 단순하고, Postman, curl 등의 도구를 사용해서 테스트를 간편하게 할 수 있다. HTTP는 방화벽 친화적(방화벽 입장에서 쉽게 접근 가능한 특성을 지닌다 - 단일 포트 사용, 인터넷에서 서버 접속 가능, TCP 사용) 중간 브로커가 필요하지 않기 때문에 시스템 아키텍처가 단순해진다. 단점 시나리오가 복잡해질수록 효율이 떨어질 수 있다. 요청/응답 스타일의 통신만 지원한다. 클라이언트/서비스가 직접 통신하기 때문에 가용성이 떨어질 수 있다.(항상 양쪽 모두가 실행중이어야 하기 때문에) 요청 한 번으로 여러 리소스를 가져오기 어렵다. 다중 업데이트 작업을 HTTP 동사에 매핑하기 어려울 수 있다. 2. 동기 RPI 패턴 : gRPC gRPC는 구글에서 개발한 RPC(Remote Procedure Call) 시스템이다. HTTP는 한정된 동사만 지원하기 때문에 다양한 업데이트 작업을 지원하는 REST API를 설계하기가 쉽지 않다.바로 이러한 문제를 해결하고자 등장한 기술이 바로 gRPC이다. 장점 다양한 업데이트 작업이 포함된 API를 설계하기 쉽다 큰 메시지를 교환할 때 콤팩트하고 효율적이다 양방향 스트리밍으로 인해 RPI, 메시징 두 가지 통신 방식이 모두 가능하다. 다양한 언어로 작성된 클라이언트/서버 간 연동이 가능하다. 단점 자바스크립트 클라이언트가 하는일이 REST/JSON 기반 API 보다 많다. 구현 방화벽은 HTTP/2를 지원하지 않는다. 3. 부분 실패 처리 분산 시스템은 서비스가 다른 서비스를 동기 호출할 때마다 부분 실패할 가능성이 항상 존재한다. 예를 들어, 요청에 제때 응답하지 못하거나,요청한 서비스가 내려가는 등의 상황이 있을 수 있다. 클라이언트는 응답을 기다리면서 블로킹되기 때문에 요청한 서비스의 실패가클라이언트로 거슬러 올라가면서 전체 시스템의 중단을 초래할 수도 있다. 따라서, 부분 실패가 애플리케이션 전체에 전파되지 않도록서비스를 설계해야 한다.견고한 RPI 프록시 설계를 통한 해결 방법 Netflix Histrix는 아래와 같은 다양한 패턴이 구현된 오픈 소스 라이브러리이므로 JVM 환경이라면 이를 통해 RPI 프록시를 구현해보는 것을 권장. 네트워크 타임아웃 응답 대기시 무한정 블로킹되지 않도록 타임아웃 설정하여 리소스 고갈 방지 미처리 요청 개수 제한 클라이언트가 특정 서비스에 요청 가능한 미처리 요청의 최대 개수 설정 최대 개수에 이르면 해당 요청은 즉시 실패 처리 회로 차단기 패턴 성공/실패 비율(에러율)이 임계치를 초과하면 그 이후 요청은 바로 실패로 처리 타임아웃 시간 이후 클라이언트가 재시도해서 성공하면 회로 차단기는 닫힌다. 실패시 조치 방법 결정 Netflix Histrix 같은 라이브러리는 부분적인 솔루션에 불과하기 때문에무응답 원격 서비스를 어떻게 복구하면 좋을지는 상황에 맞게 판단해야 한다. 클라이언트가 알 수 있도록 요청 서비스에서 클라이언트에게 에러 반환 부분 실패 시 미리 정해진 기본값이나 캐시된 응답 등 대체 값(fallback value) 반환 위 방법 이외에도 각각의 서비스가 실패할 경우를 대비한 대응 방안이 준비되어 있어야 한다.4. 서비스 디스커버리 서비스를 호출하는 코드는 서비스 인스턴스의 IP 주소 및 포트를 알고 있어야 한다. 클라우드 기반의 마이크로서비스 애플리케이션은 이러한 네트워크 위치가 동적이기 때문에이를 식별하는 일이 결코 간단하지 않다. 서비스 인스턴스마다 네트워크 위치가 동적 배정되고, 자동 확장, 실패, 업그레이드 등 여러 가지 사유로 계속 달라지므로이러한 네트워크 위치를 관리해줄 ‘서비스 디스커버리’가 반드시 필요하다.서비스 레지스트리(Service Registry) 서비스 인스턴스의 네트워크 위치를 DB화 한 것 서비스 인스턴스가 시작/종료할 때마다 서비스 레지스트리 업데이트 클라이언트에서 서비스 호출시, 서비스 디스커버리가 서비스 레지스트리에서 가용 서비스 인스턴스 목록을 가져오고 그중 한 서비스로 요청을 보낸다. 서비스 레지스트리를 사용하는 일반적인 방법 클라이언트/서비스가 직접 서비스 레지스트리와 상호 작용 배포 인프라로 서비스 디스커버리 처리 대표적인 서비스 레지스트리로는 Netflix Eureka가 있다.애플리케이션 수준의 서비스 디스커버리 패턴 서비스 클라이언트는 서비스 레지스트리로부터 전체 인스턴스 목록을 가져와 한 인스턴스로 요청을 라우팅한다. 라우팅시 라운드-로빈, 랜덤과 같은 부하분산 알고리즘 사용 장점 다양한 플랫폼에 서비스가 배포된 경우에도 처리 가능 (ex : 서비스 일부는 쿠버네티스, 나머지는 레거시 환경) 단점 사용하는 언어(프레임워크)에 맞는 각각의 서비스 디스커버리 라이브러리가 필요(따라서, 배포 인프라를 사용하여 서비스 디스커버리 매커니즘 활용하는 것이 좋다) 출처 : https://thebook.io/007035/ch03/02/04/02/ 플랫폼에 내장된 서비스 디스커버리 패턴 도커, 쿠버네티스 등과 같은 최신 배포 플랫폼에는 대부분 서비스 레지스트리, 서비스 디스커버리 매커니즘이 탑재되어 있다. 배포 플랫폼은 가상IP(VIP) 주소, VIP 주소로 해석되는 DNS명을 각 서비스마다 부여 서비스 클라이언트가 DNS명/VIP를 요청하면 배포 플랫폼이 가용 서비스 인스턴스 중 하나로 요청을 라우팅 즉, 배포 플랫폼이 서비스 등록, 서비스 디스커버리, 요청 라우팅을 전부 담당 출처 : https://thebook.io/007035/ch03/02/04/03/ 실제로 적용해보기 토이 프로젝트 ‘나만의 웨딩 매니저’ 통신 방식 구조더 공부해야할 부분 gRPC 블로킹과 동기참고 자료 크리스 리처드슨, 『마이크로서비스 패턴』, 길벗(2020), p104-152." }, { "title": "마이크로서비스간 통신(1) - 개요", "url": "/posts/msa-pattern-chap3-1/", "categories": "", "tags": "", "date": "2021-06-14 22:25:00 +0900", "snippet": "※ 해당 내용은 ‘마이크로서비스 패턴(크리스 리처드슨)’ 3장을 읽고 정리한 내용입니다.Intro 프로세스 간 통신(Inter-Process Communication) 프로세스들 사이에 서로 데이터를 주고받는 행위 또는 그에 대한 방법이나 경로 마이크로서비스 아키텍처는 애플리케이션을 여러 서비스로 구성하며, 서비스 인스턴...", "content": "※ 해당 내용은 ‘마이크로서비스 패턴(크리스 리처드슨)’ 3장을 읽고 정리한 내용입니다.Intro 프로세스 간 통신(Inter-Process Communication) 프로세스들 사이에 서로 데이터를 주고받는 행위 또는 그에 대한 방법이나 경로 마이크로서비스 아키텍처는 애플리케이션을 여러 서비스로 구성하며, 서비스 인스턴스는 여러 머신에서 실행되는 프로세스 형태이므로반드시 IPC를 통해 상호작용 해야한다. IPC는 애플리케이션 가용성(정상적으로 사용가능한 정도)에 영향을 미치며, 트랜잭션 관리와도 맞물려 있다.IPC 개요1. 통신 방식   일대일 일대다 동기 요청 / 응답 - 비동기 비동기 요청 / 응답 단방향 알림 발행 / 구독 발행 / 비동기 응답 요청/응답 클라이언트는 응답이 제때 도착할 것을 기다리며 대기하는 동안 블로킹된다. 서비스와 강하게 결합된다. 비동기 요청/응답 클라이언트는 서비스에 요청하고 서비스는 비동기적으로 응답. 클라이언트는 응답에 대한 대기 중에 블로킹하지 않는다. 단방향 알림 클라이언트는 서비스에 요청을 하고 서비스는 응답을 보내지 않는다. (어떤 경우 ??) 발행/구독 클라이언트는 알림 메시지를 발행하고 여기에 관심 있는 0개 이상의 서비스가 메시지를 소비 발행/비동기 응답 클라이언트는 요청 메시지를 발행하고 주어진 시간 동안 관련 서비스가 응답하길 기다린다. 2. 메시지 포맷 IPC의 핵심은 메시지 교환이며, 대부분의 메시지는 데이터를 담고 있기 때문에 데이터 포맷은 중요한 설계 결정 항목이다. 메시지 포맷은 크게 텍스트와 이진 포맷으로 분류된다.텍스트 메시지 포맷 대표적인 예로 JSON, XML이 있다. 장점 자기 서술적(그 자체만으로도 의미가 분명한)이다. 메시지 컨슈머는 자신이 관심 있는 값만 골라 쓰면 되므로 스키마가 자주 바뀌어도 하위 호환성이 쉽게 보장된다. 단점 메시지가 다소 길다 (특히, XML) 속성값 이외에 속성명이 추가되는 오버헤드가 있다. 데이터가 많은 메시지는 텍스트를 파싱하는 오버헤드가 있다. 이진 메시지 포맷 (???) 대표적으로 프로토콜 버퍼와 아브로가 있다. 메시지 구조 정의에 필요한 타입 IDL(Interface Definition Language)를 제공 컴파일러는 메시지를 직렬화/역직렬화하는 코드 생성 따라서, 서비스를 API 우선 접근 방식으로 설계할 수밖에 없다.더 공부해야할 부분 이진 메세지 포맷참고 자료 크리스 리처드슨, 『마이크로서비스 패턴』, 길벗(2020), p104-152." }, { "title": "Multi Page Application vs Single Page Application", "url": "/posts/spa-vs-mpa/", "categories": "", "tags": "", "date": "2021-06-04 00:25:00 +0900", "snippet": "MPA(Multi Page Application) 클라이언트가 새로운 페이지를 요청할 때마다 서버에 랜더링하도록 요청한다. 이러한 방식을 SSR(Server-Side-Rendering)이라고 한다. MPA를 사용한다고 알려진 대표적인 서비스로는 Amazon, eBay 등이 있다. 출처 : https://dzone.com/articles/th...", "content": "MPA(Multi Page Application) 클라이언트가 새로운 페이지를 요청할 때마다 서버에 랜더링하도록 요청한다. 이러한 방식을 SSR(Server-Side-Rendering)이라고 한다. MPA를 사용한다고 알려진 대표적인 서비스로는 Amazon, eBay 등이 있다. 출처 : https://dzone.com/articles/the-comparison-of-single-page-and-multi-page-appli장/단점 장점 한 페이지당 하나의 키워드에 대해 애플리케이션을 최적화할 수 있기 때문에 SEO(Search Engine Optimization, 검색 엔진 최적화)가 쉽다. 필요한 만큼 페이지를 추가할 수 있다. 페이지에 많은 정보를 표시해야 할 때 유리하다. 단점 프론트와 백엔드 개발이 강하게 결합된다. 애플리케이션의 컨텐츠가 지속적으로 다시 로드되므로 서버의 부하가 증가한다. 따라서, 웹 페이지 속도와 전체 시스템 성능에 부정적인 영향을 미칠 수 있다. SPA(Single Page Application) 에플리케이션에 필요한 모든 정적 리소스를 최초 한 번에 다운로드한다. 그 이후 새로운 페이지 요청이 있을 때, 페이지 갱신에 필요한 데이터만 전달 받아서 페이지를 갱신한다. 브라우저 내에서 작동하며 사용하는 동안 페이지를 다시 로드할 필요가 없다. JavaScript를 사용하여 모든 컨텐츠를 로드한다. SPA를 사용한다고 알려진 대표적인 서비스로는 Gmail, Google 지도, Facebook, GitHub 등이 있다. 이러한 방식을 CSR(Client-Side-Rendering)이라고 한다. SPA도 필요에 따라 SSR(Server Side Rendering) 방식으로 렌더링할 수 있다. 출처 : https://dzone.com/articles/the-comparison-of-single-page-and-multi-page-appli장/단점 장점 대부분의 리소스(HTML+CSS+js)는 애플리케이션 최초 한 번만 로드되므로, 최초 로드된 이후 페이지 랜더링이 빠르다. 서버에서 페이지를 렌더링하기 위해 코드를 작성할 필요가 없다. 일반적으로 서버를 전혀 사용하지 않고도 개발을 시작할 수 있다. 웹 애플리케이션 및 네이티브 모바일 애플리케이션에 동일한 백엔드 코드를 재사용할 수 있다. SPA는 모든 로컬 스토리지를 효과적으로 캐시할 수 있다. 애플리케이션은 요청을 하나만 전송하고 모든 데이터를 저장한 다음 이 데이터를 사용할 수 있으며 오프라인에서도 작동한다. 단점 페이지 컨텐츠가 AJAX에 의해 로드되기 때문에, SEO(Search Engine Optimization)를 최적화하기가 어렵다. 최초에 모든 정적 리소스를 한번에 다 받기 때문에 초기 구동 속도가 느리다. MPA에 비해 SPA의 보안성이 떨어진다. 공격자는 XSS를 통해 클라이언트 측 스크립트를 웹 애플리케이션에 주입할 수 있다. 많은 기능을 통합할수록 로딩 시간이 길어지고 전반적인 성능이 저하될 수 있다. Hybrid Approach Gatsby JS 및 Next.js와 같은 프레임워크는 하이브리드 방식으로 작동한다. 즉, SSR로 초기 페이지를 로드한 다음 CSR을 사용하여 후속 페이지를 다시 로드하는 방식으로 두 가지 장점을 모두 누릴 수 있다.나의 선택은 ? 단순한 생각으로는 두 가지 방식의 장점을 취합한 하이브리드 방식이 가장 좋을 것 같다. 만약, SPA나 MPA 둘 중 하나를 선택해야 한다면, 애플리케이션에서 보여줄 페이지, 컨텐츠의 특징, 각 방식의 장/단점을 고려해야 할 것 같다. 내 서비스가 특정 키워드에 잘 노출이 되어야한다면 SEO에 강점이 있는 MPA를 선택해야 할 것 같다 조금 더 빠른 페이지 로딩을 원하고 서버측 부하를 덜고 싶다면 SPA를 선택해야 할 것 같다 더 공부해야할 부분 SPA Framework Next.js참고 자료 https://medium.com/@NeotericEU/single-page-application-vs-multiple-page-application-2591588efe58 https://dzone.com/articles/the-comparison-of-single-page-and-multi-page-appli https://www.growth-rocket.com/blog/a-closer-look-at-client-side-server-side-rendering/" }, { "title": "BufferedReader의 readLine() 메서드 이후의 코드로 진행이 안되는 현상", "url": "/posts/issue-bufferedReader/", "categories": "경험하기, 이슈 노트", "tags": "JAVA I/O", "date": "2021-06-01 00:29:00 +0900", "snippet": "상황 ‘자바 웹 프로그래밍 NextStep’ 3장의 과제 중 하나인 웹 서버를 구현하기 위해 HTTP 요청이 서버에 어떤식으로 들어오는지를 보고싶었다. 따라서, 단순 확인을 위해 다음과 같은 코드를 작성하여 실행시키고 브라우저에서 localhost:8080 으로 요청을 보냈다. public class TestServer { public vo...", "content": "상황 ‘자바 웹 프로그래밍 NextStep’ 3장의 과제 중 하나인 웹 서버를 구현하기 위해 HTTP 요청이 서버에 어떤식으로 들어오는지를 보고싶었다. 따라서, 단순 확인을 위해 다음과 같은 코드를 작성하여 실행시키고 브라우저에서 localhost:8080 으로 요청을 보냈다. public class TestServer { public void start() throws IOException { ServerSocket serverSocket = new ServerSocket(8080); Socket connection; while((connection = serverSocket.accept()) !=null) { System.out.println(String.format(\"[connection info]%nIpAddr : %s, Port : %s%n\", connection.getInetAddress(), connection.getPort())); BufferedReader br = new BufferedReader(new InputStreamReader(connection.getInputStream())); String line; while((line=br.readLine())!=null) { // 여기서 멈춘다 System.out.println(line); } System.out.println(\"END!\"); } } public static void main(String[] args) throws IOException { TestServer server = new TestServer(); server.start(); }} 아래와 같이 요청이 들어오는 것을 확인할 수 있었고, ‘Http Method(GET, POST, …)가 뭔지 확인하려면 요청의 첫 번째 라인을 확인하면 되겠구나’ 등의 판단을 할 수 있었다. 하지만, 문제는 코드의 15번째 라인 System.out.println(\"END!\"); 에 대한 출력이 콘솔에 찍히지 않았다는 것이었다.디버거로 확인해보니 무한루프를 도는 것도 아니고 위 코드의 11번째 라인에서 대기 상태가 되는 것을 확인할 수 있었다.해결 과정 스택오버플로우에 있는 글 을 통해BufferedReader의 readLine() 메서드는 라인이 종료되었다고 판단되지 않으면 값을 리턴하지 않는다는 것을 알게되었다. 그렇다면 라인이 종료되었다는 것을 판단하는 기준은 뭘까?? 자바 공식문서 에 나와있는 readLine() 메서드에 대한 설명을 보면끝에 다음 문자(‘\\n’, ‘\\r’, ‘\\r\\n’) 중 하나가 있어야 하나의 라인으로 인식한다고 한다. Reads a line of text. A line is considered to be terminated by any one of a line feed (‘\\n’), a carriage return (‘\\r’), or a carriage return followed immediately by a linefeed. 또한, BufferedReader의 readLine() 메서드를 직접 들여다보면 다음과 같은 로직이 있는 것을 확인할 수 있다. eol은 ‘end of line’이지 않을까 싶다. charLoop: for (i = nextChar; i &lt; nChars; i++) { c = cb[i]; if ((c == '\\n') || (c == '\\r')) { eol = true; break charLoop; } } 그렇다면, HTTP 요청의 마지막 라인이 어떻길래 아직 라인이 종료되었다고 판단하지 않는걸까 ? 스택오버플로우에 있는 글 RFC 문서 34p HTTP 요청 형태 (HTTP 요청 헤더의 마지막 라인은 공백이었고 이로 인해 readLine() 메서드가 라인이 끝나지 않았다고 판단한 것) CRLF : Carriage Return(커서의 위치를 맨 앞으로 이동) + Line Feed(커서를 한 칸 아래로 이동) Request-Line*(( general-header | request-header | entity-header ) CRLF)CRLF[ message-body ] 결과적으로 line이 공백이면 \"공백\"이라는 문자열을 출력해봄으로써 실제 HTTP 요청이 위와 같이 들어온다는 것을 알 수 있었고,공백인 경우 break를 통해 while문 내에서 계속 대기상태에 머물러있지 않게 할 수 있었다.※ 참고. HTTP Response 포맷Status-Line*(( general-header | response-header | entity-header ) CRLF)CRLF[ message-body ]요약현상 HTTP 요청을 받아서 BufferedReader의 readLine() 메서드로 읽어 올 때 특정 라인에 도달하면 대기 상태에 머문다.원인 readLine() 메서드 내에는 ‘하나의 라인’이라고 판단하는 기준(라인 마지막에 ‘\\n’, ‘\\r’, ‘\\r\\n’)이 있는데, HTTP 요청의 마지막 라인은 공백이었기 때문에 아직 한 라인이 끝나지 않았다고 판단하여 계속 대기하고 있었다.배운 것 HTTP 요청 형태 BufferedReader의 readLine() 메서드가 라인을 인식하는 방법실제 코드 적용 간단한 웹 서버 구현 레파지토리 오늘 내용 관련 코드 private static HttpRequest processGetRequest(String requestUrl, BufferedReader bufferedReader) throws IOException { Map&lt;String, String&gt; cookies = null; for (String line = bufferedReader.readLine(); (line != null &amp;&amp; !line.isEmpty()); line = bufferedReader.readLine()) { if (line.contains(\"Cookie\")) { String[] info = line.split(\":\"); cookies = parseCookies(info[1]); break; } } if (!requestUrl.contains(\"?\")) { return new HttpRequest(HttpMethod.GET, requestUrl, cookies); } String[] info = requestUrl.split(\"\\\\?\"); Map&lt;String, String&gt; params = parseQueryString(info[1]); return new HttpRequest(HttpMethod.GET, info[0], params, cookies); } private static HttpRequest processPostRequest(String requestUrl, BufferedReader bufferedReader) throws IOException { int contentLen = 0; String contentType = \"\"; Map&lt;String, String&gt; params = null; Map&lt;String, String&gt; cookies = null; for (String line = bufferedReader.readLine(); (line != null &amp;&amp; !line.isEmpty()); line = bufferedReader.readLine()) { if (line.contains(\"Content-Length\")) { String[] info = line.split(\":\"); contentLen = Integer.parseInt(info[1].trim()); } else if (line.contains(\"Content-Type\")) { String[] info = line.split(\":\"); contentType = info[1].trim(); // ex) application/x-www-form-urlencoded } else if (line.contains(\"Cookie\")) { String[] info = line.split(\":\"); cookies = parseCookies(info[1]); } } if (contentLen &gt; 0) { char[] body = new char[contentLen]; bufferedReader.read(body); if (contentType.equals(\"application/x-www-form-urlencoded\")) { String queryString = new String(body); params = parseQueryString(queryString); } else if (contentType.equals(\"application/json\")) { // TODO } } return new HttpRequest(HttpMethod.POST, requestUrl, params, cookies); } 더 공부해야할 부분 JAVA I/O HTTP 응답 분할(HTTP Response Splitting, CRLF) 취약점참고자료 https://stackoverflow.com/questions/7855822/bufferedreader-readline-method-hangs-and-block-program https://docs.oracle.com/javase/8/docs/api/java/io/BufferedReader.html https://stackoverflow.com/questions/50447483/end-of-http-header https://datatracker.ietf.org/doc/html/rfc2616" }, { "title": "화면 리디렉션시 쿠키 송신이 안되는 현상", "url": "/posts/cookie-not-setting/", "categories": "경험하기, 이슈 노트", "tags": "Cookie, Set-Cookie", "date": "2021-05-28 00:29:00 +0900", "snippet": "상황 login.html 화면에서 로그인에 성공하면 쿠키에 logined=true 값을 세팅하고 index.html로 리디렉션 시킨다 하지만, index.html 화면을 요청할 때, 기존에 세팅되었던 cookie값이 없어진다. 원인 Set-Cookie 헤더의 속성 중 Path값을 따로 설정하지 않으면 쿠키를 응답한 화면이 포함...", "content": "상황 login.html 화면에서 로그인에 성공하면 쿠키에 logined=true 값을 세팅하고 index.html로 리디렉션 시킨다 하지만, index.html 화면을 요청할 때, 기존에 세팅되었던 cookie값이 없어진다. 원인 Set-Cookie 헤더의 속성 중 Path값을 따로 설정하지 않으면 쿠키를 응답한 화면이 포함된 디렉토리와 그 하위 디렉토리로 요청하는 경우에만 쿠키를 송신한다. 파일 디렉토리는 아래와 같기 때문에 Path를 따로 설정하지 않으면, login 화면이 포함된 /user 디렉토리와 그 하위에 있는 자원을 요청하는 경우에만 쿠키가 송신된다. 결과적으로, index 화면은 login 화면보다 상위 디렉토리에 존재하기 때문에 index 화면에 리디렉션시 쿠키가 송신되지 않았던 것이다. ├── ./index.html├── ./js│   ├── ./js/bootstrap.min.js│   ├── ./js/jquery-2.2.0.min.js│   └── ./js/scripts.js├── ./qna│   ├── ./qna/form.html│   └── ./qna/show.html└── ./user ├── ./user/form.html ├── ./user/list.html ├── ./user/login.html ├── ./user/login_failed.html └── ./user/profile.html 해결 방법 Path 속성을 추가해준다. public String login(String id, String pw, HttpResponse response) throws IOException { User findUser = findUser(id); if(findUser!=null &amp;&amp; pw.equals(findUser.getPassword())) { response.setCookie(\"logined=true; Path=/\"); return \"/index.html\"; } response.setCookie(\"logined=false; Path=/\"); return \"/user/login_failed.html\";} 하지만, 이 글 에 보면 여러 개의 애플리케이션이 웹 서버에 있는 경우, Path를 루트 디렉토리(/)로 세팅하는 것은보안상 좋지 않다고 말한다. 왜냐하면 하위 디렉토리(ex : …/newapp)에 다른 애플리케이션이 있다면, 해당 애플리케이션에 요청하는 경우에도 쿠키가 송신되기 때문이다. 따라서, 여러 애플리케이션을 동일한 웹 서버에서 호스팅하는 시나리오에서는 가능한 개별 애플리케이션을 자체 하위 디렉토리(“…/myapp1”, “…/myapp2” 등)에 배치하고개별 쿠키를 개별 애플리케이션 경로에만 유효하도록 선언하는 것이 좋다. Set-Cookie 필드 속성 값 살펴보기 속성 설명 name = VALUE 쿠키에 부여된 이름과 값(필수) Expires = DATE 쿠키의 유효 기간(default : 브라우저 닫을 때까지) Path = PATH 쿠키 적용이 되는 서버 상의 디렉토리(default : 쿠키를 생성한 도큐먼트와 같은 디렉토리) Domain = 도메인 명 쿠키 적용 대상이 되는 도메인 명(default : 쿠키를 생성한 서버의 도메인) Secure HTTPS로 통신하는 경우에만 쿠키 송신 HttpOnly 쿠키를 자바스크립트에서 엑세스하지 못하도록 제한 참고 자료 우에노 센, 『그림으로 배우는 Http &amp; Network Basic』, 영진닷컴(2015) https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Set-Cookie https://secureteam.co.uk/articles/how-to-make-the-perfect-cookies/" }, { "title": "JPA 내부 구조 살펴보기", "url": "/posts/jpa-inside-persistence-context/", "categories": "", "tags": "", "date": "2021-05-12 00:29:00 +0900", "snippet": "들어가기 전JPA 내부 구조를 제대로 모르고 사용하면, 오히려 성능 저하를 발생시킬 수도 있다고 한다. 따라서, JPA는 내부적으로 어떻게 동작하는 것인지 알기 위해인프런 김영한님 강의 를 들으며 공부해보았다.JPA 구동 방식 persistence.xml 예시&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;per...", "content": "들어가기 전JPA 내부 구조를 제대로 모르고 사용하면, 오히려 성능 저하를 발생시킬 수도 있다고 한다. 따라서, JPA는 내부적으로 어떻게 동작하는 것인지 알기 위해인프런 김영한님 강의 를 들으며 공부해보았다.JPA 구동 방식 persistence.xml 예시&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;persistence version=\"2.2\" xmlns=\"http://xmlns.jcp.org/xml/ns/persistence\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://xmlns.jcp.org/xml/ns/persistence http://xmlns.jcp.org/xml/ns/persistence/persistence_2_2.xsd\"&gt; &lt;persistence-unit name=\"hello\"&gt; &lt;properties&gt; &lt;!-- 필수 속성 --&gt; &lt;property name=\"javax.persistence.jdbc.driver\" value=\"org.h2.Driver\"/&gt; &lt;property name=\"javax.persistence.jdbc.user\" value=\"sa\"/&gt; &lt;property name=\"javax.persistence.jdbc.password\" value=\"\"/&gt; &lt;property name=\"javax.persistence.jdbc.url\" value=\"jdbc:h2:tcp://localhost/~/test\"/&gt; &lt;property name=\"hibernate.dialect\" value=\"org.hibernate.dialect.H2Dialect\"/&gt; &lt;!-- 옵션 --&gt; &lt;property name=\"hibernate.show_sql\" value=\"true\"/&gt; &lt;property name=\"hibernate.format_sql\" value=\"true\"/&gt; &lt;property name=\"hibernate.use_sql_comments\" value=\"true\"/&gt; &lt;property name=\"hibernate.hbm2ddl.auto\" value=\"create\" /&gt; &lt;/properties&gt; &lt;/persistence-unit&gt;&lt;/persistence&gt; EntityManagerFactory, EntityManager 사용public class JpaMain { public static void main(String[] args) { EntityManagerFactory emf = Persistence.createEntityManagerFactory(\"hello\"); EntityManager em = emf.createEntityManager(); EntityTransaction tx = em.getTransaction(); tx.begin(); try { Order order = new Order(); order.setId(1L); em.persist(order); tx.commit(); } catch (Exception e) { tx.rollback(); } finally { em.close(); } emf.close(); }}주의사항 엔티티 매니저 팩토리는 하나만 생성해서 애플리케이션 전체에서 공유된다. 엔티티 매니저는 쓰레드간에 공유X (그렇지 않으면 데이터 정합성이 깨질 수 있다). JPA의 모든 데이터 변경은 트랜잭션 안에서 실행되어야 한다.영속성 컨텍스트(Peristence Context) “엔티티를 영구 저장하는 환경”이라는 뜻으로 논리적인 개념이다.Hibernate와 같은 Persistence provider는 영속성 컨텍스트를 사용하여 애플리케이션의 엔티티 라이프사이클을 관리한다. EntityManager.persist(entity); EntityManager는 영속성 컨텍스트와 상호작용할 수 있게 해주는 인터페이스이다. 즉, EntityManager를 통해 영속성 컨텍스트와 상호작용 할 수 있다. persist(entity)는 DB가 아닌 영속성 컨텍스트에 저장하는 것. 엔티티의 라이프사이클 출처 : https://thorben-janssen.com/entity-lifecycle-model/1.비영속 (new/transient) 영속성 컨텍스트와 전혀 관계가 없는 새로운 상태로, 객체를 생성만 해놓은 상태이다.//객체를 생성한 상태(비영속)Member member = new Member();member.setId(\"member1\");member.setUsername(\"회원1\");2. 영속 (managed) 영속성 컨텍스트에 관리되는 상태이다. 영속 상태가 된다고해서 DB에 바로 쿼리가 날라가지 않는다. 쿼리는 트랜잭션 커밋시 날라간다.//객체를 생성한 상태(비영속)Member member = new Member();member.setId(\"member1\");member.setUsername(“회원1”);EntityManager em = emf.createEntityManager();em.getTransaction().begin();//객체를 저장한 상태(영속)em.persist(member);// DB에 쿼리 날린다.tx.commit();3. 준영속 (detached) 영속성 컨텍스트에 저장되었다가 분리된 상태이다. 준영속 상태 영속 -&gt; 준영속 영속 상태의 엔티티가 영속성 컨텍스트에서 분리(detached) 영속성 컨텍스트가 제공하는 기능을 사용 못함 준영속 상태로 만드는 방법 em.detach(entity) 특정 엔티티만 준영속 상태로 전환 em.clear() 영속성 컨텍스트를 완전히 초기화 em.close() 영속성 컨텍스트를 종료 //회원 엔티티를 영속성 컨텍스트에서 분리, 준영속 상태em.detach(member);4. 삭제 (removed) 엔티티가 영속성 컨텍스트에서 삭제된 상태이다.//객체를 삭제한 상태(삭제)em.remove(member);영속성 컨텍스트의 이점1. 1차 캐시 같은 트랜잭션 내에서만(하나의 사용자 요청) 유효하다. 즉, 엔티티 매니저는 트랜잭션 종료와 동시에 사라진다. 조회시Member member = new Member();member.setId(\"member1\");member.setUsername(\"회원1\");//1차 캐시에 저장됨em.persist(member);//1차 캐시에서 조회Member findMember = em.find(Member.class, \"member1\"); 1차 캐시에 없는 데이터 조회시 Member findMember2 = em.find(Member.class, \"member2\"); 2. 동일성(identity) 보장 1차 캐시로 반복 가능한 읽기(REPEATABLE READ) 등급의 트랜잭션 격리 수준을 데이터베이스가 아닌 애플리케이션 차원에서 제공Member a = em.find(Member.class, \"member1\");Member b = em.find(Member.class, \"member1\");System.out.println(a == b); //동일성 비교 true3. 트랜잭션을 지원하는 쓰기 지연(transactional write-behind)EntityManager em = emf.createEntityManager();EntityTransaction transaction = em.getTransaction();//엔티티 매니저는 데이터 변경시 트랜잭션을 시작해야 한다.transaction.begin(); // [트랜잭션] 시작em.persist(memberA);em.persist(memberB);//여기까지 INSERT SQL을 데이터베이스에 보내지 않는다.//커밋하는 순간 데이터베이스에 INSERT SQL을 보낸다.transaction.commit(); // [트랜잭션] 커밋 영속성 컨텍스트 내부에는 ‘쓰기 지연 SQL 저장소’라는 것도 있다. hibernate.jdbc.batch_size 옵션을 주게되면 해당 사이즈만큼 쿼리를 쌓아뒀다가 트랜잭션 커밋시 DB로 날리게된다. 4. 변경 감지(Dirty Checking) 자바 컬렉션 다루듯이 다룬다고 생각하면 된다.EntityManager em = emf.createEntityManager();EntityTransaction transaction = em.getTransaction();transaction.begin(); // [트랜잭션] 시작// 영속 엔티티 조회Member memberA = em.find(Member.class, \"memberA\");// 영속 엔티티 데이터 수정memberA.setUsername(\"hi\");memberA.setAge(10);//em.update(member) 불필요한 코드transaction.commit(); // [트랜잭션] 커밋 DB 트랜잭션을 커밋하게 되면 내부적으로 flush() 앤티티와 스냅샷 비교 스냅샷 : 1차 캐시에 최초로 들어왔을 때의 값 출처 : https://www.inflearn.com/course/ORM-JPA-Basic/dashboardflush 영속성 컨텍스트의 ‘쓰기 지연 SQL 저장소’에 있는 내용을 데이터베이스에 반영 (1차 캐시를 비우는게 아님) 트랜잭션이라는 작업 단위가 중요하다. 즉, 커밋 직전에만 동기화 하면 됨 플러시 발생 과정 변경 감지 수정된 엔티티를 쓰기 지연 SQL 저장소에 등록 쓰기 지연 SQL 저장소의 쿼리를 데이터베이스에 전송 (등록, 수정, 삭제 쿼리) 영속성 컨텍스트를 플러시하는 방법 직접 호출 em.flush() 플러시 자동 호출 트랜잭션 커밋 JPQL 쿼리 실행 플러시 모드 옵션 em.setFlushMode(FlushModeType.COMMIT) FlushModeType.AUTO 커밋이나 쿼리를 실행할 때 플러시 (기본값) FlushModeType.COMMIT 커밋할 때만 플러시 참고자료 김영한, 자바 ORM 표준 JPA 프로그래밍 - 기본편 https://www.baeldung.com/jpa-hibernate-persistence-context" }, { "title": "JPA 입문하기", "url": "/posts/jpa-introduction/", "categories": "", "tags": "", "date": "2021-05-10 00:29:00 +0900", "snippet": "들어가기 전JPA를 회사 프로젝트에서 사용하긴 했지만, 매우 간단한 CRUD에만 사용해서 사실 정확히는 알지 못했다. 따라서, JPA가 정확히 무엇이고 언제, 어디에 사용하는 것인지에 대해 인프런 김영한님 강의 를 들으며 공부해보았다.JPA란 ? ‘Java Persistence API’를 의미하며 자바 진영의 ORM 기술 표준이다. ORM? ...", "content": "들어가기 전JPA를 회사 프로젝트에서 사용하긴 했지만, 매우 간단한 CRUD에만 사용해서 사실 정확히는 알지 못했다. 따라서, JPA가 정확히 무엇이고 언제, 어디에 사용하는 것인지에 대해 인프런 김영한님 강의 를 들으며 공부해보았다.JPA란 ? ‘Java Persistence API’를 의미하며 자바 진영의 ORM 기술 표준이다. ORM? Object-relational mapping(객체 관계 매핑) 객체는 객체대로 설계 RDB는 RDB대로 설계한다. 각각 설계한 것을 ORM 프레임워크가 중간에서 매핑한다. 대중적인 언어에는 대부분 ORM 기술이 존재한다. JPA는 표준 명세이다. 즉, JPA는 인터페이스의 모음이다. JPA 2.1 표준 명세를 구현한 3가지 대표적인 구현체가 있다. Hibernate, EclipseLink, DataNucleus 발전 과정 : JDBC → MyBatis, JdbcTemplate → JPASQL 중심 코드의 문제점(JPA의 출현 배경)1. 생산성Without JPA 대부분은 객체를 관계형 DB에 관리한다. 지속적으로 반복되는 코드 자바 객체 → SQL SQL → 자바 객체 SQL을 일일이 작성해야한다.With JPA- 저장: jpa.persist(member)- 조회: Member member = jpa.find(memberId)- 수정: member.setName(“변경할 이름”)- 삭제: jpa.remove(member)2. 유지보수성Without JPA 필드 변경시 관련된 모든 SQL 수정public class Member { private String memberId; private String name; private String tel; // 항목 추가 ...}INSERT INTO MEMBER(MEMBER_ID, NAME, TEL) VALUES SELECT MEMBER_ID, NAME, TEL FROM MEMBER M UPDATE MEMBER SET ... TEL = ?With JPA 필드만 추가하면 됨, SQL은 JPA가 처리public class Member { private String memberId; private String name; private String tel; // 필드 추가 ...}3. 패러다임 불일치 발생 객체가 나온 사상과, RDB가 나온 사상이 다르다. 하지만, RDB에 맞춰 객체를 다루다보니 사실상 개발자가 SQL Mapper의 역할을 하게되었다.3-1. 상속 출처 : https://www.inflearn.com/course/ORM-JPA-Basic/dashboardWithout JPA Album 저장 객체 분해 INSERT INTO ITEM … INSERT INTO ALBUM … Album 조회 각각의 테이블에 따른 조인 SQL 작성 각각의 객체 생성 Album뿐 아니라 다른 자식 객체들도 이러한 작업을 해야한다고 생각하면 매우 번거로울 것이다. 따라서, 일반적으로 DB에 저장할 객체에는 상속 관계를 사용하지 않는다.With JPA Album 저장 jpa.persist(album); 나머진 JPA가 처리 Album 조회 Album album = jpa.find(Album.class, albumId); 나머진 JPA가 처리 SELECT I.*, A.* FROM ITEM I JOIN ALBUM A ON I.ITEM_ID = A.ITEM_ID 3-2. 연관관계 객체는 참조를 사용(단방향): member.getTeam() 테이블은 외래 키를 사용(양방향): JOIN ON M.TEAM_ID = T.TEAM_IDWithout JPA 객체를 테이블에 맞추어 모델링class Member { String id; // MEMBER_ID 컬럼 사용 Long teamId; // TEAM_ID FK 컬럼 사용 String username;// USERNAME 컬럼 사용}class Team { Long id; // TEAM_ID PK 사용 String name; // NAME 컬럼 사용}INSERT INTO MEMBER(MEMBER_ID, TEAM_ID, USERNAME) VALUES ... 객체다운 모델링class Member { String id; // MEMBER_ID 컬럼 사용 Team team; // 참조로 연관관계를 맺는다. String username; // USERNAME 컬럼 사용 Team getTeam() { return team; }}class Team { Long id; // TEAM_ID PK 사용 String name; // NAME 컬럼 사용}// TEAM_ID : member.getTeam().getId(); INSERT INTO MEMBER(MEMBER_ID, TEAM_ID, USERNAME) VALUES ...이렇게 객체다운 모델링을 했을때의 문제점은 조회시 매우 번거로워질 수 있다는 것이다. 아래 쿼리를 통해 얻은 결과를 Member 객체에 세팅하는 상황을 가정해보자. SELECT M.*, T.* FROM MEMBER M JOIN TEAM T ON M.TEAM_ID = T.TEAM_ID 조회 결과에 섞여있는 모든 데이터를 Member, Team 각각의 객체에 알맞게 세팅한 뒤, Team 객체는 Member 객체에 세팅해줘야 한다. 따라서, 실무에서는 생산성을 위해 Member와 Team의 필드를 모두 합친 SuperDTO 등을 만들거나 하는 일이 비일비재하다. public Member find(String memberId) { //SQL 실행 ... Member member = new Member(); //데이터베이스에서 조회한 회원 관련 정보를 모두 입력 Team team = new Team(); //데이터베이스에서 조회한 팀 관련 정보를 모두 입력 //회원과 팀 관계 설정 member.setTeam(team); return member;}With JPAMember member = list.get(memberId);Team team = member.getTeam();member.setTeam(team);jpa.persist(member);3-3. 객체 그래프 탐색 객체는 자유롭게 객체 그래프를 탐색할 수 있어야 한다.Without JPA 처음 실행하는 SQL에 따라 객체 그래프 탐색 범위기 결정된다. 만약 아래 쿼리에 대한 결과로 객체 그래프를 탐색한다면 member.getOrder();는 null을 반환할 것이다. SELECT M.*, T.* FROM MEMBER M JOIN TEAM T ON M.TEAM_ID = T.TEAM_ID 이와 같은 상황은 엔티티 신뢰 문제로 이어진다. 즉, 아래 코드의 경우 getTeam(), getOrder() 등이 null이 아닌지 확신하려면 memberDAO.find(memberId) 내부에 어떤 쿼리가 동작하는지 살펴봐야 한다. class MemberService {... public void process() { Member member = memberDAO.find(memberId); member.getTeam(); //??? member.getOrder().getDelivery(); // ??? }}With JPAMember member = jpa.find(Member.class, memberId);Team team = member.getTeam(); 신뢰할 수 있는 엔티티, 계층 class MemberService { ... public void process() { Member member = memberDAO.find(memberId); member.getTeam(); // 자유로운 객체 그래프 탐색 member.getOrder().getDelivery(); }} 3-4. 객체 비교Without JPAString memberId = \"100\";Member member1 = memberDAO.getMember(memberId);Member member2 = memberDAO.getMember(memberId);member1 == member2; //다르다.class MemberDAO { public Member getMember(String memberId) { String sql = \"SELECT * FROM MEMBER WHERE MEMBER_ID = ?\"; ... //JDBC API, SQL 실행 return new Member(...); }}With JPA 동일한 트랜잭션에서 조회한 엔티티는 같음을 보장한다.String memberId = \"100\";Member member1 = list.get(memberId);Member member2 = list.get(memberId);member1 == member2; //같다.JPA와 성능1. 1차 캐시와 동일성(identity) 보장 같은 트랜잭션 안에서는 같은 엔티티를 반환 약간의 조회 성능이 향상된다. (미미한 수준) DB Isolation Level이 Read Committed라도 애플리케이션에서 Repeatable Read를 보장한다. String memberId = \"100\"; Member m1 = jpa.find(Member.class, memberId); //SQL Member m2 = jpa.find(Member.class, memberId); //캐시 println(m1 == m2) //true // 결과적으로는 SQL 1번만 실행 2. 트랜잭션을 지원하는 쓰기 지연(transactional write-behind) 트랜잭션을 커밋할 때까지 INSERT SQL을 모음 JDBC BATCH SQL 기능을 사용해서 한번에 SQL 전송transaction.begin(); // 트랜잭션 시작em.persist(memberA);em.persist(memberB);em.persist(memberC);//여기까지 INSERT SQL을 데이터베이스에 보내지 않는다.//커밋하는 순간 데이터베이스에 INSERT SQL을 모아서 보낸다.transaction.commit(); // 트랜잭션 커밋3. 지연 로딩과 즉시 로딩 옵션을 통해 지연 로딩과 즉시 로딩을 자유롭게 선택할 수 있다. 만약 SQL 중심이었다면 관련된 쿼리를 모두 변경해야 했을 것이다. 지연로딩 : 객체가 실제 사용될 때 로딩 Member member = memberDAO.find(memberId); // SELECT * FROM MEMBERTeam team = member.getTeam();String teamName = team.getName(); // SELECT * FROM TEAM 즉시 로딩: JOIN SQL로 한번에 연관된 객체까지 미리 조회 Member member = memberDAO.find(memberId); // SELECT * FROM M.*, T.* FROM MEMBER JOIN TEAM ...Team team = member.getTeam();String teamName = team.getName(); 참고자료 김영한, 자바 ORM 표준 JPA 프로그래밍 - 기본편" }, { "title": "간단한 DI 컨테이너 구현해보기", "url": "/posts/my-di-container/", "categories": "경험하기, 작업 노트", "tags": "Spring", "date": "2021-04-09 22:25:00 +0900", "snippet": "들어가기 전순수 자바코드로 간단한 스프링 DI 컨테이너를 구현해보면서 DI의 필요성, 스프링 컨테이너 동작 원리 등을 이해해보기 위해 약 일주일간 토이 프로젝트를 진행했다.아래의 요구사항들을 구현하며 간단한 컨테이너를 구현해보았다. 컨테이너 생성자 파라미터로 AppConfig.class를 전달받고 해당 클래스의 정의된 메서드를 기반으로 빈을 생성한다...", "content": "들어가기 전순수 자바코드로 간단한 스프링 DI 컨테이너를 구현해보면서 DI의 필요성, 스프링 컨테이너 동작 원리 등을 이해해보기 위해 약 일주일간 토이 프로젝트를 진행했다.아래의 요구사항들을 구현하며 간단한 컨테이너를 구현해보았다. 컨테이너 생성자 파라미터로 AppConfig.class를 전달받고 해당 클래스의 정의된 메서드를 기반으로 빈을 생성한다. 생성되는 빈은 유일해야 한다(싱글톤). 빈의 이름으로 빈을 가져올 수 있다. 빈의 타입으로 빈을 가져올 수 있다. 관리되는 모든 빈의 이름을 조회할 수 있다. 예외 처리 부모 타입으로 조회시, 자식이 둘 이상 있으면 중복 오류가 발생한다. 등록되지 않은 빈의 이름으로 조회시 오류가 발생해야 한다. 이를 바탕으로 간단한 애플리케이션 로직 작성※ 구현 코드 repositoryDI(Dependency Injection) Container란 ? 애플리케이션에서 사용하는 객체들을 생성하고 객체들 간의 의존관계 주입, 객체의 생명주기 관리 등을 담당하는 주체 스프링 DI 컨테이너는 의존관계 주입 이외에도 매우 많은 역할을 수행한다.의존성(의존관계) 주입이란 ? 호출할 객체를 직접 선언하는게 아니라 런타임시 외부에서 주입해주는 것 DI가 가능한 이유는 구체적인 객체가 아닌 인터페이스에 의존하기 때문이다. 따라서, 객체 간의 의존관계를 정적인 클래스 관계에서는 알 수 없다.public class CustomerServiceImpl implements CustomerService { // 인터페이스에 의존 private final CustomerRepository customerRepository; // 런타임시 외부에서 주입 public CustomerServiceImpl(CustomerRepository customerRepository) { this.customerRepository = customerRepository; }의존성(의존관계) 주입이 필요한 이유 DIP, SRP, OCP를 지킨 좋은 설계를 가능하게 해준다. 이를 통해, 변화에 유연하게 대처할 수 있다. DIP(Dependency Inversion Principle) : 추상화에 의존해야지, 구체화에 의존하면 안된다. SRP(Single Responsibility Principle) : 한 클래스는 하나의 책임만 가져야 한다. OCP(Open Closed Principle) : 소프트웨어 요소는 확장에는 열려 있으나 변경에는 닫혀 있어야 한다. 만약, 다음과 같은 코드에서 인메모리 DB를 사용하는 TemporaryCustomerRepository를 RDB를 사용하는 구현체로 바꿔야한다고 했을 때 어떤 문제가 있을지 살펴보자 CustomerServiceImpl이 TemporaryCustomerRepository() 라는 구체적인 객체에 의존하고 있다 (DIP 위반) 따라서 구현체를 변경하려면 TemporaryCustomerRepository() 를 새로운 구현체로 변경해줘야 한다 (OCP 위반) CustomerServiceImpl의 책임은 비즈니스 로직을 수행하는 것인데, 구체적인 구현체까지 결정하고 있다 (SRP 위반) public class CustomerServiceImpl implements CustomerService { private final CustomerRepository customerRepository = new TemporaryCustomerRepository(); @Override public void join(Customer customer) { customerRepository.save(customer.getId(), customer); } 반면, 맨 처음 살펴봤던 의존관계 주입 코드의 경우 CustomerRepository라는 인터페이스에만 의존한다 (DIP) 따라서 다른 구현체로 변경해야 하는 경우 해당 코드에 변경사항은 없다 (OCP) CustomerRepository의 구현체는 런타임시 외부에서 주입해주고 CustomerServiceImpl클래스는 비즈니스 로직만 수행한다 (SRP) public class CustomerServiceImpl implements CustomerService {// 인터페이스에 의존private final CustomerRepository customerRepository;// 런타임시 외부에서 주입public CustomerServiceImpl(CustomerRepository customerRepository) { this.customerRepository = customerRepository;} 구현해보기※ 구현하면서 스스로 했던 질문들을 적어봤습니다1. AppConfig 클래스 활용하여 컨테이너에 빈 등록하기 빈으로 등록하기 위해 AppConfig에 선언한 다양한 메서드의 이름, 리턴타입, 실제 구현체를 어떻게 가져오지 ? reflection, 제네릭에 대해 공부 빈을 담아두는 자료 구조는 뭐가 좋을까 ? 빈 이름/클래스 타입 or 클래스 타입 or 빈 이름으로 원하는 객체를 가져올 수 있어야 한다 2개의 Map을 활용해서 (빈 이름 - 인스턴스) 쌍과 (타입 - 빈 이름 리스트) 쌍 만든다. 두 번째 Map에서 value가 리스트인 이유는 타입이 같은 빈이 여러 개인 경우도 있기 때문에 결과적으로 타입으로만 조회하는 경우 해당 타입의 빈 이름을 가져와서 그 이름으로 첫 번째 Map에서 인스턴스를 얻어올 수 있다. 컨테이너 클래스의 의존 관계는 어떤식으로 구성 하는게 좋을까 ? 현재는 빈을 등록하고 조회하는 기능 위주의 컨테이너이지만 다른 기능의 확장성을 고려해서 Container 인터페이스에 기능별로 인터페이스를 상속하는게 좋을 것 같다 public interface BeanManagement { void registerBeans(Class&lt;?&gt;... clazz); Object getBean(String beanName) throws NoSuchBeanDefinitionException; &lt;T&gt; T getBean(String beanName, Class&lt;T&gt; beanType) throws NoSuchBeanDefinitionException; &lt;T&gt; T getBean(Class&lt;T&gt; beanType) throws NoUniqueBeanDefinitionException; String[] getBeanDefinitionNames(); int getBeanDefinitionCount(); } public interface Container extends BeanManagement, 추가적인 기능들... { } public class MyContainer implements Container { ... } 실제 스프링의 ApplicationContext도 아래와 같이 되어있었다.public interface ApplicationContext extends EnvironmentCapable, ListableBeanFactory, HierarchicalBeanFactory,\t\tMessageSource, ApplicationEventPublisher, ResourcePatternResolver {2. 의존관계 주입 방식 결정하기 생성자를 통한 의존관계 주입방식으로 결정수정자(setter)를 통한 의존관계 주입 방식 public 으로 set메서드를 선언해야 하기 때문에 추후에 실수로라도 변경될 가능성이 있음 실수로 의존성 주입을 해주지 않으면 런타임시 NPE 발생할 수 있다.생성자를 통한 의존관계 주입 방식 인스턴스 변수를 final로 선언할 수 있기 때문에 객체가 주입되고 나면 불변이다. 대부분의 의존관계는 애플리케이션 종료 전까지 불변해야하므로 불변이 보장되어야 한다. final로 선언한 변수에 대해 생성자에서 초기화를 하지 않으면 컴파일 오류가 나기 때문에 주입하지 않는 실수를 범할 일이 없다. 스프링 공식 문서 에서도 생성자를 통한 주입방식 권장 The Spring team generally advocates constructor injection as it enables one to implement application componentsas immutable objects and to ensure that required dependencies are not null.Furthermore constructor-injected components are always returned to client (calling) code in a fully initialized state.As a side note, a large number of constructor arguments is a bad code smell,implying that the class likely has too many responsibilities and should be refactored to better address proper separation of concerns. 3. 빈을 싱글톤 객체로 만들기모든 구현체 클래스에 싱글톤 패턴 적용 boilerplate 코드가 만들어진다. 싱글톤 패턴을 적용하면 유연성이 떨어진다(상속 불가 등) 싱글톤으로 생성되는 객체는 구현체를 내부에 선언하므로 객체간에 결합도가 높아진다.Config 클래스들에 공통적으로 적용될 수 있는 부모 클래스 생성 Container가 여러번 생성되더라도 Config에 있는 빈은 한 번만 등록될 수 있도록 static으로 선언 멀티스레딩 환경에서의 동시 접근 문제를 방지하기 위해 ConcurrentHashMap 사용 문제점 AppConfig에서 boilerplate 코드가 생긴다 (빈 있는지 체크, 없으면 객체 생성) 메서드 이름이 바뀌면 beanName 변수의 값도 변경해줘야 한다. 즉, 실수할 여지를 제공한다. 상속 구조로 인해 AppConfig는 CommonConfig에 의존하게된다. CommonConfig의 변경이 AppConfig에 영향을 줄 수 있다. public class CommonConfig { private static Map&lt;String, Object&gt; beanStore = new ConcurrentHashMap&lt;&gt;(); private static Set&lt;String&gt; beanNames = ConcurrentHashMap.newKeySet(); public Object getBean(String beanName) { return beanStore.get(beanName); } public &lt;T&gt; void createBean(String beanName, T instance) { beanStore.put(beanName, instance); beanNames.add(beanName); } public boolean isExist(String beanName) { return beanNames.contains(beanName); }}public class AppConfig extends CommonConfig { public CustomerService customerService() { String beanName = \"customerService\"; if(!isExist(beanName)) { CustomerService customerService = new CustomerServiceImpl(customerRepository(), plannerService()); createBean(beanName, customerService); } return (CustomerService) getBean(beanName); } public PlannerService plannerService() { String beanName = \"plannerService\"; if(!isExist(beanName)) { PlannerService plannerService = new PlannerServiceImpl(customerRepository()); createBean(beanName, plannerService); } return (PlannerService) getBean(beanName); } public CustomerRepository customerRepository() { String beanName = \"customerRepository\"; if(!isExist(beanName)) { CustomerRepository customerRepository = new TemporaryCustomerRepository(); createBean(beanName, customerRepository); } return (CustomerRepository) getBean(beanName); }}4. 만들어진 컨테이너를 기반으로 간단한 애플리케이션 작성더 공부해야할 부분 스프링에서 싱글톤 객체를 보장하는 방법참고 자료 인프런 김영한님 강의(스프링 핵심 원리 - 기본편) SOLID 원칙" }, { "title": "Java - 자바 제네릭", "url": "/posts/java-generic/", "categories": "지식 더하기, 이론", "tags": "Java", "date": "2021-04-06 23:00:00 +0900", "snippet": "제네릭 generic : 포괄적인, 총칭[통칭]의 자바는 여러 타입이 존재하기 때문에, 형 변환을 하면서 많은 예외가 발생할 수 있다.따라서, Java5 부터 도입된 제네릭을 통해 타입 형 변환에서 발생할 수 있는 문제점을 사전에 방지해준다.즉, 컴파일 시 이러한 부분을 점검할 수 있도록 해준다.public class CastingGenericDT...", "content": "제네릭 generic : 포괄적인, 총칭[통칭]의 자바는 여러 타입이 존재하기 때문에, 형 변환을 하면서 많은 예외가 발생할 수 있다.따라서, Java5 부터 도입된 제네릭을 통해 타입 형 변환에서 발생할 수 있는 문제점을 사전에 방지해준다.즉, 컴파일 시 이러한 부분을 점검할 수 있도록 해준다.public class CastingGenericDTO&lt;T&gt; implements Serialiazble { private T object; public void setObject(T obj) { \tthis.object= obj; } public T getObejct() { \treturn object; }}public class GenricTester {\tpublic static void main(String[] args) { \tCastingGenericDTO&lt;String&gt; dto1 = new CastingGenericDTO&lt;&gt;(); dto1.setObject(\"string type\");\t\tdto1.setObject(new StringBuilder(\"builder\")); // 형 불일치로 컴파일 에러 발생 ! CastingGenericDTO&lt;StringBuilder&gt; dto2 = new CastingGenericDTO&lt;&gt;(); dto2.setObject(new StringBuilder(\"builder type\")); String val1 = dto2.getObject(); // 형 불일치로 컴파일 에러 발생 ! StringBuilder val2 = dto1.getObject(); // 형 불일치로 컴파일 에러 발생 ! }}제네릭 타입의 이름 아래 규칙을 따라야 컴파일 되는 것은 아니지만, 통상적으로 쓰이는 것이므로 다른 사람이 보기에도 이해하기 쉽도록 해당 규칙을 따르는게 좋다. E : 요소 (Element, 자바 컬렉션에서 주로 사용됨) K : 키 N : 숫자 T : 타입 V : 값 S,U,V : 두 번째, 세 번째, 네 번째에 선언된 타입wildcard : ? 만약 어떤 메서드에서 CastingGenericDTO 타입의 파라미터를 받고 싶으면 어떻게 해야할까 ? 다음과 같이 할 수 있다. public void printValue(CastingGenericDTO&lt;?&gt; dto) { \tSystem.out.println(dto.getObejct());} 하지만 위와 같이 하게 되면, CastingGenericDTO 타입의 파라미터에 값을 추가하거나 하는 동작을 할 수는 없다. 만약 그러한 로직을 처리해야 한다면 아래와 같이 메서드를 선언할 수 있다. public &lt;T&gt; void setValue(CastingGenericDTO&lt;T&gt; dto, T newValue) { dto.setObject(newValue); System.out.println(dto.getObejct()); } 제네릭 선언에 사용하는 타입 범위 지정하기 &lt;&gt; 안에 어떤 타입도 상관 없지만, wildcard로 사용하는 타입을 제한할 수는 있다. 다음과 같이 하면 Car 클래스를 상속받거나, Car 인터페이스를 구현하는 클래스만 파라미터로 받을 수 있다. public void printCarType(CastingGenericDTO&lt;? extends Car&gt; car) { System.out.println(car.getType()); } public &lt;T extends Car&gt; void setCarType(CastingGenericDTO&lt;T&gt; car, T newType) { car.setType(newType); System.out.println(car.getObejct()); } 멀티 제네릭 타입 public &lt;S,T extends Car&gt; void multiValues(CastingGenericDTO&lt;S&gt; dto, T newCar, S value) { }실제로 적용하기 간단한 웹 서버를 구현해본 토이 프로젝트에서 사용해보았다. 클라이언트 측으로 HTTP 응답을 보낼 때, setStatusCode()로 상태 코드 세팅시 엉뚱한 값이 들어오지 못하도록HttpStatusCode 타입인 값만 세팅될 수 있게 했다.public &lt;T extends HttpStatusCode&gt; void setStatusCode(T statusCode) throws IOException { dos.writeBytes(String.format(\"HTTP/1.1 %s \\r\\n\", statusCode.getValue()));}public interface HttpStatusCode { String getValue();}public enum HttpStatusCode2xx implements HttpStatusCode { OK(\"200 OK\"), Created(\"201 Created\"), Accepted(\"202 Accepted\"), NonAuthoritativeInformation(\"203 Non-Authoritative Information\"), NoContent(\"204 No Content\"), ResetContent(\"205 Reset Content\"), PartialContent(\"206 Partial Content\"); private String statusCode; HttpStatusCode2xx(String statusCode) { this.statusCode = statusCode; } @Override public String getValue() { return this.statusCode; }}참고 자료 이상민, 『자바의 신 2』, 로드북(2017), 21장" } ]
